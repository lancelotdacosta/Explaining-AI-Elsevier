
@inproceedings{amariWhyNaturalGradient1998,
  address = {{Seattle, WA, USA}},
  title = {Why Natural Gradient?},
  volume = {2},
  isbn = {978-0-7803-4428-0},
  language = {en},
  booktitle = {Proceedings of the 1998 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}, {{ICASSP}} '98 ({{Cat}}. {{No}}.{{98CH36181}})},
  publisher = {{IEEE}},
  doi = {10.1109/ICASSP.1998.675489},
  author = {Amari, S. and Douglas, S.C.},
  year = {1998},
  pages = {1213-1216},
  file = {/Users/lancelotdacosta/Zotero/storage/7IX8M78V/Amari and Douglas - 1998 - Why natural gradient.pdf;/Users/lancelotdacosta/Zotero/storage/DRF4IXSN/Amari and Douglas - 1998 - Why natural gradient.pdf;/Users/lancelotdacosta/Zotero/storage/HB3ELQUW/Amari and Douglas - 1998 - Why natural gradient.pdf}
}


@article{fristonActiveListening2020,
  title = {Active {{Listening}}},
  author = {Friston, Karl J. and Sajid, Noor and {Quiroga-Martinez}, David Ricardo and Parr, Thomas and Price, Cathy J. and Holmes, Emma},
  year = {2020},
  month = mar,
  pages = {2020.03.18.997122},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.03.18.997122},
  abstract = {{$<$}p{$>$}This paper introduces active listening, as a unified framework for synthesising and recognising speech. The notion of active listening inherits from active inference, which considers perception and action under one universal imperative: to maximise the evidence for our (generative) models of the world. First, we describe a generative model of spoken words that simulates (i) how discrete lexical, prosodic, and speaker attributes give rise to continuous acoustic signals; and conversely (ii) how continuous acoustic signals are recognised as words. The 9active9 aspect involves (covertly) segmenting spoken sentences and borrows ideas from active vision. It casts speech segmentation as the selection of internal actions, corresponding to the placement of word boundaries. Practically, word boundaries are selected that maximise the evidence for an internal model of how individual words are generated. We establish face validity by simulating speech recognition and showing how the inferred content of a sentence depends on prior beliefs and background noise. Finally, we consider predictive validity by associating neuronal or physiological responses, such as the mismatch negativity and P300, with belief updating under active listening, which is greatest in the absence of accurate prior beliefs about what will be heard next.{$<$}/p{$>$}},
  chapter = {New Results},
  copyright = {\textcopyright{} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  file = {/Users/lancelotdacosta/Zotero/storage/FCP49C6G/Friston et al. - 2020 - Active Listening.pdf;/Users/lancelotdacosta/Zotero/storage/NWQT77C6/2020.03.18.html},
  journal = {bioRxiv},
  language = {en}
}




@article{amariNaturalGradientWorks1998,
  title = {Natural {{Gradient Works Efficiently}} in {{Learning}}},
  language = {en},
  author = {Amari, Shun-ichi},
  year = {1998},
  pages = {36},
  file = {/Users/lancelotdacosta/Zotero/storage/6P89MXG7/Amari - Natural Gradient Works Efﬁciently in Learning.pdf;/Users/lancelotdacosta/Zotero/storage/EGEUN5PX/Amari - Natural Gradient Works Efﬁciently in Learning.pdf;/Users/lancelotdacosta/Zotero/storage/L9SDXRJE/Amari - Natural Gradient Works Efﬁciently in Learning.pdf}
}

@article{parrDiscreteContinuousBrain2018,
  title = {The {{Discrete}} and {{Continuous Brain}}: {{From Decisions}} to {{Movement}}\textemdash{{And Back Again}}},
  volume = {30},
  issn = {0899-7667, 1530-888X},
  shorttitle = {The {{Discrete}} and {{Continuous Brain}}},
  language = {en},
  number = {9},
  journal = {Neural Computation},
  doi = {10.1162/neco_a_01102},
  author = {Parr, Thomas and Friston, Karl J.},
  month = sep,
  year = {2018},
  pages = {2319-2347},
  file = {/Users/lancelotdacosta/Zotero/storage/CK25GFGJ/Parr and Friston - 2018 - The Discrete and Continuous Brain From Decisions .pdf;/Users/lancelotdacosta/Zotero/storage/MAMJEMHT/Parr and Friston - 2018 - The Discrete and Continuous Brain From Decisions .pdf}
}

@article{marreirosPopulationDynamicsVariance2008,
  title = {Population Dynamics: {{Variance}} and the Sigmoid Activation Function},
  volume = {42},
  issn = {10538119},
  shorttitle = {Population Dynamics},
  language = {en},
  number = {1},
  journal = {NeuroImage},
  doi = {10.1016/j.neuroimage.2008.04.239},
  author = {Marreiros, Andr{\'e} C. and Daunizeau, Jean and Kiebel, Stefan J. and Friston, Karl J.},
  month = aug,
  year = {2008},
  pages = {147-157},
  file = {/Users/lancelotdacosta/Zotero/storage/5Z7K9PD6/Marreiros et al. - 2008 - Population dynamics Variance and the sigmoid acti.pdf;/Users/lancelotdacosta/Zotero/storage/9IGRFL9B/Marreiros et al. - 2008 - Population dynamics Variance and the sigmoid acti.pdf}
}

@article{barlowRedundancyReductionRevisited2001,
  title = {Redundancy Reduction Revisited},
  abstract = {Soon after Shannon defined the concept of redundancy it was suggested that it gave insight into mechanisms of sensory processing, perception, intelligence and inference. Can we now judge whether there is anything in this idea, and can we see where it should direct our thinking? This paper argues that the original hypothesis was wrong in over-emphasizing the role of compressive coding and economy in neuron numbers, but right in drawing attention to the importance of redundancy. Furthermore there is a clear direction in which it now points, namely to the overwhelming importance of probabilities and statistics in neuroscience. The brain has to decide upon actions in a competitive, chance-driven world, and to do this well it must know about and exploit the non-random probabilities and interdependences of objects and events signalled by sensory messages. These are particularly relevant for Bayesian calculations of the optimum course of action. Instead of thinking of neural representations as transformations of stimulus energies, we should regard them as approximate estimates of the probable truths of hypotheses about the current environment, for these are the quantities required by a probabilistic brain working on Bayesian principles.},
  language = {en},
  journal = {Comput. Neural Syst.},
  author = {Barlow, Horace},
  year = {2001},
  pages = {13},
  file = {/Users/lancelotdacosta/Zotero/storage/2BHF2QLA/Barlow - Redundancy reduction revisited.pdf}
}

@article{crooksMeasuringThermodynamicLength2007,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0706.0559},
  title = {Measuring Thermodynamic Length},
  volume = {99},
  issn = {0031-9007, 1079-7114},
  abstract = {Thermodynamic length is a metric distance between equilibrium thermodynamic states. Among other interesting properties, this metric asymptotically bounds the dissipation induced by a finite time transformation of a thermodynamic system. It is also connected to the Jensen-Shannon divergence, Fisher information and Rao's entropy differential metric. Therefore, thermodynamic length is of central interest in understanding matter out-of-equilibrium. In this paper, we will consider how to define thermodynamic length for a small system described by equilibrium statistical mechanics and how to measure thermodynamic length within a computer simulation. Surprisingly, Bennett's classic acceptance ratio method for measuring free energy differences also measures thermodynamic length.},
  language = {en},
  number = {10},
  journal = {Physical Review Letters},
  doi = {10.1103/PhysRevLett.99.100602},
  author = {Crooks, Gavin E.},
  month = sep,
  year = {2007},
  keywords = {Condensed Matter - Statistical Mechanics},
  pages = {100602},
  file = {/Users/lancelotdacosta/Zotero/storage/T2WYYMZS/Crooks - 2007 - Measuring thermodynamic length.pdf}
}

@article{moranNeuralMassesFields2013,
  title = {Neural Masses and Fields in Dynamic Causal Modeling},
  volume = {7},
  issn = {1662-5188},
  abstract = {Dynamic causal modeling (DCM) provides a framework for the analysis of effective connectivity among neuronal subpopulations that subtend invasive (electrocorticograms and local field potentials) and non-invasive (electroencephalography and magnetoencephalography) electrophysiological responses. This paper reviews the suite of neuronal population models including neural masses, fields and conductance-based models that are used in DCM. These models are expressed in terms of sets of differential equations that allow one to model the synaptic underpinnings of connectivity. We describe early developments using neural mass models, where convolution-based dynamics are used to generate responses in laminar-specific populations of excitatory and inhibitory cells. We show that these models, though resting on only two simple transforms, can recapitulate the characteristics of both evoked and spectral responses observed empirically. Using an identical neuronal architecture, we show that a set of conductance based models\textemdash{}that consider the dynamics of specific ion-channels\textemdash{}present a richer space of responses; owing to non-linear interactions between conductances and membrane potentials. We propose that conductance-based models may be more appropriate when spectra present with multiple resonances. Finally, we outline a third class of models, where each neuronal subpopulation is treated as a field; in other words, as a manifold on the cortical surface. By explicitly accounting for the spatial propagation of cortical activity through partial differential equations (PDEs), we show that the topology of connectivity\textemdash{}through local lateral interactions among cortical layers\textemdash{}may be inferred, even in the absence of spatially resolved data. We also show that these models allow for a detailed analysis of structure\textendash{}function relationships in the cortex. Our review highlights the relationship among these models and how the hypothesis asked of empirical data suggests an appropriate model class.},
  language = {en},
  journal = {Frontiers in Computational Neuroscience},
  doi = {10.3389/fncom.2013.00057},
  author = {Moran, Rosalyn and Pinotsis, Dimitris A. and Friston, Karl},
  year = {2013},
  file = {/Users/lancelotdacosta/Zotero/storage/C25G6J8E/Moran et al. - 2013 - Neural masses and fields in dynamic causal modelin.pdf;/Users/lancelotdacosta/Zotero/storage/THE93X3N/Moran et al. - 2013 - Neural masses and fields in dynamic causal modelin.pdf}
}

@article{decoDynamicBrainSpiking2008,
  title = {The {{Dynamic Brain}}: {{From Spiking Neurons}} to {{Neural Masses}} and {{Cortical Fields}}},
  volume = {4},
  issn = {1553-7358},
  shorttitle = {The {{Dynamic Brain}}},
  abstract = {The cortex is a complex system, characterized by its dynamics and architecture, which underlie many functions such as action, perception, learning, language, and cognition. Its structural architecture has been studied for more than a hundred years; however, its dynamics have been addressed much less thoroughly. In this paper, we review and integrate, in a unifying framework, a variety of computational approaches that have been used to characterize the dynamics of the cortex, as evidenced at different levels of measurement. Computational models at different space\textendash{}time scales help us understand the fundamental mechanisms that underpin neural processes and relate these processes to neuroscience data. Modeling at the single neuron level is necessary because this is the level at which information is exchanged between the computing elements of the brain; the neurons. Mesoscopic models tell us how neural elements interact to yield emergent behavior at the level of microcolumns and cortical columns. Macroscopic models can inform us about whole brain dynamics and interactions between large-scale neural systems such as cortical regions, the thalamus, and brain stem. Each level of description relates uniquely to neuroscience data, from single-unit recordings, through local field potentials to functional magnetic resonance imaging (fMRI), electroencephalogram (EEG), and magnetoencephalogram (MEG). Models of the cortex can establish which types of large-scale neuronal networks can perform computations and characterize their emergent properties. Mean-field and related formulations of dynamics also play an essential and complementary role as forward models that can be inverted given empirical data. This makes dynamic models critical in integrating theory and experiments. We argue that elaborating principled and informed models is a prerequisite for grounding empirical neuroscience in a cogent theoretical framework, commensurate with the achievements in the physical sciences.},
  language = {en},
  number = {8},
  journal = {PLoS Computational Biology},
  doi = {10.1371/journal.pcbi.1000092},
  author = {Deco, Gustavo and Jirsa, Viktor K. and Robinson, Peter A. and Breakspear, Michael and Friston, Karl},
  editor = {Sporns, Olaf},
  month = aug,
  year = {2008},
  pages = {e1000092},
  file = {/Users/lancelotdacosta/Zotero/storage/I6AC96IB/Deco et al. - 2008 - The Dynamic Brain From Spiking Neurons to Neural .PDF;/Users/lancelotdacosta/Zotero/storage/V22YBLPJ/Deco et al. - 2008 - The Dynamic Brain From Spiking Neurons to Neural .PDF}
}

@article{senguptaInformationEfficiencyNervous2013,
  title = {Information and {{Efficiency}} in the {{Nervous System}}\textemdash{{A Synthesis}}},
  volume = {9},
  issn = {1553-7358},
  abstract = {In systems biology, questions concerning the molecular and cellular makeup of an organism are of utmost importance, especially when trying to understand how unreliable components\textemdash{}like genetic circuits, biochemical cascades, and ion channels, among others\textemdash{}enable reliable and adaptive behaviour. The repertoire and speed of biological computations are limited by thermodynamic or metabolic constraints: an example can be found in neurons, where fluctuations in biophysical states limit the information they can encode\textemdash{}with almost 20\textendash{}60\% of the total energy allocated for the brain used for signalling purposes, either via action potentials or by synaptic transmission. Here, we consider the imperatives for neurons to optimise computational and metabolic efficiency, wherein benefits and costs trade-off against each other in the context of self-organised and adaptive behaviour. In particular, we try to link information theoretic (variational) and thermodynamic (Helmholtz) free-energy formulations of neuronal processing and show how they are related in a fundamental way through a complexity minimisation lemma.},
  language = {en},
  number = {7},
  journal = {PLoS Computational Biology},
  doi = {10.1371/journal.pcbi.1003157},
  author = {Sengupta, Biswa and Stemmler, Martin B. and Friston, Karl J.},
  editor = {Sporns, Olaf},
  month = jul,
  year = {2013},
  pages = {e1003157},
  file = {/Users/lancelotdacosta/Zotero/storage/4KQTQL9Y/Sengupta et al. - 2013 - Information and Efficiency in the Nervous System—A.PDF;/Users/lancelotdacosta/Zotero/storage/TVCQK5KM/Sengupta et al. - 2013 - Information and Efficiency in the Nervous System—A.PDF}
}

@article{hartmannJarzynskiEqualityFluctuation2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.09347},
  title = {Jarzynski's Equality, Fluctuation Theorems, and Variance Reduction: {{Mathematical}} Analysis and Numerical Algorithms},
  volume = {175},
  issn = {0022-4715, 1572-9613},
  shorttitle = {Jarzynski's Equality, Fluctuation Theorems, and Variance Reduction},
  abstract = {In this paper, we study Jarzynski's equality and fluctuation theorems for diffusion processes. While some of the results considered in the current work are known in the (mainly physics) literature, we review and generalize these nonequilibrium theorems using mathematical arguments, therefore enabling further investigations in the mathematical community. On the numerical side, variance reduction approaches such as importance sampling method are studied in order to compute free energy differences based on Jarzynski's equality.},
  language = {en},
  number = {6},
  journal = {Journal of Statistical Physics},
  doi = {10.1007/s10955-019-02286-4},
  author = {Hartmann, Carsten and Schuette, Christof and Zhang, Wei},
  month = jun,
  year = {2019},
  keywords = {82C05; 60J60,Mathematical Physics},
  pages = {1214-1261},
  file = {/Users/lancelotdacosta/Zotero/storage/IDHR7YYN/Hartmann et al. - 2019 - Jarzynski's equality, fluctuation theorems, and va.pdf;/Users/lancelotdacosta/Zotero/storage/S3E8GP7A/Hartmann et al. - 2019 - Jarzynski's equality, fluctuation theorems, and va.pdf}
}

@article{bachmanArchitectureDeepHierarchical,
  title = {An {{Architecture}} for {{Deep}}, {{Hierarchical Generative Models}}},
  abstract = {We present an architecture which lets us train deep, directed generative models with many layers of latent variables. We include deterministic paths between all latent variables and the generated output, and provide a richer set of connections between computations for inference and generation, which enables more effective communication of information throughout the model during training. To improve performance on natural images, we incorporate a lightweight autoregressive model in the reconstruction distribution. These techniques permit end-to-end training of models with 10+ layers of latent variables. Experiments show that our approach achieves state-of-the-art performance on standard image modelling benchmarks, can expose latent class structure in the absence of label information, and can provide convincing imputations of occluded regions in natural images.},
  language = {en},
  author = {Bachman, Philip},
  pages = {9},
  file = {/Users/lancelotdacosta/Zotero/storage/269HRYJ3/Bachman - An Architecture for Deep, Hierarchical Generative .pdf;/Users/lancelotdacosta/Zotero/storage/3GW5DUIB/Bachman - An Architecture for Deep, Hierarchical Generative .pdf;/Users/lancelotdacosta/Zotero/storage/44KIEZZK/Bachman - An Architecture for Deep, Hierarchical Generative .pdf;/Users/lancelotdacosta/Zotero/storage/P6JAXW9V/Bachman - An Architecture for Deep, Hierarchical Generative .pdf}
}

@article{schwartenbeckComputationalMechanismsCuriosity2019,
  title = {Computational Mechanisms of Curiosity and Goal-Directed Exploration},
  language = {en},
  journal = {eLife},
  author = {Schwartenbeck, Philipp and Passecker, Johannes and Hauser, Tobias U and FitzGerald, Thomas HB and Kronbichler, Martin and Friston, Karl J},
  year = {2019},
  pages = {45},
  file = {/Users/lancelotdacosta/Zotero/storage/9T7RYQBV/Schwartenbeck et al. - Computational mechanisms of curiosity and goal-dir.pdf;/Users/lancelotdacosta/Zotero/storage/9WFMTHP9/Schwartenbeck et al. - Computational mechanisms of curiosity and goal-dir.pdf;/Users/lancelotdacosta/Zotero/storage/PQSLC3B9/Schwartenbeck et al. - Computational mechanisms of curiosity and goal-dir.pdf;/Users/lancelotdacosta/Zotero/storage/SPLWPGFI/Schwartenbeck et al. - Computational mechanisms of curiosity and goal-dir.pdf}
}

@article{bassettEfficientPhysicalEmbedding2010,
  title = {Efficient {{Physical Embedding}} of {{Topologically Complex Information Processing Networks}} in {{Brains}} and {{Computer Circuits}}},
  volume = {6},
  issn = {1553-7358},
  abstract = {Nervous systems are information processing networks that evolved by natural selection, whereas very large scale integrated (VLSI) computer circuits have evolved by commercially driven technology development. Here we follow historic intuition that all physical information processing systems will share key organizational properties, such as modularity, that generally confer adaptivity of function. It has long been observed that modular VLSI circuits demonstrate an isometric scaling relationship between the number of processing elements and the number of connections, known as Rent's rule, which is related to the dimensionality of the circuit's interconnect topology and its logical capacity. We show that human brain structural networks, and the nervous system of the nematode C. elegans, also obey Rent's rule, and exhibit some degree of hierarchical modularity. We further show that the estimated Rent exponent of human brain networks, derived from MRI data, can explain the allometric scaling relations between gray and white matter volumes across a wide range of mammalian species, again suggesting that these principles of nervous system design are highly conserved. For each of these fractal modular networks, the dimensionality of the interconnect topology was greater than the 2 or 3 Euclidean dimensions of the space in which it was embedded. This relatively high complexity entailed extra cost in physical wiring: although all networks were economically or cost-efficiently wired they did not strictly minimize wiring costs. Artificial and biological information processing systems both may evolve to optimize a trade-off between physical cost and topological complexity, resulting in the emergence of homologous principles of economical, fractal and modular design across many different kinds of nervous and computational networks.},
  language = {en},
  number = {4},
  journal = {PLoS Computational Biology},
  doi = {10.1371/journal.pcbi.1000748},
  author = {Bassett, Danielle S. and Greenfield, Daniel L. and {Meyer-Lindenberg}, Andreas and Weinberger, Daniel R. and Moore, Simon W. and Bullmore, Edward T.},
  editor = {Friston, Karl J.},
  month = apr,
  year = {2010},
  pages = {e1000748},
  file = {/Users/lancelotdacosta/Zotero/storage/VXKQ5JMB/Bassett et al. - 2010 - Efficient Physical Embedding of Topologically Comp.PDF}
}

@article{levyEnergyEfficientNeural1996,
  title = {Energy {{Efficient Neural Codes}}},
  volume = {8},
  issn = {0899-7667, 1530-888X},
  language = {en},
  number = {3},
  journal = {Neural Computation},
  doi = {10.1162/neco.1996.8.3.531},
  author = {Levy, William B and Baxter, Robert A.},
  month = apr,
  year = {1996},
  pages = {531-543},
  file = {/Users/lancelotdacosta/Zotero/storage/23ARIFRR/Levy and Baxter - 1996 - Energy Efficient Neural Codes.pdf;/Users/lancelotdacosta/Zotero/storage/62CZXZRE/Levy and Baxter - 1996 - Energy Efficient Neural Codes.pdf;/Users/lancelotdacosta/Zotero/storage/F8XN2ZKW/Levy and Baxter - 1996 - Energy Efficient Neural Codes.pdf;/Users/lancelotdacosta/Zotero/storage/SISKPY5L/Levy and Baxter - 1996 - Energy Efficient Neural Codes.pdf}
}

@article{mattiaPopulationDynamicsInteracting2002,
  title = {Population Dynamics of Interacting Spiking Neurons},
  volume = {66},
  issn = {1063-651X, 1095-3787},
  language = {en},
  number = {5},
  journal = {Physical Review E},
  doi = {10.1103/PhysRevE.66.051917},
  author = {Mattia, Maurizio and Del Giudice, Paolo},
  month = nov,
  year = {2002},
  pages = {051917},
  file = {/Users/lancelotdacosta/Zotero/storage/99S7PBJD/Mattia and Del Giudice - 2002 - Population dynamics of interacting spiking neurons.pdf;/Users/lancelotdacosta/Zotero/storage/DW6T6FGG/A Mathematical Theory of the Functional Dynamics of Cortical and Thalamic Nervous Tissue.pdf;/Users/lancelotdacosta/Zotero/storage/INPI873E/Mattia and Del Giudice - 2002 - Population dynamics of interacting spiking neurons.pdf;/Users/lancelotdacosta/Zotero/storage/RFQSB3JS/Mattia and Del Giudice - 2002 - Population dynamics of interacting spiking neurons.pdf;/Users/lancelotdacosta/Zotero/storage/WF7K5NVX/Mattia and Del Giudice - 2002 - Population dynamics of interacting spiking neurons.pdf}
}

@article{fristonPredictiveCodingFreeenergy2009,
  title = {Predictive Coding under the Free-Energy Principle},
  volume = {364},
  issn = {0962-8436, 1471-2970},
  language = {en},
  number = {1521},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  doi = {10.1098/rstb.2008.0300},
  author = {Friston, Karl and Kiebel, Stefan},
  month = may,
  year = {2009},
  keywords = {Computer Simulation,Humans,Birds,Animals,Brain,Models; Neurological,Perception,Concept Formation,Recognition (Psychology),Vocalization; Animal},
  pages = {1211-1221},
  file = {/Users/lancelotdacosta/Zotero/storage/9TWHW7W9/Friston and Kiebel - 2009 - Predictive coding under the free-energy principle.pdf;/Users/lancelotdacosta/Zotero/storage/GULJR5H2/Friston and Kiebel - 2009 - Predictive coding under the free-energy principle.pdf;/Users/lancelotdacosta/Zotero/storage/ISQQD7FB/Friston and Kiebel - 2009 - Predictive coding under the free-energy principle.pdf;/Users/lancelotdacosta/Zotero/storage/TLSMNSJ7/Friston and Kiebel - 2009 - Predictive coding under the free-energy principle.pdf;/Users/lancelotdacosta/Zotero/storage/W3CE99PP/Friston and Kiebel - 2009 - Predictive coding under the free-energy principle.pdf}
}

@article{rossBayesianApproachLearning,
  title = {A {{Bayesian Approach}} for {{Learning}} and {{Planning}} in {{Partially Observable Markov Decision Processes}}},
  abstract = {Bayesian learning methods have recently been shown to provide an elegant solution to the explorationexploitation trade-off in reinforcement learning. However most investigations of Bayesian reinforcement learning to date focus on the standard Markov Decision Processes (MDPs). The primary focus of this paper is to extend these ideas to the case of partially observable domains, by introducing the Bayes-Adaptive Partially Observable Markov Decision Processes. This new framework can be used to simultaneously (1) learn a model of the POMDP domain through interaction with the environment, (2) track the state of the system under partial observability, and (3) plan (near-)optimal sequences of actions. An important contribution of this paper is to provide theoretical results showing how the model can be finitely approximated while preserving good learning performance. We present approximate algorithms for belief tracking and planning in this model, as well as empirical results that illustrate how the model estimate and agent's return improve as a function of experience.},
  language = {en},
  author = {Ross, St{\'e}phane and Pineau, Joelle and {Chaib-draa}, Brahim and Kreitmann, Pierre},
  pages = {42},
  file = {/Users/lancelotdacosta/Zotero/storage/6WJA5ME3/Ross et al. - A Bayesian Approach for Learning and Planning in P.pdf;/Users/lancelotdacosta/Zotero/storage/AET8CXI8/Ross et al. - A Bayesian Approach for Learning and Planning in P.pdf;/Users/lancelotdacosta/Zotero/storage/DWW2GVQN/Ross et al. - A Bayesian Approach for Learning and Planning in P.pdf;/Users/lancelotdacosta/Zotero/storage/G4HPPMIQ/Ross et al. - A Bayesian Approach for Learning and Planning in P.pdf}
}

@article{tanakaTheoryMeanField1999,
  title = {A {{Theory}} of {{Mean Field Approximation}}},
  abstract = {I present a theory of mean field approximation based on information geometry. This theory includes in a consistent way the naive mean field approximation, as well as the TAP approach and the linear response theorem in statistical physics, giving clear information-theoretic interpretations to them.},
  language = {en},
  author = {Tanaka, Toshiyuki},
  year = {1999},
  pages = {10},
  file = {/Users/lancelotdacosta/Zotero/storage/3FAISIIY/Mapping of a non-spatial dimension by the hippocampalentorhinal circuit .pdf;/Users/lancelotdacosta/Zotero/storage/AKTBNTFX/Coding of position by simultaneously recorded sensory neurones in the cat dorsal root ganglion.pdf;/Users/lancelotdacosta/Zotero/storage/KZVDU32Q/The hippocampus as a predictive map.pdf;/Users/lancelotdacosta/Zotero/storage/M4QVCF2N/The hippocampus as a spatial map. Preliminary evidence from unit activity in the freely-moving rat..pdf;/Users/lancelotdacosta/Zotero/storage/MCYESDWU/Tanaka - A Theory of Mean Field Approximation.pdf;/Users/lancelotdacosta/Zotero/storage/MJYBD99X/Tanaka - A Theory of Mean Field Approximation.pdf;/Users/lancelotdacosta/Zotero/storage/SPBGH32S/Tanaka - A Theory of Mean Field Approximation.pdf;/Users/lancelotdacosta/Zotero/storage/T2L6BVQX/Microstructure of a spatial map in the entorhinal cortex.pdf;/Users/lancelotdacosta/Zotero/storage/YCNWR8RP/Decoding sensory feedback from firing rates of afferent ensembles recorded in cat dorsal root ganglia in normal locomotion.pdf;/Users/lancelotdacosta/Zotero/storage/ZDN4UD97/Tanaka - A Theory of Mean Field Approximation.pdf}
}

@article{zhangAdvancesVariationalInference2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.05597},
  primaryClass = {cs, stat},
  title = {Advances in {{Variational Inference}}},
  abstract = {Many modern unsupervised or semi-supervised machine learning algorithms rely on Bayesian probabilistic models. These models are usually intractable and thus require approximate inference. Variational inference (VI) lets us approximate a high-dimensional Bayesian posterior with a simpler variational distribution by solving an optimization problem. This approach has been successfully applied to various models and large-scale applications. In this review, we give an overview of recent trends in variational inference. We first introduce standard mean field variational inference, then review recent advances focusing on the following aspects: (a) scalable VI, which includes stochastic approximations, (b) generic VI, which extends the applicability of VI to a large class of otherwise intractable models, such as non-conjugate models, (c) accurate VI, which includes variational models beyond the mean field approximation or with atypical divergences, and (d) amortized VI, which implements the inference over local latent variables with inference networks. Finally, we provide a summary of promising future research directions.},
  language = {en},
  journal = {arXiv:1711.05597 [cs, stat]},
  author = {Zhang, Cheng and Butepage, Judith and Kjellstrom, Hedvig and Mandt, Stephan},
  month = nov,
  year = {2017},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/lancelotdacosta/Zotero/storage/GTYKH882/Zhang et al. - 2017 - Advances in Variational Inference.pdf}
}

@article{lewOVERVIEWVARIATIONALINTEGRATORS,
  title = {{{AN OVERVIEW OF VARIATIONAL INTEGRATORS}}},
  abstract = {The purpose of this paper is to survey some recent advances in variational integrators for both finite dimensional mechanical systems as well as continuum mechanics. These advances include the general development of discrete mechanics, applications to dissipative systems, collisions, spacetime integration algorithms, AVI's (Asynchronous Variational Integrators), as well as reduction for discrete mechanical systems. To keep the article within the set limits, we will only treat each topic briefly and will not attempt to develop any particular topic in any depth. We hope, nonetheless, that this paper serves as a useful guide to the literature as well as to future directions and open problems in the subject.},
  language = {en},
  journal = {Finite Element Methods},
  author = {Lew, Adrian and Marsden, Jerrold E and Ortiz, Michael and West, Matthew},
  pages = {18},
  file = {/Users/lancelotdacosta/Zotero/storage/AULDV5WJ/Lew et al. - AN OVERVIEW OF VARIATIONAL INTEGRATORS.pdf;/Users/lancelotdacosta/Zotero/storage/KTRAWZAN/Lew et al. - AN OVERVIEW OF VARIATIONAL INTEGRATORS.pdf;/Users/lancelotdacosta/Zotero/storage/M983X3SS/Lew et al. - AN OVERVIEW OF VARIATIONAL INTEGRATORS.pdf;/Users/lancelotdacosta/Zotero/storage/MVK2CRXT/Lew et al. - AN OVERVIEW OF VARIATIONAL INTEGRATORS.pdf}
}

@article{lakeBuildingMachinesThat2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1604.00289},
  primaryClass = {cs, stat},
  title = {Building {{Machines That Learn}} and {{Think Like People}}},
  abstract = {Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
  language = {en},
  journal = {arXiv:1604.00289 [cs, stat]},
  author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
  month = apr,
  year = {2016},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/lancelotdacosta/Zotero/storage/RVT9YFEE/Lake et al. - 2016 - Building Machines That Learn and Think Like People.pdf;/Users/lancelotdacosta/Zotero/storage/WDCM55W7/Lake et al. - 2016 - Building Machines That Learn and Think Like People.pdf;/Users/lancelotdacosta/Zotero/storage/YZ247FTG/Lake et al. - 2016 - Building Machines That Learn and Think Like People.pdf;/Users/lancelotdacosta/Zotero/storage/Z5ZUTGD7/Lake et al. - 2016 - Building Machines That Learn and Think Like People.pdf}
}

@article{lauritzenCausalInferenceGraphical,
  title = {Causal {{Inference}} from {{Graphical Models}} - {{I}}},
  language = {en},
  author = {Lauritzen, Steffen},
  pages = {28},
  file = {/Users/lancelotdacosta/Zotero/storage/7K7LZYA3/Lauritzen - Causal Inference from Graphical Models - I.pdf;/Users/lancelotdacosta/Zotero/storage/K6JTGBFH/Lauritzen - Causal Inference from Graphical Models - I.pdf;/Users/lancelotdacosta/Zotero/storage/U25FPRTP/Lauritzen - Causal Inference from Graphical Models - I.pdf;/Users/lancelotdacosta/Zotero/storage/YQ2555QY/Lauritzen - Causal Inference from Graphical Models - I.pdf}
}

@article{colomboFirstPrinciplesLife2018,
  title = {First Principles in the Life Sciences: The Free-Energy Principle, Organicism, and Mechanism},
  issn = {0039-7857, 1573-0964},
  shorttitle = {First Principles in the Life Sciences},
  abstract = {The free-energy principle states that all systems that minimize their free energy resist a tendency to physical disintegration. Originally proposed to account for perception, learning, and action, the free-energy principle has been applied to the evolution, development, morphology, anatomy and function of the brain, and has been called a postulate, an unfalsifiable principle, a natural law, and an imperative. While it might afford a theoretical foundation for understanding the relationship between environment, life, and mind, its epistemic status is unclear. Also unclear is how the free-energy principle relates to prominent theoretical approaches to life science phenomena, such as organicism and mechanism. This paper clarifies both issues, and identifies limits and prospects for the free-energy principle as a first principle in the life sciences.},
  language = {en},
  journal = {Synthese},
  doi = {10.1007/s11229-018-01932-w},
  author = {Colombo, Matteo and Wright, Cory},
  month = sep,
  year = {2018},
  file = {/Users/lancelotdacosta/Zotero/storage/E5T34PZY/Colombo and Wright - 2018 - First principles in the life sciences the free-en.pdf}
}

@article{fristonFreeenergyBrain2007,
  title = {Free-Energy and the Brain},
  volume = {159},
  issn = {0039-7857, 1573-0964},
  abstract = {If one formulates Helmholtz's ideas about perception in terms of modern-day theories one arrives at a model of perceptual inference and learning that can explain a remarkable range of neurobiological facts. Using constructs from statistical physics it can be shown that the problems of inferring what cause our sensory inputs and learning causal regularities in the sensorium can be resolved using exactly the same principles. Furthermore, inference and learning can proceed in a biologically plausible fashion. The ensuing scheme rests on Empirical Bayes and hierarchical models of how sensory information is generated. The use of hierarchical models enables the brain to construct prior expectations in a dynamic and context-sensitive fashion. This scheme provides a principled way to understand many aspects of the brain's organisation and responses. In this paper, we suggest that these perceptual processes are just one emergent property of systems that conform to a free-energy principle. The free-energy considered here represents a bound on the surprise inherent in any exchange with the environment, under expectations encoded by its state or configuration. A system can minimise free-energy by changing its configuration to change the way it samples the environment, or to change its expectations. These changes correspond to action and perception, respectively, and lead to an adaptive exchange with the environment that is characteristic of biological systems. This treatment implies that the system's state and structure encode an implicit and probabilistic model of the environment. We will look at models entailed by the brain and how minimisation of free-energy can explain its dynamics and structure.},
  language = {en},
  number = {3},
  journal = {Synthese},
  doi = {10.1007/s11229-007-9237-y},
  author = {Friston, Karl J. and Stephan, Klaas E.},
  month = nov,
  year = {2007},
  pages = {417-458},
  file = {/Users/lancelotdacosta/Zotero/storage/PR3DSQPC/Friston and Stephan - 2007 - Free-energy and the brain.pdf;/Users/lancelotdacosta/Zotero/storage/RGN29H9H/Friston and Stephan - 2007 - Free-energy and the brain.pdf;/Users/lancelotdacosta/Zotero/storage/UQRKJ8FA/Friston and Stephan - 2007 - Free-energy and the brain.pdf;/Users/lancelotdacosta/Zotero/storage/Y4LSYPGU/Friston and Stephan - 2007 - Free-energy and the brain.pdf}
}

@article{fristonFunctionalEffectiveConnectivity2011,
  title = {Functional and {{Effective Connectivity}}: {{A Review}}},
  volume = {1},
  issn = {2158-0014, 2158-0022},
  shorttitle = {Functional and {{Effective Connectivity}}},
  abstract = {Over the past 20 years, neuroimaging has become a predominant technique in systems neuroscience. One might envisage that over the next 20 years the neuroimaging of distributed processing and connectivity will play a major role in disclosing the brain's functional architecture and operational principles. The inception of this journal has been foreshadowed by an ever-increasing number of publications on functional connectivity, causal modeling, connectomics, and multivariate analyses of distributed patterns of brain responses. I accepted the invitation to write this review with great pleasure and hope to celebrate and critique the achievements to date, while addressing the challenges ahead.},
  language = {en},
  number = {1},
  journal = {Brain Connectivity},
  doi = {10.1089/brain.2011.0008},
  author = {Friston, Karl J.},
  month = jan,
  year = {2011},
  pages = {13-36},
  file = {/Users/lancelotdacosta/Zotero/storage/AHKBLFGD/Friston - 2011 - Functional and Effective Connectivity A Review.pdf;/Users/lancelotdacosta/Zotero/storage/F8R8KHS7/Friston - 2011 - Functional and Effective Connectivity A Review.pdf;/Users/lancelotdacosta/Zotero/storage/MXLHWA2G/Friston - 2011 - Functional and Effective Connectivity A Review.pdf;/Users/lancelotdacosta/Zotero/storage/R6WCEH7D/Friston - 2011 - Functional and Effective Connectivity A Review.pdf}
}

@article{wainwrightGraphicalModelsExponential2007,
  title = {Graphical {{Models}}, {{Exponential Families}}, and {{Variational Inference}}},
  volume = {1},
  issn = {1935-8237, 1935-8245},
  language = {en},
  number = {1\textendash{}2},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  doi = {10.1561/2200000001},
  author = {Wainwright, Martin J. and Jordan, Michael I.},
  year = {2007},
  pages = {1-305},
  file = {/Users/lancelotdacosta/Zotero/storage/DY2V937Q/Wainwright and Jordan - 2007 - Graphical Models, Exponential Families, and Variat.pdf;/Users/lancelotdacosta/Zotero/storage/PVPT7G5V/Wainwright and Jordan - 2007 - Graphical Models, Exponential Families, and Variat.pdf;/Users/lancelotdacosta/Zotero/storage/UDINF2RK/Wainwright and Jordan - 2007 - Graphical Models, Exponential Families, and Variat.pdf;/Users/lancelotdacosta/Zotero/storage/XRNSQAEZ/Wainwright and Jordan - 2007 - Graphical Models, Exponential Families, and Variat.pdf}
}

@article{bunneLearningGenerativeModels,
  title = {Learning {{Generative Models Across Incomparable Spaces}}},
  abstract = {Adversarial training has become the de facto standard for generative modeling. While adversarial approaches have shown remarkable success in learning a distribution that faithfully recovers a reference distribution in its entirety, they are not applicable when one wishes the generated distribution to recover some \textemdash{}but not all\textemdash{} aspects of it. For example, one might be interested in modeling purely relational or topological aspects (such as cluster or manifold structure) while ignoring or constraining absolute characteristics (e.g., global orientation in Euclidean spaces). Furthermore, such absolute aspects are not available if the data is provided in an intrinsically relational form, such as a weighted graph. In this work, we propose an approach to learn generative models across such incomparable spaces that relies on the Gromov-Wasserstein distance, a notion of discrepancy that compares distributions relationally rather than absolutely. We show how the resulting framework can be used to learn distributions across spaces of different dimensionality or even different data types.},
  language = {en},
  author = {Bunne, Charlotte and {Alvarez-Melis}, David and Krause, Andreas and Jegelka, Stefanie},
  pages = {6},
  file = {/Users/lancelotdacosta/Zotero/storage/AHEUL8I8/Bunne et al. - Learning Generative Models Across Incomparable Spa.pdf}
}

@inproceedings{dauwelsVariationalMessagePassing2007,
  address = {{Nice}},
  title = {On {{Variational Message Passing}} on {{Factor Graphs}}},
  isbn = {978-1-4244-1397-3},
  abstract = {In this paper, it is shown how (naive and structured) variational algorithms may be derived from a factor graph by mechanically applying generic message computation rules; in this way, one can bypass error-prone variational calculus. In prior work by Bishop et al., Xing et al., and Geiger, directed and undirected graphical models have been used for this purpose. The factor graph notation amounts to simpler generic variational message computation rules; by means of factor graphs, variational methods can straightforwardly be compared to and combined with various other message-passing inference algorithms, e.g., Kalman filters and smoothers, iterated conditional modes, expectation maximization (EM), gradient methods, and particle filters. Some of those combinations have been explored in the literature, others seem to be new. Generic message computation rules for such combinations are formulated.},
  language = {en},
  booktitle = {2007 {{IEEE International Symposium}} on {{Information Theory}}},
  publisher = {{IEEE}},
  doi = {10.1109/ISIT.2007.4557602},
  author = {Dauwels, Justin},
  month = jun,
  year = {2007},
  pages = {2546-2550},
  file = {/Users/lancelotdacosta/Zotero/storage/6ZG3K9BX/Dauwels - 2007 - On Variational Message Passing on Factor Graphs.pdf;/Users/lancelotdacosta/Zotero/storage/BIGWTB63/Dauwels - 2007 - On Variational Message Passing on Factor Graphs.pdf;/Users/lancelotdacosta/Zotero/storage/KN6TPU9C/Dauwels - 2007 - On Variational Message Passing on Factor Graphs.pdf;/Users/lancelotdacosta/Zotero/storage/TDEN9ZX9/Dauwels - 2007 - On Variational Message Passing on Factor Graphs.pdf}
}

@article{rezendeStochasticBackpropagationApproximate2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1401.4082},
  primaryClass = {cs, stat},
  title = {Stochastic {{Backpropagation}} and {{Approximate Inference}} in {{Deep Generative Models}}},
  abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound. We develop stochastic backpropagation \textendash{} rules for gradient backpropagation through stochastic variables \textendash{} and derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models. We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.},
  language = {en},
  journal = {arXiv:1401.4082 [cs, stat]},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  month = jan,
  year = {2014},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence,Statistics - Computation,Statistics - Methodology},
  file = {/Users/lancelotdacosta/Zotero/storage/AWV9ICAN/Rezende et al. - 2014 - Stochastic Backpropagation and Approximate Inferen.pdf;/Users/lancelotdacosta/Zotero/storage/JPSF53VN/Rezende et al. - 2014 - Stochastic Backpropagation and Approximate Inferen.pdf;/Users/lancelotdacosta/Zotero/storage/NGLZKDAI/Rezende et al. - 2014 - Stochastic Backpropagation and Approximate Inferen.pdf;/Users/lancelotdacosta/Zotero/storage/Q86KUBBW/Rezende et al. - 2014 - Stochastic Backpropagation and Approximate Inferen.pdf}
}

@article{degroffStochasticalAspectsNeuronal1993,
  title = {Stochastical Aspects of Neuronal Dynamics: {{Fokker}}-{{Planck}} Approach},
  volume = {69},
  issn = {0340-1200, 1432-0770},
  shorttitle = {Stochastical Aspects of Neuronal Dynamics},
  abstract = {The stochastical aspects of noise-perturbed neuronal dynamics are studied via the Fokker-Planck equation by considering the Langevin-type relaxational, nonlinear process associated with neuronal states. On the basis of a canonical, stochastically driven, dichotomous state modeling, the equilibrium conditions in the neuronal assembly are analyzed. The markovian structure of the random occurrence of action potentials due to the disturbances (noise) in the neuronal state is considered, and the corresponding solutions relevant to the colored noise spectrum of the disturbance effects are addressed. Stochastical instability (Lyapunov) considerations in solving discrete optimization problems via neural networks are discussed. The bounded estimate(s) of the stochastical variates involved are presented, and the noise-induced perturbations on the saturated-state neuronal population are elucidated.},
  language = {en},
  number = {2},
  journal = {Biological Cybernetics},
  doi = {10.1007/BF00226199},
  author = {De Groff, D. and Neelakanta, P. S. and Sudhakar, R. and Aalo, V.},
  month = jun,
  year = {1993},
  pages = {155-164},
  file = {/Users/lancelotdacosta/Zotero/storage/GA7LADNT/De Groff et al. - 1993 - Stochastical aspects of neuronal dynamics Fokker-.pdf}
}

@article{fristonFreeenergyPrincipleRough2009,
  title = {The Free-Energy Principle: A Rough Guide to the Brain?},
  volume = {13},
  issn = {13646613},
  shorttitle = {The Free-Energy Principle},
  language = {en},
  number = {7},
  journal = {Trends in Cognitive Sciences},
  doi = {10.1016/j.tics.2009.04.005},
  author = {Friston, Karl},
  month = jul,
  year = {2009},
  pages = {293-301},
  file = {/Users/lancelotdacosta/Zotero/storage/E9V6U58W/Friston - 2009 - The free-energy principle a rough guide to the br.pdf;/Users/lancelotdacosta/Zotero/storage/GQ42455W/Friston - 2009 - The free-energy principle a rough guide to the br.pdf;/Users/lancelotdacosta/Zotero/storage/L7NAKY69/Friston - 2009 - The free-energy principle a rough guide to the br.pdf;/Users/lancelotdacosta/Zotero/storage/PKJZT9LN/Friston - 2009 - The free-energy principle a rough guide to the br.pdf}
}

@article{crickNeurobiologicalTheoryConsciousness,
  title = {Towards a Neurobiological Theory of Consciousness},
  language = {en},
  author = {Crick, Francis},
  pages = {13},
  file = {/Users/lancelotdacosta/Zotero/storage/N8XZPA3M/Crick - Towards a neurobiological theory of consciousness.pdf}
}

@article{linWhyDoesDeep2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1608.08225},
  title = {Why Does Deep and Cheap Learning Work so Well?},
  volume = {168},
  issn = {0022-4715, 1572-9613},
  abstract = {We show how the success of deep learning could depend not only on mathematics but also on physics: although well-known mathematical theorems guarantee that neural networks can approximate arbitrary functions well, the class of functions of practical interest can frequently be approximated through "cheap learning" with exponentially fewer parameters than generic ones. We explore how properties frequently encountered in physics such as symmetry, locality, compositionality, and polynomial log-probability translate into exceptionally simple neural networks. We further argue that when the statistical process generating the data is of a certain hierarchical form prevalent in physics and machine-learning, a deep neural network can be more efficient than a shallow one. We formalize these claims using information theory and discuss the relation to the renormalization group. We prove various "no-flattening theorems" showing when efficient linear deep networks cannot be accurately approximated by shallow ones without efficiency loss, for example, we show that \$n\$ variables cannot be multiplied using fewer than 2\^n neurons in a single hidden layer.},
  language = {en},
  number = {6},
  journal = {Journal of Statistical Physics},
  doi = {10.1007/s10955-017-1836-5},
  author = {Lin, Henry W. and Tegmark, Max and Rolnick, David},
  month = sep,
  year = {2017},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks},
  pages = {1223-1247},
  file = {/Users/lancelotdacosta/Zotero/storage/3L2V8STP/Lin et al. - 2017 - Why does deep and cheap learning work so well.pdf;/Users/lancelotdacosta/Zotero/storage/4DF2K659/Lin et al. - 2017 - Why does deep and cheap learning work so well.pdf;/Users/lancelotdacosta/Zotero/storage/MD2BPNT2/Lin et al. - 2017 - Why does deep and cheap learning work so well.pdf;/Users/lancelotdacosta/Zotero/storage/UE2K5YWD/Lin et al. - 2017 - Why does deep and cheap learning work so well.pdf}
}

@article{al-bashabshehFactorGraphApproachAlgebraic2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1607.02361},
  title = {A {{Factor}}-{{Graph Approach}} to {{Algebraic Topology}}, {{With Applications}} to {{Kramers}}--{{Wannier Duality}}},
  volume = {64},
  issn = {0018-9448, 1557-9654},
  abstract = {Algebraic topology studies topological spaces with the help of tools from abstract algebra. The main focus of this paper is to show that many concepts from algebraic topology can be conveniently expressed in terms of (normal) factor graphs. As an application, we give an alternative proof of a classical duality result of Kramers and Wannier, which expresses the partition function of the two-dimensional Ising model at a low temperature in terms of the partition function of the two-dimensional Ising model at a high temperature. Moreover, we discuss analogous results for the three-dimensional Ising model and the Potts model.},
  language = {en},
  number = {12},
  journal = {IEEE Transactions on Information Theory},
  doi = {10.1109/TIT.2018.2837878},
  author = {{Al-Bashabsheh}, Ali and Vontobel, Pascal O.},
  month = dec,
  year = {2018},
  keywords = {Computer Science - Information Theory},
  pages = {7488-7510},
  file = {/Users/lancelotdacosta/Zotero/storage/3FFIEHXW/Al-Bashabsheh and Vontobel - 2018 - A Factor-Graph Approach to Algebraic Topology, Wit.pdf}
}

@article{heskesConvexityArgumentsEfficient2006,
  title = {Convexity {{Arguments}} for {{Efficient Minimization}} of the {{Bethe}} and {{Kikuchi Free Energies}}},
  volume = {26},
  issn = {1076-9757},
  abstract = {Loopy and generalized belief propagation are popular algorithms for approximate inference in Markov random fields and Bayesian networks. Fixed points of these algorithms have been shown to correspond to extrema of the Bethe and Kikuchi free energy, both of which are approximations of the exact Helmholtz free energy. However, belief propagation does not always converge, which motivates approaches that explicitly minimize the Kikuchi/Bethe free energy, such as CCCP and UPS.},
  language = {en},
  journal = {Journal of Artificial Intelligence Research},
  doi = {10.1613/jair.1933},
  author = {Heskes, T.},
  month = jun,
  year = {2006},
  pages = {153-190},
  file = {/Users/lancelotdacosta/Zotero/storage/284VYEST/Heskes - 2006 - Convexity Arguments for Efficient Minimization of .pdf;/Users/lancelotdacosta/Zotero/storage/6QVSAHXC/Heskes - 2006 - Convexity Arguments for Efficient Minimization of .pdf;/Users/lancelotdacosta/Zotero/storage/7R53MMS3/Heskes - 2006 - Convexity Arguments for Efficient Minimization of .pdf;/Users/lancelotdacosta/Zotero/storage/AV5HUYYT/Heskes - 2006 - Convexity Arguments for Efficient Minimization of .pdf}
}

@article{biehlExpandingActiveInference2018,
  title = {Expanding the {{Active Inference Landscape}}: {{More Intrinsic Motivations}} in the {{Perception}}-{{Action Loop}}},
  volume = {12},
  issn = {1662-5218},
  shorttitle = {Expanding the {{Active Inference Landscape}}},
  abstract = {Active inference is an ambitious theory that treats perception, inference, and action selection of autonomous agents under the heading of a single principle. It suggests biologically plausible explanations for many cognitive phenomena, including consciousness. In active inference, action selection is driven by an objective function that evaluates possible future actions with respect to current, inferred beliefs about the world. Active inference at its core is independent from extrinsic rewards, resulting in a high level of robustness across e.g., different environments or agent morphologies. In the literature, paradigms that share this independence have been summarized under the notion of intrinsic motivations. In general and in contrast to active inference, these models of motivation come without a commitment to particular inference and action selection mechanisms. In this article, we study if the inference and action selection machinery of active inference can also be used by alternatives to the originally included intrinsic motivation. The perception-action loop explicitly relates inference and action selection to the environment and agent memory, and is consequently used as foundation for our analysis. We reconstruct the active inference approach, locate the original formulation within, and show how alternative intrinsic motivations can be used while keeping many of the original features intact. Furthermore, we illustrate the connection to universal reinforcement learning by means of our formalism. Active inference research may profit from comparisons of the dynamics induced by alternative intrinsic motivations. Research on intrinsic motivations may profit from an additional way to implement intrinsically motivated agents that also share the biological plausibility of active inference.},
  language = {en},
  journal = {Frontiers in Neurorobotics},
  doi = {10.3389/fnbot.2018.00045},
  author = {Biehl, Martin and Guckelsberger, Christian and Salge, Christoph and Smith, Sim{\'o}n C. and Polani, Daniel},
  month = aug,
  year = {2018},
  pages = {45},
  file = {/Users/lancelotdacosta/Zotero/storage/TZRFCYET/Biehl et al. - 2018 - Expanding the Active Inference Landscape More Int.pdf}
}

@article{yangGeodesicClusteringDeep2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1809.04747},
  primaryClass = {cs, stat},
  title = {Geodesic {{Clustering}} in {{Deep Generative Models}}},
  abstract = {Deep generative models are tremendously successful in learning low-dimensional latent representations that welldescribe the data. These representations, however, tend to much distort relationships between points, i.e. pairwise distances tend to not reflect semantic similarities well. This renders unsupervised tasks, such as clustering, difficult when working with the latent representations. We demonstrate that taking the geometry of the generative model into account is sufficient to make simple clustering algorithms work well over latent representations. Leaning on the recent finding that deep generative models constitute stochastically immersed Riemannian manifolds, we propose an efficient algorithm for computing geodesics (shortest paths) and computing distances in the latent space, while taking its distortion into account. We further propose a new architecture for modeling uncertainty in variational autoencoders, which is essential for understanding the geometry of deep generative models. Experiments show that the geodesic distance is very likely to reflect the internal structure of the data.},
  language = {en},
  journal = {arXiv:1809.04747 [cs, stat]},
  author = {Yang, Tao and Arvanitidis, Georgios and Fu, Dongmei and Li, Xiaogang and Hauberg, S{\o}ren},
  month = sep,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/lancelotdacosta/Zotero/storage/K26DU3U4/Yang et al. - 2018 - Geodesic Clustering in Deep Generative Models.pdf}
}

@article{wangLandscapeFluxTheory2015,
  title = {Landscape and Flux Theory of Non-Equilibrium Dynamical Systems with Application to Biology},
  volume = {64},
  issn = {0001-8732, 1460-6976},
  language = {en},
  number = {1},
  journal = {Advances in Physics},
  doi = {10.1080/00018732.2015.1037068},
  author = {Wang, Jin},
  month = jan,
  year = {2015},
  pages = {1-137},
  file = {/Users/lancelotdacosta/Zotero/storage/DMT3DVK8/Wang - 2015 - Landscape and flux theory of non-equilibrium dynam.pdf;/Users/lancelotdacosta/Zotero/storage/PWWXBJNH/Wang - 2015 - Landscape and flux theory of non-equilibrium dynam.pdf;/Users/lancelotdacosta/Zotero/storage/VH843YUG/Wang - 2015 - Landscape and flux theory of non-equilibrium dynam.pdf;/Users/lancelotdacosta/Zotero/storage/WLJMEQS5/Wang - 2015 - Landscape and flux theory of non-equilibrium dynam.pdf}
}

@book{gerstnerNeuronalDynamicsSingle2014,
  address = {{Cambridge}},
  title = {Neuronal {{Dynamics}}: {{From Single Neurons}} to {{Networks}} and {{Models}} of {{Cognition}}},
  isbn = {978-1-107-44761-5},
  shorttitle = {Neuronal {{Dynamics}}},
  language = {en},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/CBO9781107447615},
  author = {Gerstner, Wulfram and Kistler, Werner M. and Naud, Richard and Paninski, Liam},
  year = {2014},
  file = {/Users/lancelotdacosta/Zotero/storage/6HUGWAZA/Gerstner et al. - 2014 - Neuronal Dynamics From Single Neurons to Networks.pdf;/Users/lancelotdacosta/Zotero/storage/BD9E8AW6/Gerstner et al. - 2014 - Neuronal Dynamics From Single Neurons to Networks.pdf;/Users/lancelotdacosta/Zotero/storage/D7GVLDVN/Gerstner et al. - 2014 - Neuronal Dynamics From Single Neurons to Networks.pdf;/Users/lancelotdacosta/Zotero/storage/N3DQRSHX/Gerstner et al. - 2014 - Neuronal Dynamics From Single Neurons to Networks.pdf}
}

@article{yanNonequilibriumLandscapeTheory2013,
  title = {Nonequilibrium Landscape Theory of Neural Networks},
  volume = {110},
  issn = {0027-8424, 1091-6490},
  language = {en},
  number = {45},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1310692110},
  author = {Yan, H. and Zhao, L. and Hu, L. and Wang, X. and Wang, E. and Wang, J.},
  month = nov,
  year = {2013},
  pages = {E4185-E4194},
  file = {/Users/lancelotdacosta/Zotero/storage/4UE5KBNP/Yan et al. - 2013 - Nonequilibrium landscape theory of neural networks.pdf;/Users/lancelotdacosta/Zotero/storage/ACSKI4CC/Yan et al. - 2013 - Nonequilibrium landscape theory of neural networks.pdf;/Users/lancelotdacosta/Zotero/storage/EKCWZJIM/Yan et al. - 2013 - Nonequilibrium landscape theory of neural networks.pdf;/Users/lancelotdacosta/Zotero/storage/FB92FNTJ/Yan et al. - 2013 - Nonequilibrium landscape theory of neural networks.pdf}
}

@article{fristonPostHocBayesian2011,
  title = {Post Hoc {{Bayesian}} Model Selection},
  volume = {56},
  issn = {10538119},
  abstract = {This note describes a Bayesian model selection or optimization procedure for post hoc inferences about reduced versions of a full model. The scheme provides the evidence (marginal likelihood) for any reduced model as a function of the posterior density over the parameters of the full model. It rests upon specifying models through priors on their parameters, under the assumption that the likelihood remains the same for all models considered. This provides a quick and efficient scheme for scoring arbitrarily large numbers of models, after inverting a single (full) model. In turn, this enables the selection among discrete models that are distinguished by the presence or absence of free parameters, where free parameters are effectively removed from the model using very precise shrinkage priors. An alternative application of this post hoc model selection considers continuous model spaces, defined in terms of hyperparameters (sufficient statistics) of the prior density over model parameters. In this instance, the prior (model) can be optimized with respect to its evidence. The expressions for model evidence become remarkably simple under the Laplace (Gaussian) approximation to the posterior density. Special cases of this scheme include Savage\textendash{}Dickey density ratio tests for reduced models and automatic relevance determination in model optimization. We illustrate the approach using general linear models and a more complicated nonlinear state-space model.},
  language = {en},
  number = {4},
  journal = {NeuroImage},
  doi = {10.1016/j.neuroimage.2011.03.062},
  author = {Friston, Karl and Penny, Will},
  month = jun,
  year = {2011},
  pages = {2089-2099},
  file = {/Users/lancelotdacosta/Zotero/storage/75IKBW5Q/Friston and Penny - 2011 - Post hoc Bayesian model selection.pdf}
}

@article{ikedaStochasticReasoningFree2004,
  title = {Stochastic {{Reasoning}}, {{Free Energy}}, and {{Information Geometry}}},
  volume = {16},
  issn = {0899-7667, 1530-888X},
  language = {en},
  number = {9},
  journal = {Neural Computation},
  doi = {10.1162/0899766041336477},
  author = {Ikeda, Shiro and Tanaka, Toshiyuki and Amari, Shun-ichi},
  month = sep,
  year = {2004},
  pages = {1779-1810},
  file = {/Users/lancelotdacosta/Zotero/storage/DXIF8VV9/Ikeda et al. - 2004 - Stochastic Reasoning, Free Energy, and Information.pdf;/Users/lancelotdacosta/Zotero/storage/EL7SGZTE/Ikeda et al. - 2004 - Stochastic Reasoning, Free Energy, and Information.pdf;/Users/lancelotdacosta/Zotero/storage/IW45JHYL/Ikeda et al. - 2004 - Stochastic Reasoning, Free Energy, and Information.pdf;/Users/lancelotdacosta/Zotero/storage/Z69M37ZU/Ikeda et al. - 2004 - Stochastic Reasoning, Free Energy, and Information.pdf}
}

@article{aitchisonHamiltonianBrainEfficient2016,
  title = {The {{Hamiltonian Brain}}: {{Efficient Probabilistic Inference}} with {{Excitatory}}-{{Inhibitory Neural Circuit Dynamics}}},
  volume = {12},
  issn = {1553-7358},
  shorttitle = {The {{Hamiltonian Brain}}},
  language = {en},
  number = {12},
  journal = {PLOS Computational Biology},
  doi = {10.1371/journal.pcbi.1005186},
  author = {Aitchison, Laurence and Lengyel, M{\'a}t{\'e}},
  editor = {Kording, Konrad P.},
  month = dec,
  year = {2016},
  pages = {e1005186},
  file = {/Users/lancelotdacosta/Zotero/storage/5QRP36DZ/Aitchison and Lengyel - 2016 - The Hamiltonian Brain Efficient Probabilistic Inf.pdf}
}

@article{willifordProjectiveConsciousnessModel2018,
  title = {The {{Projective Consciousness Model}} and {{Phenomenal Selfhood}}},
  volume = {9},
  issn = {1664-1078},
  language = {en},
  journal = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2018.02571},
  author = {Williford, Kenneth and Bennequin, Daniel and Friston, Karl and Rudrauf, David},
  month = dec,
  year = {2018},
  pages = {2571},
  file = {/Users/lancelotdacosta/Zotero/storage/299YM846/Williford et al. - 2018 - The Projective Consciousness Model and Phenomenal .pdf;/Users/lancelotdacosta/Zotero/storage/3PTHCN82/Williford et al. - 2018 - The Projective Consciousness Model and Phenomenal .pdf;/Users/lancelotdacosta/Zotero/storage/DZRLDFIG/Williford et al. - 2018 - The Projective Consciousness Model and Phenomenal .pdf;/Users/lancelotdacosta/Zotero/storage/ZIQNCZFX/Williford et al. - 2018 - The Projective Consciousness Model and Phenomenal .pdf}
}

@article{bleiVariationalInferenceReview2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1601.00670},
  title = {Variational {{Inference}}: {{A Review}} for {{Statisticians}}},
  volume = {112},
  issn = {0162-1459, 1537-274X},
  shorttitle = {Variational {{Inference}}},
  abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
  language = {en},
  number = {518},
  journal = {Journal of the American Statistical Association},
  doi = {10.1080/01621459.2017.1285773},
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  month = apr,
  year = {2017},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Computation},
  pages = {859-877},
  file = {/Users/lancelotdacosta/Zotero/storage/YU6T9LK4/Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf}
}

@article{winnVariationalMessagePassing2005,
  title = {Variational {{Message Passing}}},
  abstract = {Bayesian inference is now widely established as one of the principal foundations for machine learning. In practice, exact inference is rarely possible, and so a variety of approximation techniques have been developed, one of the most widely used being a deterministic framework called variational inference. In this paper we introduce Variational Message Passing (VMP), a general purpose algorithm for applying variational inference to Bayesian Networks. Like belief propagation, VMP proceeds by sending messages between nodes in the network and updating posterior beliefs using local operations at each node. Each such update increases a lower bound on the log evidence (unless already at a local maximum). In contrast to belief propagation, VMP can be applied to a very general class of conjugate-exponential models because it uses a factorised variational approximation. Furthermore, by introducing additional variational parameters, VMP can be applied to models containing non-conjugate distributions. The VMP framework also allows the lower bound to be evaluated, and this can be used both for model comparison and for detection of convergence. Variational message passing has been implemented in the form of a general purpose inference engine called VIBES (`Variational Inference for BayEsian networkS') which allows models to be specified graphically and then solved variationally without recourse to coding.},
  language = {en},
  journal = {Journal of Machine Learning Research},
  author = {Winn, John and Bishop, Christopher M},
  year = {2005},
  pages = {34},
  file = {/Users/lancelotdacosta/Zotero/storage/AXZHEEWD/Winn and Bishop - Variational Message Passing.pdf;/Users/lancelotdacosta/Zotero/storage/JVXTA9MK/Winn and Bishop - Variational Message Passing.pdf;/Users/lancelotdacosta/Zotero/storage/WHDXMB3V/Winn and Bishop - Variational Message Passing.pdf;/Users/lancelotdacosta/Zotero/storage/YCBHZMDS/Winn and Bishop - Variational Message Passing.pdf}
}

@article{fristonWhatValueAccumulated2012,
  title = {What Is Value\textemdash{}Accumulated Reward or Evidence?},
  volume = {6},
  issn = {1662-5218},
  abstract = {Why are you reading this abstract? In some sense, your answer will cast the exercise as valuable\textemdash{}but what is value? In what follows, we suggest that value is evidence or, more exactly, log Bayesian evidence. This implies that a sufficient explanation for valuable behavior is the accumulation of evidence for internal models of our world. This contrasts with normative models of optimal control and reinforcement learning, which assume the existence of a value function that explains behavior, where (somewhat tautologically) behavior maximizes value. In this paper, we consider an alternative formulation\textemdash{}active inference\textemdash{}that replaces policies in normative models with prior beliefs about the (future) states agents should occupy. This enables optimal behavior to be cast purely in terms of inference: where agents sample their sensorium to maximize the evidence for their generative model of hidden states in the world, and minimize their uncertainty about those states. Crucially, this formulation resolves the tautology inherent in normative models and allows one to consider how prior beliefs are themselves optimized in a hierarchical setting. We illustrate these points by showing that any optimal policy can be specified with prior beliefs in the context of Bayesian inference. We then show how these prior beliefs are themselves prescribed by an imperative to minimize uncertainty. This formulation explains the saccadic eye movements required to read this text and defines the value of the visual sensations you are soliciting.},
  language = {en},
  journal = {Frontiers in Neurorobotics},
  doi = {10.3389/fnbot.2012.00011},
  author = {Friston, Karl and Adams, Rick and Montague, Read},
  year = {2012},
  file = {/Users/lancelotdacosta/Zotero/storage/9223PP95/Friston et al. - 2012 - What is value—accumulated reward or evidence.pdf;/Users/lancelotdacosta/Zotero/storage/BK4CQR9R/Friston et al. - 2012 - What is value—accumulated reward or evidence.pdf;/Users/lancelotdacosta/Zotero/storage/GMXNIVUB/Friston et al. - 2012 - What is value—accumulated reward or evidence.pdf;/Users/lancelotdacosta/Zotero/storage/XS3UQA5G/Friston et al. - 2012 - What is value—accumulated reward or evidence.pdf}
}

@article{nielsenElementaryIntroductionInformation2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1808.08271},
  primaryClass = {cs, math, stat},
  title = {An Elementary Introduction to Information Geometry},
  abstract = {We describe the fundamental differential-geometric structures of information manifolds, state the fundamental theorem of information geometry, and illustrate some uses of these information manifolds in information sciences. The exposition is self-contained by concisely introducing the necessary concepts of differential geometry with proofs omitted for brevity.},
  language = {en},
  journal = {arXiv:1808.08271 [cs, math, stat]},
  author = {Nielsen, Frank},
  month = aug,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Information Theory},
  file = {/Users/lancelotdacosta/Zotero/storage/9BXKD6PG/Nielsen - 2018 - An elementary introduction to information geometry.pdf;/Users/lancelotdacosta/Zotero/storage/I427PS3J/Nielsen - 2018 - An elementary introduction to information geometry.pdf;/Users/lancelotdacosta/Zotero/storage/RGBARVEM/Nielsen - 2018 - An elementary introduction to information geometry.pdf;/Users/lancelotdacosta/Zotero/storage/TIYRW3LX/Nielsen - 2018 - An elementary introduction to information geometry.pdf}
}

@article{fragosoBayesianModelAveraging2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.08864},
  title = {Bayesian Model Averaging: {{A}} Systematic Review and Conceptual Classification},
  volume = {86},
  issn = {03067734},
  shorttitle = {Bayesian Model Averaging},
  abstract = {Bayesian Model Averaging (BMA) is an application of Bayesian inference to the problems of model selection, combined estimation and prediction that produces a straightforward model choice criteria and less risky predictions. However, the application of BMA is not always straightforward, leading to diverse assumptions and situational choices on its different aspects. Despite the widespread application of BMA in the literature, there were not many accounts of these differences and trends besides a few landmark revisions in the late 1990s and early 2000s, therefore not taking into account any advancements made in the last 15 years. In this work, we present an account of these developments through a careful content analysis of 587 articles in BMA published between 1996 and 2014. We also develop a conceptual classification scheme to better describe this vast literature, understand its trends and future directions and provide guidance for the researcher interested in both the application and development of the methodology. The results of the classification scheme and content review are then used to discuss the present and future of the BMA literature.},
  language = {en},
  number = {1},
  journal = {International Statistical Review},
  doi = {10.1111/insr.12243},
  author = {Fragoso, Tiago M. and Neto, Francisco Louzada},
  month = apr,
  year = {2018},
  keywords = {Statistics - Methodology,Statistics - Applications},
  pages = {1-28},
  file = {/Users/lancelotdacosta/Zotero/storage/9M6FIP4B/Fragoso and Neto - 2018 - Bayesian model averaging A systematic review and .pdf;/Users/lancelotdacosta/Zotero/storage/BUUCWUIV/Fragoso and Neto - 2018 - Bayesian model averaging A systematic review and .pdf;/Users/lancelotdacosta/Zotero/storage/BWAL6GRI/Fragoso and Neto - 2018 - Bayesian model averaging A systematic review and .pdf;/Users/lancelotdacosta/Zotero/storage/ICGEHKMM/Fragoso and Neto - 2018 - Bayesian model averaging A systematic review and .pdf}
}

@article{hoetingBayesianModelAveraging,
  title = {Bayesian {{Model Averaging}}: {{A Tutorial}}},
  abstract = {Standard statistical practice ignores model uncertainty. Data analysts typically select a model from some class of models and then proceed as if the selected model had generated the data. This approach ignores the uncertainty in model selection, leading to over-confident inferences and decisions that are more risky than one thinks they are. Bayesian model averaging (BMA) provides a coherent mechanism for accounting for this model uncertainty. Several methods for implementing BMA have recently emerged. We discuss these methods and present a number of examples. In these examples, BMA provides improved out-ofsample predictive performance. We also provide a catalogue of currently available BMA software.},
  language = {en},
  author = {Hoeting, Jennifer A and Madigan, David and Raftery, Adrian E and Volinsky, Chris T},
  pages = {36},
  file = {/Users/lancelotdacosta/Zotero/storage/7QIUVJYW/Hoeting et al. - Bayesian Model Averaging A Tutorial.pdf;/Users/lancelotdacosta/Zotero/storage/JPMC37LL/Hoeting et al. - Bayesian Model Averaging A Tutorial.pdf;/Users/lancelotdacosta/Zotero/storage/VAXMWXAR/Hoeting et al. - Bayesian Model Averaging A Tutorial.pdf;/Users/lancelotdacosta/Zotero/storage/WAXFWW6I/Hoeting et al. - Bayesian Model Averaging A Tutorial.pdf}
}

@article{weissCorrectnessBeliefPropagation2001,
  title = {Correctness of {{Belief Propagation}} in {{Gaussian Graphical Models}} of {{Arbitrary Topology}}},
  volume = {13},
  issn = {0899-7667, 1530-888X},
  abstract = {Local ''belief propagation'' rules of the sort proposed by Pearl (1988) are guaranteed to converge to the correct posterior probabilities in singly connected graphical models. Recently, a number of researchers have empirically demonstrated good performance of ''loopy belief propagation'' \textendash{} using these same rules on graphs with loops. Perhaps the most dramatic instance is the near Shannon-limit performance of ''Turbo codes'', whose decoding algorithm is equivalent to loopy belief propagation. These results motivate using the powerful belief propagation algorithm in a broader class of networks, and help clarify the empirical performance results.},
  language = {en},
  number = {10},
  journal = {Neural Computation},
  doi = {10.1162/089976601750541769},
  author = {Weiss, Yair and Freeman, William T.},
  month = oct,
  year = {2001},
  pages = {2173-2200},
  file = {/Users/lancelotdacosta/Zotero/storage/BXMVMDAT/Weiss and Freeman - 2001 - Correctness of Belief Propagation in Gaussian Grap.pdf;/Users/lancelotdacosta/Zotero/storage/BYG4MB9D/Weiss and Freeman - 2001 - Correctness of Belief Propagation in Gaussian Grap.pdf;/Users/lancelotdacosta/Zotero/storage/E28DFBDE/Weiss and Freeman - 2001 - Correctness of Belief Propagation in Gaussian Grap.pdf;/Users/lancelotdacosta/Zotero/storage/S9N89RCJ/Weiss and Freeman - 2001 - Correctness of Belief Propagation in Gaussian Grap.pdf}
}

@article{liangFisherRaoMetricGeometry2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.01530},
  primaryClass = {cs, stat},
  title = {Fisher-{{Rao Metric}}, {{Geometry}}, and {{Complexity}} of {{Neural Networks}}},
  abstract = {We study the relationship between geometry and capacity measures for deep neural networks from an invariance viewpoint. We introduce a new notion of capacity \textemdash{} the Fisher-Rao norm \textemdash{} that possesses desirable invariance properties and is motivated by Information Geometry. We discover an analytical characterization of the new capacity measure, through which we establish norm-comparison inequalities and further show that the new measure serves as an umbrella for several existing norm-based complexity measures. We discuss upper bounds on the generalization error induced by the proposed measure. Extensive numerical experiments on CIFAR-10 support our theoretical findings. Our theoretical analysis rests on a key structural lemma about partial derivatives of multi-layer rectifier networks. Key words and phrases: deep learning, statistical learning theory, information geometry, Fisher-Rao metric, invariance, ReLU activation, natural gradient, capacity control, generalization error.},
  language = {en},
  journal = {arXiv:1711.01530 [cs, stat]},
  author = {Liang, Tengyuan and Poggio, Tomaso and Rakhlin, Alexander and Stokes, James},
  month = nov,
  year = {2017},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence},
  file = {/Users/lancelotdacosta/Zotero/storage/D7DVW62F/Liang et al. - 2017 - Fisher-Rao Metric, Geometry, and Complexity of Neu.pdf;/Users/lancelotdacosta/Zotero/storage/TMXIFEUA/Liang et al. - 2017 - Fisher-Rao Metric, Geometry, and Complexity of Neu.pdf;/Users/lancelotdacosta/Zotero/storage/UEL9A3FQ/Liang et al. - 2017 - Fisher-Rao Metric, Geometry, and Complexity of Neu.pdf;/Users/lancelotdacosta/Zotero/storage/WTBWWS96/Liang et al. - 2017 - Fisher-Rao Metric, Geometry, and Complexity of Neu.pdf}
}

@article{chenForwardBackwardConnections2009,
  title = {Forward and Backward Connections in the Brain: {{A DCM}} Study of Functional Asymmetries},
  volume = {45},
  issn = {10538119},
  shorttitle = {Forward and Backward Connections in the Brain},
  abstract = {In this paper, we provide evidence for functional asymmetries in forward and backward connections that define hierarchical architectures in the brain. We exploit the fact that modulatory or nonlinear influences of one neuronal system on another (i.e., effective connectivity) entail coupling between different frequencies. Functional asymmetry in forward and backward connections was addressed by comparing dynamic causal models of MEG responses induced by visual processing of normal and scrambled faces. We compared models with and without nonlinear (between-frequency) coupling in both forward and backward connections. Bayesian model comparison indicated that the best model had nonlinear forward and backward connections. Using the best model we then quantified frequency-specific causal influences mediating observed spectral responses. We found a striking asymmetry between forward and backward connections; in which high (gamma) frequencies in higher cortical areas suppressed low (alpha) frequencies in lower areas. This suppression was significantly greater than the homologous coupling in the forward connections. Furthermore, exactly the asymmetry was observed when we examined face-selective coupling (i.e., coupling under faces minus scrambled faces). These results highlight the importance of nonlinear coupling among brain regions and point to a functional asymmetry between forward and backward connections in the human brain that is consistent with anatomical and physiological evidence from animal studies. This asymmetry is also consistent with functional architectures implied by theories of perceptual inference in the brain, based on hierarchical generative models.},
  language = {en},
  number = {2},
  journal = {NeuroImage},
  doi = {10.1016/j.neuroimage.2008.12.041},
  author = {Chen, C.C. and Henson, R.N. and Stephan, K.E. and Kilner, J.M. and Friston, K.J.},
  month = apr,
  year = {2009},
  pages = {453-462},
  file = {/Users/lancelotdacosta/Zotero/storage/X9JQ7NIN/Chen et al. - 2009 - Forward and backward connections in the brain A D.pdf}
}

@article{conantEveryGoodRegulator1970,
  title = {Every Good Regulator of a System Must Be a Model of That System},
  volume = {1},
  language = {en},
  number = {2},
  journal = {Int. J. Systems Sci.},
  author = {Conant, Roger C and Ashby, W. R.},
  year = {1970},
  pages = {89-97},
  file = {/Users/lancelotdacosta/Zotero/storage/IJLZ5JB7/Conant - EVERY GOOD REGULATOR OF A SYSTEM MUST BE A MODEL O.pdf}
}

@article{kirchhoffMarkovBlanketsLife2018,
  title = {The {{Markov}} Blankets of Life: Autonomy, Active Inference and the Free Energy Principle},
  volume = {15},
  issn = {1742-5689, 1742-5662},
  shorttitle = {The {{Markov}} Blankets of Life},
  language = {en},
  number = {138},
  journal = {Journal of The Royal Society Interface},
  doi = {10.1098/rsif.2017.0792},
  author = {Kirchhoff, Michael and Parr, Thomas and Palacios, Ensor and Friston, Karl and Kiverstein, Julian},
  month = jan,
  year = {2018},
  pages = {20170792},
  file = {/Users/lancelotdacosta/Zotero/storage/FPHSE2RG/Kirchhoff et al. - 2018 - The Markov blankets of life autonomy, active infe.pdf;/Users/lancelotdacosta/Zotero/storage/G4IQXNWG/Kirchhoff et al. - 2018 - The Markov blankets of life autonomy, active infe.pdf;/Users/lancelotdacosta/Zotero/storage/HW4QR9FX/Kirchhoff et al. - 2018 - The Markov blankets of life autonomy, active infe.pdf;/Users/lancelotdacosta/Zotero/storage/Z9DTJZWG/Kirchhoff et al. - 2018 - The Markov blankets of life autonomy, active infe.pdf}
}

@book{hamiltonMathematicalGaugeTheory2017,
  address = {{New York, NY}},
  edition = {1st edition},
  title = {Mathematical Gauge Theory: With Applications to the Standard Model of Particle Physics},
  isbn = {978-3-319-68438-3},
  shorttitle = {Mathematical Gauge Theory},
  language = {en},
  publisher = {{Springer Berlin Heidelberg}},
  author = {d Hamilton, Mark J.},
  year = {2017},
  file = {/Users/lancelotdacosta/Zotero/storage/D6QTDLCE/Hamilton - 2017 - Mathematical gauge theory with applications to th.pdf;/Users/lancelotdacosta/Zotero/storage/GGRG36T6/Hamilton - 2017 - Mathematical gauge theory with applications to th.pdf;/Users/lancelotdacosta/Zotero/storage/IF8VYDEY/Hamilton - 2017 - Mathematical gauge theory with applications to th.pdf;/Users/lancelotdacosta/Zotero/storage/YDVE8B9I/Hamilton - 2017 - Mathematical gauge theory with applications to th.pdf}
}

@article{parrWorkingMemoryAttention2017,
  title = {Working Memory, Attention, and Salience in Active Inference},
  volume = {7},
  issn = {2045-2322},
  language = {en},
  number = {1},
  journal = {Scientific Reports},
  doi = {10.1038/s41598-017-15249-0},
  author = {Parr, Thomas and Friston, Karl J},
  month = dec,
  year = {2017},
  pages = {14678},
  file = {/Users/lancelotdacosta/Zotero/storage/4C2JF6JP/Parr and Friston - 2017 - Working memory, attention, and salience in active .pdf;/Users/lancelotdacosta/Zotero/storage/628F5HK8/Parr and Friston - 2017 - Working memory, attention, and salience in active .pdf;/Users/lancelotdacosta/Zotero/storage/CCTH83AG/Parr and Friston - 2017 - Working memory, attention, and salience in active .pdf;/Users/lancelotdacosta/Zotero/storage/Z4Y6QNCA/Parr and Friston - 2017 - Working memory, attention, and salience in active .pdf}
}

@article{fristonActiveInferenceEpistemic2015,
  title = {Active Inference and Epistemic Value},
  volume = {6},
  issn = {1758-8928, 1758-8936},
  language = {en},
  number = {4},
  journal = {Cognitive Neuroscience},
  doi = {10.1080/17588928.2015.1020053},
  author = {Friston, Karl and Rigoli, Francesco and Ognibene, Dimitri and Mathys, Christoph and Fitzgerald, Thomas and Pezzulo, Giovanni},
  month = oct,
  year = {2015},
  pages = {187-214},
  file = {/Users/lancelotdacosta/Zotero/storage/CETW2APE/Friston et al. - 2015 - Active inference and epistemic value.pdf;/Users/lancelotdacosta/Zotero/storage/JIN89I4Q/Friston et al. - 2015 - Active inference and epistemic value.pdf;/Users/lancelotdacosta/Zotero/storage/XYNHNSI5/Friston et al. - 2015 - Active inference and epistemic value.pdf;/Users/lancelotdacosta/Zotero/storage/ZMSU3TBX/Friston et al. - 2015 - Active inference and epistemic value.pdf}
}

@article{huysComputationalPsychiatryBridge2016,
  title = {Computational Psychiatry as a Bridge from Neuroscience to Clinical Applications},
  volume = {19},
  issn = {1097-6256, 1546-1726},
  language = {en},
  number = {3},
  journal = {Nature Neuroscience},
  doi = {10.1038/nn.4238},
  author = {Huys, Quentin J M and Maia, Tiago V and Frank, Michael J},
  month = mar,
  year = {2016},
  pages = {404-413},
  file = {/Users/lancelotdacosta/Zotero/storage/6YL23CSW/Huys et al. - 2016 - Computational psychiatry as a bridge from neurosci.pdf;/Users/lancelotdacosta/Zotero/storage/FIERVQ5S/Huys et al. - 2016 - Computational psychiatry as a bridge from neurosci.pdf;/Users/lancelotdacosta/Zotero/storage/X3MMLNR5/Huys et al. - 2016 - Computational psychiatry as a bridge from neurosci.pdf;/Users/lancelotdacosta/Zotero/storage/Y4CSH3DM/Huys et al. - 2016 - Computational psychiatry as a bridge from neurosci.pdf}
}

@article{stolzComputationalTopologyNeuroscience,
  title = {Computational {{Topology}} in {{Neuroscience}}},
  language = {en},
  author = {Stolz, Bernadette},
  pages = {77},
  file = {/Users/lancelotdacosta/Zotero/storage/BG42WWS2/Stolz - Computational Topology in Neuroscience.pdf;/Users/lancelotdacosta/Zotero/storage/EVH6HVSX/Stolz - Computational Topology in Neuroscience.pdf;/Users/lancelotdacosta/Zotero/storage/JBJVZALS/Stolz - Computational Topology in Neuroscience.pdf;/Users/lancelotdacosta/Zotero/storage/WVCHZH2J/Stolz - Computational Topology in Neuroscience.pdf}
}

@article{lakeHumanlevelConceptLearning2015,
  title = {Human-Level Concept Learning through Probabilistic Program Induction},
  volume = {350},
  issn = {0036-8075, 1095-9203},
  language = {en},
  number = {6266},
  journal = {Science},
  doi = {10.1126/science.aab3050},
  author = {Lake, B. M. and Salakhutdinov, R. and Tenenbaum, J. B.},
  month = dec,
  year = {2015},
  pages = {1332-1338},
  file = {/Users/lancelotdacosta/Zotero/storage/BRJTWMUD/Lake et al. - 2015 - Human-level concept learning through probabilistic.pdf;/Users/lancelotdacosta/Zotero/storage/I62Y4VDB/Lake et al. - 2015 - Human-level concept learning through probabilistic.pdf;/Users/lancelotdacosta/Zotero/storage/SC5IFW79/Lake et al. - 2015 - Human-level concept learning through probabilistic.pdf;/Users/lancelotdacosta/Zotero/storage/TA7P3M5I/Lake et al. - 2015 - Human-level concept learning through probabilistic.pdf}
}

@book{ayInformationGeometry2017,
  address = {{Cham}},
  series = {Ergebnisse Der {{Mathematik}} Und Ihrer {{Grenzgebiete}} 34},
  title = {Information {{Geometry}}},
  volume = {64},
  isbn = {978-3-319-56477-7 978-3-319-56478-4},
  language = {en},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-319-56478-4},
  author = {Ay, Nihat and Jost, J{\"u}rgen and L{\^e}, H{\^o}ng V{\^a}n and Schwachh{\"o}fer, Lorenz},
  year = {2017},
  file = {/Users/lancelotdacosta/Zotero/storage/4GV864VG/Ay et al. - 2017 - Information Geometry.pdf;/Users/lancelotdacosta/Zotero/storage/66PZTG5Y/Ay et al. - 2017 - Information Geometry.pdf;/Users/lancelotdacosta/Zotero/storage/GATV2332/Ay et al. - 2017 - Information Geometry.pdf;/Users/lancelotdacosta/Zotero/storage/SQ3ATFY6/Ay et al. - 2017 - Information Geometry.pdf}
}

@article{chenNeuralOrdinaryDifferential2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.07366},
  primaryClass = {cs, stat},
  title = {Neural {{Ordinary Differential Equations}}},
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  language = {en},
  journal = {arXiv:1806.07366 [cs, stat]},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  month = jun,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence},
  file = {/Users/lancelotdacosta/Zotero/storage/WNUJY7IN/Chen et al. - 2018 - Neural Ordinary Differential Equations.pdf}
}

@article{fletcherPerceivingBelievingBayesian2009,
  title = {Perceiving Is Believing: A {{Bayesian}} Approach to Explaining the Positive Symptoms of Schizophrenia},
  volume = {10},
  issn = {1471-003X, 1471-0048},
  shorttitle = {Perceiving Is Believing},
  abstract = {Advances in cognitive neuroscience offer us new ways to understand the symptoms of mental illness by uniting basic neurochemical and neurophysiological observations with the conscious experiences that characterize these symptoms. Cognitive theories about the positive symptoms of schizophrenia \textemdash{} hallucinations and delusions \textemdash{}have tended to treat perception and belief formation as distinct processes. However, recent advances in computational neuroscience have led us to consider the unusual perceptual experiences of patients and their sometimes bizarre beliefs as part of the same core abnormality \textemdash{} a disturbance in error-dependent updating of inferences and beliefs about the world. We suggest that it is possible to understand these symptoms in terms of a disturbed hierarchical Bayesian framework, without recourse to separate considerations of experience and belief.},
  language = {en},
  number = {1},
  journal = {Nature Reviews Neuroscience},
  doi = {10.1038/nrn2536},
  author = {Fletcher, Paul C. and Frith, Chris D.},
  month = jan,
  year = {2009},
  pages = {48-58},
  file = {/Users/lancelotdacosta/Zotero/storage/YZASV3UL/Fletcher and Frith - 2009 - Perceiving is believing a Bayesian approach to ex.pdf}
}

@article{zwaanSeeingActingUnderstanding2006,
  title = {Seeing, Acting, Understanding: {{Motor}} Resonance in Language Comprehension.},
  volume = {135},
  issn = {1939-2222, 0096-3445},
  shorttitle = {Seeing, Acting, Understanding},
  abstract = {Observing actions and understanding sentences about actions activates corresponding motor processes in the observer\textendash{} comprehender. In 5 experiments, the authors addressed 2 novel questions regarding language-based motor resonance. The 1st question asks whether visual motion that is associated with an action produces motor resonance in sentence comprehension. The 2nd question asks whether motor resonance is modulated during sentence comprehension. The authors' experiments provide an affirmative response to both questions. A rotating visual stimulus affects both actual manual rotation and the comprehension of manual rotation sentences. Motor resonance is modulated by the linguistic input and is a rather immediate and localized phenomenon. The results are discussed in the context of theories of action observation and mental simulation.},
  language = {en},
  number = {1},
  journal = {Journal of Experimental Psychology: General},
  doi = {10.1037/0096-3445.135.1.1},
  author = {Zwaan, Rolf A. and Taylor, Lawrence J.},
  year = {2006},
  pages = {1-11},
  file = {/Users/lancelotdacosta/Zotero/storage/HSIPJGPS/Zwaan and Taylor - 2006 - Seeing, acting, understanding Motor resonance in .pdf}
}

@article{barsalouSimulationSituatedConceptualization2009,
  title = {Simulation, Situated Conceptualization, and Prediction},
  volume = {364},
  issn = {0962-8436, 1471-2970},
  language = {en},
  number = {1521},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  doi = {10.1098/rstb.2008.0319},
  author = {Barsalou, Lawrence W.},
  month = may,
  year = {2009},
  pages = {1281-1289},
  file = {/Users/lancelotdacosta/Zotero/storage/H7CX4QF7/Barsalou - 2009 - Simulation, situated conceptualization, and predic.pdf}
}

@book{minskySocietyMind1986,
  address = {{New York}},
  title = {The Society of Mind},
  isbn = {978-0-671-60740-1},
  lccn = {BF431 .M553 1986},
  language = {en},
  publisher = {{Simon and Schuster}},
  author = {Minsky, Marvin},
  year = {1986},
  keywords = {Human information processing,Intellect,Philosophy,Science},
  file = {/Users/lancelotdacosta/Zotero/storage/7PVJKBNB/Minsky - 1986 - The society of mind.pdf;/Users/lancelotdacosta/Zotero/storage/BWNMIKPD/Minsky - 1986 - The society of mind.pdf;/Users/lancelotdacosta/Zotero/storage/MBVTNIFX/Minsky - 1986 - The society of mind.pdf;/Users/lancelotdacosta/Zotero/storage/TWRIAHJE/Minsky - 1986 - The society of mind.pdf}
}

@article{adamsComputationalAnatomyPsychosis2013,
  title = {The {{Computational Anatomy}} of {{Psychosis}}},
  volume = {4},
  issn = {1664-0640},
  abstract = {This paper considers psychotic symptoms in terms of false inferences or beliefs. It is based on the notion that the brain is an inference machine that actively constructs hypotheses to explain or predict its sensations. This perspective provides a normative (Bayes-optimal) account of action and perception that emphasizes probabilistic representations; in particular, the confidence or precision of beliefs about the world. We will consider hallucinosis, abnormal eye movements, sensory attenuation deficits, catatonia, and delusions as various expressions of the same core pathology: namely, an aberrant encoding of precision. From a cognitive perspective, this represents a pernicious failure of metacognition (beliefs about beliefs) that can confound perceptual inference. In the embodied setting of active (Bayesian) inference, it can lead to behaviors that are paradoxically more accurate than Bayes-optimal behavior. Crucially, this normative account is accompanied by a neuronally plausible process theory based upon hierarchical predictive coding. In predictive coding, precision is thought to be encoded by the post-synaptic gain of neurons reporting prediction error. This suggests that both pervasive trait abnormalities and florid failures of inference in the psychotic state can be linked to factors controlling post-synaptic gain \textendash{} such as NMDA receptor function and (dopaminergic) neuromodulation. We illustrate these points using biologically plausible simulations of perceptual synthesis, smooth pursuit eye movements and attribution of agency \textendash{} that all use the same predictive coding scheme and pathology: namely, a reduction in the precision of prior beliefs, relative to sensory evidence.},
  language = {en},
  journal = {Frontiers in Psychiatry},
  doi = {10.3389/fpsyt.2013.00047},
  author = {Adams, Rick A. and Stephan, Klaas Enno and Brown, Harriet R. and Frith, Christopher D. and Friston, Karl J.},
  year = {2013},
  keywords = {active inference,free energy,precision,illusions,psychosis,schizophrenia,sensory attenuation},
  file = {/Users/lancelotdacosta/Zotero/storage/2G6VQQVI/Adams et al. - 2013 - The Computational Anatomy of Psychosis.pdf;/Users/lancelotdacosta/Zotero/storage/JUZXTNPW/Adams et al. - 2013 - The Computational Anatomy of Psychosis.pdf;/Users/lancelotdacosta/Zotero/storage/KNP4QZ7J/Adams et al. - 2013 - The computational anatomy of psychosis.pdf;/Users/lancelotdacosta/Zotero/storage/NWPAZ5AK/Adams et al. - 2013 - The Computational Anatomy of Psychosis.pdf;/Users/lancelotdacosta/Zotero/storage/XQ2JB5WM/Adams et al. - 2013 - The Computational Anatomy of Psychosis.pdf}
}

@article{chalmersSEARCHTHEORYCONSCIOUS,
  title = {{{IN SEARCH OF A THEORY OF CONSCIOUS EXPERIENCE}}},
  language = {en},
  author = {Chalmers, David J},
  pages = {391},
  file = {/Users/lancelotdacosta/Zotero/storage/KC7DBEP6/Chalmers - IN SEARCH OF A THEORY OF CONSCIOUS EXPERIENCE.pdf}
}

@article{tononiMeasureBrainComplexity1994,
  title = {A Measure for Brain Complexity: Relating Functional Segregation and Integration in the Nervous System.},
  volume = {91},
  issn = {0027-8424, 1091-6490},
  shorttitle = {A Measure for Brain Complexity},
  abstract = {In brains ofhigher vertebrates, the functional segregation of local areas that differ in their anatomy and physiology contrasts sharply with their global ination during perception and behavior. In this paper, we introduce a measure, called neural complexity (CN), that captures the interplay between these two dental aspects of brain organization. We express functional segregation within a neural system in terms of the relative statistical independence of small subsets of the system and functional integration in terms of signicant deviations from independence oflarge subsets. CN is then obtained from estimates of the average deviation from statistical independence for subsets of increasing size. CN is shown to be high when functional segregation coexists with integration and to be low when the components of a system are either completely independent (segregated) or completely dependent (integrated). We apply this complexity measure in computer simulations of cortical areas to examine how some basic principles of neuroanatomical organization constrain brain dynamics. We show that the connectivity patterns of the cerebral cortex, such as a high density of connections, strong local connectivity ornizing cells into neuronal groups, patchiness in the connectivity am neuronal groups, and prevalent reciprocal connections, are associated with hi values of CN. The approach outlined here may prove useful in analyzing complexity in other biological domains such as gene regulation and embryogenesis.},
  language = {en},
  number = {11},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.91.11.5033},
  author = {Tononi, G. and Sporns, O. and Edelman, G. M.},
  month = may,
  year = {1994},
  pages = {5033-5037},
  file = {/Users/lancelotdacosta/Zotero/storage/34BX9Q38/Tononi et al. - 1994 - A measure for brain complexity relating functiona.pdf;/Users/lancelotdacosta/Zotero/storage/4YFMVVIQ/Tononi et al. - 1994 - A measure for brain complexity relating functiona.pdf;/Users/lancelotdacosta/Zotero/storage/64QUWIX3/Tononi et al. - 1994 - A measure for brain complexity relating functiona.pdf;/Users/lancelotdacosta/Zotero/storage/G8QT3AKK/Tononi et al. - 1994 - A measure for brain complexity relating functiona.pdf}
}

@article{schwobelActiveInferenceBelief2018,
  title = {Active {{Inference}}, {{Belief Propagation}}, and the {{Bethe Approximation}}},
  volume = {30},
  issn = {0899-7667, 1530-888X},
  language = {en},
  number = {9},
  journal = {Neural Computation},
  doi = {10.1162/neco_a_01108},
  author = {Schw{\"o}bel, Sarah and Kiebel, Stefan and Markovi{\'c}, Dimitrije},
  month = sep,
  year = {2018},
  pages = {2530-2567},
  file = {/Users/lancelotdacosta/Zotero/storage/8GB5P659/Schwöbel et al. - 2018 - Active Inference, Belief Propagation, and the Beth.pdf}
}

@article{pezzuloActiveInferenceView2012,
  title = {An {{Active Inference}} View of Cognitive Control},
  volume = {3},
  issn = {1664-1078},
  language = {en},
  journal = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2012.00478},
  author = {Pezzulo, Giovanni},
  year = {2012},
  file = {/Users/lancelotdacosta/Zotero/storage/3BUL2XBU/Pezzulo - 2012 - An Active Inference view of cognitive control.pdf;/Users/lancelotdacosta/Zotero/storage/DESW9ZJN/Pezzulo - 2012 - An Active Inference view of cognitive control.pdf;/Users/lancelotdacosta/Zotero/storage/KQYFJQZU/Pezzulo - 2012 - An Active Inference view of cognitive control.pdf}
}

@article{huysBonsaiTreesYour2012,
  title = {Bonsai {{Trees}} in {{Your Head}}: {{How}} the {{Pavlovian System Sculpts Goal}}-{{Directed Choices}} by {{Pruning Decision Trees}}},
  volume = {8},
  issn = {1553-7358},
  shorttitle = {Bonsai {{Trees}} in {{Your Head}}},
  abstract = {When planning a series of actions, it is usually infeasible to consider all potential future sequences; instead, one must prune the decision tree. Provably optimal pruning is, however, still computationally ruinous and the specific approximations humans employ remain unknown. We designed a new sequential reinforcement-based task and showed that human subjects adopted a simple pruning strategy: during mental evaluation of a sequence of choices, they curtailed any further evaluation of a sequence as soon as they encountered a large loss. This pruning strategy was Pavlovian: it was reflexively evoked by large losses and persisted even when overwhelmingly counterproductive. It was also evident above and beyond loss aversion. We found that the tendency towards Pavlovian pruning was selectively predicted by the degree to which subjects exhibited sub-clinical mood disturbance, in accordance with theories that ascribe Pavlovian behavioural inhibition, via serotonin, a role in mood disorders. We conclude that Pavlovian behavioural inhibition shapes highly flexible, goaldirected choices in a manner that may be important for theories of decision-making in mood disorders.},
  language = {en},
  number = {3},
  journal = {PLoS Computational Biology},
  doi = {10.1371/journal.pcbi.1002410},
  author = {Huys, Quentin J. M. and Eshel, Neir and O'Nions, Elizabeth and Sheridan, Luke and Dayan, Peter and Roiser, Jonathan P.},
  editor = {Maloney, Laurence T.},
  month = mar,
  year = {2012},
  pages = {e1002410},
  file = {/Users/lancelotdacosta/Zotero/storage/7H7T5XEE/Huys et al. - 2012 - Bonsai Trees in Your Head How the Pavlovian Syste.PDF;/Users/lancelotdacosta/Zotero/storage/H8GG75JV/Huys et al. - 2012 - Bonsai Trees in Your Head How the Pavlovian Syste.PDF;/Users/lancelotdacosta/Zotero/storage/IV5X9WBI/Huys et al. - 2012 - Bonsai Trees in Your Head How the Pavlovian Syste.PDF;/Users/lancelotdacosta/Zotero/storage/TVZJVBYK/Huys et al. - 2012 - Bonsai Trees in Your Head How the Pavlovian Syste.PDF}
}

@article{bullmoreComplexBrainNetworks2009,
  title = {Complex Brain Networks: Graph Theoretical Analysis of Structural and Functional Systems},
  volume = {10},
  issn = {1471-003X, 1471-0048},
  shorttitle = {Complex Brain Networks},
  abstract = {Recent developments in the quantitative analysis of complex networks, based largely on graph theory, have been rapidly translated to studies of brain network organization. The brain's structural and functional systems have features of complex networks \textemdash{} such as small-world topology, highly connected hubs and modularity \textemdash{} both at the whole-brain scale of human neuroimaging and at a cellular scale in non-human animals. In this article, we review studies investigating complex brain networks in diverse experimental modalities (including structural and functional MRI, diffusion tensor imaging, magnetoencephalography and electroencephalography in humans) and provide an accessible introduction to the basic principles of graph theory. We also highlight some of the technical challenges and key questions to be addressed by future developments in this rapidly moving field.},
  language = {en},
  number = {3},
  journal = {Nature Reviews Neuroscience},
  doi = {10.1038/nrn2575},
  author = {Bullmore, Ed and Sporns, Olaf},
  month = mar,
  year = {2009},
  pages = {186-198},
  file = {/Users/lancelotdacosta/Zotero/storage/HXPWVZF9/Bullmore and Sporns - 2009 - Complex brain networks graph theoretical analysis.pdf}
}

@techreport{dekampsComputationalGeometryModeling2018,
  type = {Preprint},
  title = {Computational {{Geometry}} for {{Modeling Neural Populations}}: From {{Visualization}} to {{Simulation}}},
  shorttitle = {Computational {{Geometry}} for {{Modeling Neural Populations}}},
  abstract = {The importance of a mesoscopic description level of the brain has now been well established. Rate based models are widely used, but have limitations. Recently, several extremely efficient population-level methods have been proposed that go beyond the characterization of a population in terms of a single variable. Here, we present a method for simulating neural populations based on two dimensional (2D) point spiking neuron models that defines the state of the population in terms of a density function over the neural state space. Our method differs in that we do not make the diffusion approximation, nor do we reduce the state space to a single dimension (1D). We do not hard code the neural model, but read in a grid describing its state space in the relevant simulation region. Novel models can be studied without even recompiling the code. The method is highly modular: variations of the deterministic neural dynamics and the stochastic process can be investigated independently. Currently, there is a trend to reduce complex high dimensional neuron models to 2D ones as they offer a rich dynamical repertoire that is not available in 1D, such as limit cycles. We will demonstrate that our method is ideally suited to investigate noise in such systems, replicating results obtained in the diffusion limit and generalizing them to a regime of large jumps. The joint probability density function is much more informative than 1D marginals, and we will argue that the study of 2D systems subject to noise is important complementary to 1D systems.},
  language = {en},
  institution = {{Neuroscience}},
  doi = {10.1101/275412},
  author = {{de Kamps}, Marc and Lepper{\o}d, Mikkel and Lai, Yi Ming},
  month = mar,
  year = {2018},
  file = {/Users/lancelotdacosta/Zotero/storage/REZK9NDH/de Kamps et al. - 2018 - Computational Geometry for Modeling Neural Populat.pdf}
}

@article{adamsComputationalPsychiatryMathematically2015,
  title = {Computational {{Psychiatry}}: Towards a Mathematically Informed Understanding of Mental Illness},
  issn = {0022-3050, 1468-330X},
  shorttitle = {Computational {{Psychiatry}}},
  abstract = {Computational Psychiatry aims to describe the relationship between the brain's neurobiology, its environment and mental symptoms in computational terms. In so doing, it may improve psychiatric classification and the diagnosis and treatment of mental illness. It can unite many levels of description in a mechanistic and rigorous fashion, while avoiding biological reductionism and artificial categorisation. We describe how computational models of cognition can infer the current state of the environment and weigh up future actions, and how these models provide new perspectives on two example disorders, depression and schizophrenia. Reinforcement learning describes how the brain can choose and value courses of actions according to their long-term future value. Some depressive symptoms may result from aberrant valuations, which could arise from prior beliefs about the loss of agency (`helplessness'), or from an inability to inhibit the mental exploration of aversive events. Predictive coding explains how the brain might perform Bayesian inference about the state of its environment by combining sensory data with prior beliefs, each weighted according to their certainty (or precision). Several cortical abnormalities in schizophrenia might reduce precision at higher levels of the inferential hierarchy, biasing inference towards sensory data and away from prior beliefs. We discuss whether striatal hyperdopaminergia might have an adaptive function in this context, and also how reinforcement learning and incentive salience models may shed light on the disorder. Finally, we review some of Computational Psychiatry's applications to neurological disorders, such as Parkinson's disease, and some pitfalls to avoid when applying its methods.},
  language = {en},
  journal = {Journal of Neurology, Neurosurgery \& Psychiatry},
  doi = {10.1136/jnnp-2015-310737},
  author = {Adams, Rick A and Huys, Quentin J M and Roiser, Jonathan P},
  month = jul,
  year = {2015},
  pages = {jnnp-2015-310737},
  file = {/Users/lancelotdacosta/Zotero/storage/GLA9BK6X/Adams et al. - 2015 - Computational Psychiatry towards a mathematically.pdf;/Users/lancelotdacosta/Zotero/storage/MEJWHK76/Adams et al. - 2015 - Computational Psychiatry towards a mathematically.pdf;/Users/lancelotdacosta/Zotero/storage/PYEX6URW/Adams et al. - 2015 - Computational Psychiatry towards a mathematically.pdf;/Users/lancelotdacosta/Zotero/storage/YZXUKW2X/Adams et al. - 2015 - Computational Psychiatry towards a mathematically.pdf}
}

@article{abelesCorticalActivityFlips1995,
  title = {Cortical Activity Flips among Quasi-Stationary States.},
  volume = {92},
  issn = {0027-8424, 1091-6490},
  abstract = {Parallel recordings of spike trains of several single cortical neurons in behaving monkeys were analyzed as a hidden Markov process. The parallel spike trains were considered as a multivariate Poisson process whose vector firing rates change with time. As a consequence of this approach, the complete recording can be segmented into a sequence of a few statistically discriminated hidden states, whose dynamics are modeled as a first-order Markov chain. The biological validity and benefits of this approach were examined in several independent ways: (i) the statistical consistency of the segmentation and its correspondence to the behavior of the animal; (ii) direct measurement of the collective flips of activity, obtained by the model; and (iii) the relation between the segmentation and the pair-wise shortterm cross-correlations between the recorded spike trains. Comparison with surrogate data was also carried out for each of the above examinations to assure their significance. Our results indicated the existence of well-separated states of activity, within which the firing rates were approximately stationary. With our present data we could reliably discriminate six to eight such states. The transitions between states were fast and were associated with concomitant changes of firing rates of several neurons. Different behavioral modes and stimuli were consistently reflected by different states of neural activity. Moreover, the pair-wise correlations between neurons varied considerably between the different states, supporting the hypothesis that these distinct states were brought about by the cooperative action of many neurons.},
  language = {en},
  number = {19},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.92.19.8616},
  author = {Abeles, M. and Bergman, H. and Gat, I. and Meilijson, I. and Seidemann, E. and Tishby, N. and Vaadia, E.},
  month = sep,
  year = {1995},
  pages = {8616-8620},
  file = {/Users/lancelotdacosta/Zotero/storage/5RHY6W59/Abeles et al. - 1995 - Cortical activity flips among quasi-stationary sta.pdf;/Users/lancelotdacosta/Zotero/storage/68E2ZG38/Abeles et al. - 1995 - Cortical activity flips among quasi-stationary sta.pdf;/Users/lancelotdacosta/Zotero/storage/QLIGXKVB/Abeles et al. - 1995 - Cortical activity flips among quasi-stationary sta.pdf;/Users/lancelotdacosta/Zotero/storage/WT8TAZ3G/Abeles et al. - 1995 - Cortical activity flips among quasi-stationary sta.pdf}
}

@article{iglDeepVariationalReinforcement2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.02426},
  primaryClass = {cs, stat},
  title = {Deep {{Variational Reinforcement Learning}} for {{POMDPs}}},
  abstract = {Many real-world sequential decision making problems are partially observable by nature, and the environment model is typically unknown. Consequently, there is great need for reinforcement learning methods that can tackle such problems given only a stream of incomplete and noisy observations. In this paper, we propose deep variational reinforcement learning (DVRL), which introduces an inductive bias that allows an agent to learn a generative model of the environment and perform inference in that model to effectively aggregate the available information. We develop an n-step approximation to the evidence lower bound (ELBO), allowing the model to be trained jointly with the policy. This ensures that the latent state representation is suitable for the control task. In experiments on Mountain Hike and flickering Atari we show that our method outperforms previous approaches relying on recurrent neural networks to encode the past.},
  language = {en},
  journal = {arXiv:1806.02426 [cs, stat]},
  author = {Igl, Maximilian and Zintgraf, Luisa and Le, Tuan Anh and Wood, Frank and Whiteson, Shimon},
  month = jun,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/lancelotdacosta/Zotero/storage/RC2XKGDQ/Igl et al. - 2018 - Deep Variational Reinforcement Learning for POMDPs.pdf;/Users/lancelotdacosta/Zotero/storage/RS3ZTD65/Igl et al. - 2018 - Deep Variational Reinforcement Learning for POMDPs.pdf;/Users/lancelotdacosta/Zotero/storage/UCGNIWSC/Igl et al. - 2018 - Deep Variational Reinforcement Learning for POMDPs.pdf;/Users/lancelotdacosta/Zotero/storage/V2RDUDF6/Igl et al. - 2018 - Deep Variational Reinforcement Learning for POMDPs.pdf}
}

@article{fristonDynamicCausalModelling2003,
  title = {Dynamic Causal Modelling},
  volume = {19},
  issn = {10538119},
  abstract = {In this paper we present an approach to the identification of nonlinear input\textendash{}state\textendash{} output systems. By using a bilinear approximation to the dynamics of interactions among states, the parameters of the implicit causal model reduce to three sets. These comprise (1) parameters that mediate the influence of extrinsic inputs on the states, (2) parameters that mediate intrinsic coupling among the states, and (3) [bilinear] parameters that allow the inputs to modulate that coupling. Identification proceeds in a Bayesian framework given known, deterministic inputs and the observed responses of the system. We developed this approach for the analysis of effective connectivity using experimentally designed inputs and fMRI responses. In this context, the coupling parameters correspond to effective connectivity and the bilinear parameters reflect the changes in connectivity induced by inputs. The ensuing framework allows one to characterise fMRI experiments, conceptually, as an experimental manipulation of integration among brain regions (by contextual or trial-free inputs, like time or attentional set) that is revealed using evoked responses (to perturbations or trial-bound inputs, like stimuli). As with previous analyses of effective connectivity, the focus is on experimentally induced changes in coupling (cf., psychophysiologic interactions). However, unlike previous approaches in neuroimaging, the causal model ascribes responses to designed deterministic inputs, as opposed to treating inputs as unknown and stochastic.},
  language = {en},
  number = {4},
  journal = {NeuroImage},
  doi = {10.1016/S1053-8119(03)00202-7},
  author = {Friston, K.J. and Harrison, L. and Penny, W.},
  month = aug,
  year = {2003},
  pages = {1273-1302},
  file = {/Users/lancelotdacosta/Zotero/storage/339PR2LP/Friston et al. - 2003 - Dynamic causal modelling.pdf;/Users/lancelotdacosta/Zotero/storage/L559ZCQM/Friston et al. - 2003 - Dynamic causal modelling.pdf;/Users/lancelotdacosta/Zotero/storage/R5IDXUY8/Friston et al. - 2003 - Dynamic causal modelling.pdf;/Users/lancelotdacosta/Zotero/storage/RE4BTAQI/Friston et al. - 2003 - Dynamic causal modelling.pdf}
}

@article{bassettDynamicReconfigurationHuman2011,
  title = {Dynamic Reconfiguration of Human Brain Networks during Learning},
  volume = {108},
  issn = {0027-8424, 1091-6490},
  language = {en},
  number = {18},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1018985108},
  author = {Bassett, D. S. and Wymbs, N. F. and Porter, M. A. and Mucha, P. J. and Carlson, J. M. and Grafton, S. T.},
  month = may,
  year = {2011},
  pages = {7641-7646},
  file = {/Users/lancelotdacosta/Zotero/storage/T2XFMB6U/Bassett et al. - 2011 - Dynamic reconfiguration of human brain networks du.pdf}
}

@article{smithEstimatingStateSpaceModel2003,
  title = {Estimating a {{State}}-{{Space Model}} from {{Point Process Observations}}},
  volume = {15},
  issn = {0899-7667, 1530-888X},
  language = {en},
  number = {5},
  journal = {Neural Computation},
  doi = {10.1162/089976603765202622},
  author = {Smith, Anne C. and Brown, Emery N.},
  month = may,
  year = {2003},
  pages = {965-991},
  file = {/Users/lancelotdacosta/Zotero/storage/7BSUVP35/Smith and Brown - 2003 - Estimating a State-Space Model from Point Process .pdf;/Users/lancelotdacosta/Zotero/storage/DMYHQRI2/Smith and Brown - 2003 - Estimating a State-Space Model from Point Process .pdf;/Users/lancelotdacosta/Zotero/storage/IAVRZMVM/Smith and Brown - 2003 - Estimating a State-Space Model from Point Process .pdf;/Users/lancelotdacosta/Zotero/storage/KPVJ38DD/Smith and Brown - 2003 - Estimating a State-Space Model from Point Process .pdf}
}

@article{walmsleyExplanationDynamicalCognitive2008,
  title = {Explanation in {{Dynamical Cognitive Science}}},
  volume = {18},
  issn = {0924-6495, 1572-8641},
  abstract = {In this paper, I outline two strands of evidence for the conclusion that the dynamical approach to cognitive science both seeks and provides covering law explanations. Two of the most successful dynamical models\textemdash{}Kelso's model of rhythmic finger movement and Thelen et al.'s model of infant perseverative reaching\textemdash{}can be seen to provide explanations which conform to the famous explanatory scheme first put forward by Hempel and Oppenheim. In addition, many prominent advocates of the dynamical approach also express the provision of this kind of explanation as a goal of dynamical cognitive science. I conclude by briefly outlining two consequences. First, dynamical cognitive science's explanatory style may strengthen its links to the so-called ``situated'' approach to cognition, but, secondly, it may also undermine the widespread intuition that dynamics is related to emergentism in the philosophy of mind.},
  language = {en},
  number = {3},
  journal = {Minds and Machines},
  doi = {10.1007/s11023-008-9103-9},
  author = {Walmsley, Joel},
  month = sep,
  year = {2008},
  pages = {331-348},
  file = {/Users/lancelotdacosta/Zotero/storage/8TX4E99M/Walmsley - 2008 - Explanation in Dynamical Cognitive Science.pdf;/Users/lancelotdacosta/Zotero/storage/P4ERCHXQ/Walmsley - 2008 - Explanation in Dynamical Cognitive Science.pdf;/Users/lancelotdacosta/Zotero/storage/R3KUYJ47/Walmsley - 2008 - Explanation in Dynamical Cognitive Science.pdf;/Users/lancelotdacosta/Zotero/storage/S7MKQVA3/Walmsley - 2008 - Explanation in Dynamical Cognitive Science.pdf}
}

@article{solwayGoaldirectedDecisionMaking2012,
  title = {Goal-Directed Decision Making as Probabilistic Inference: {{A}} Computational Framework and Potential Neural Correlates.},
  volume = {119},
  issn = {1939-1471, 0033-295X},
  shorttitle = {Goal-Directed Decision Making as Probabilistic Inference},
  abstract = {Recent work has given rise to the view that reward-based decision making is governed by two key controllers: a habit system, which stores stimulus-response associations shaped by past reward, and a goal-oriented system that selects actions based on their anticipated outcomes. The current literature provides a rich body of computational theory addressing habit formation, centering on temporal-difference learning mechanisms. Less progress has been made toward formalizing the processes involved in goal-directed decision making. We draw on recent work in cognitive neuroscience, animal conditioning, cognitive and developmental psychology and machine learning, to outline a new theory of goal-directed decision making. Our basic proposal is that the brain, within an identifiable network of cortical and subcortical structures, implements a probabilistic generative model of reward, and that goal-directed decision making is effected through Bayesian inversion of this model. We present a set of simulations implementing the account, which address benchmark behavioral and neuroscientific findings, and which give rise to a set of testable predictions. We also discuss the relationship between the proposed framework and other models of decision making, including recent models of perceptual choice, to which our theory bears a direct connection.},
  language = {en},
  number = {1},
  journal = {Psychological Review},
  doi = {10.1037/a0026435},
  author = {Solway, Alec and Botvinick, Matthew M.},
  year = {2012},
  pages = {120-154},
  file = {/Users/lancelotdacosta/Zotero/storage/5EUPCWCT/Solway and Botvinick - 2012 - Goal-directed decision making as probabilistic inf.pdf;/Users/lancelotdacosta/Zotero/storage/NJKKU6AU/Solway and Botvinick - 2012 - Goal-directed decision making as probabilistic inf.pdf;/Users/lancelotdacosta/Zotero/storage/NVJWZQ3B/Solway and Botvinick - 2012 - Goal-directed decision making as probabilistic inf.pdf;/Users/lancelotdacosta/Zotero/storage/Y8YZISQP/Solway and Botvinick - 2012 - Goal-directed decision making as probabilistic inf.pdf}
}

@incollection{gromovSearchStructurePart2013,
  address = {{Zuerich, Switzerland}},
  title = {In a {{Search}} for a {{Structure}}, {{Part}} 1: {{On Entropy}}},
  isbn = {978-3-03719-120-0},
  shorttitle = {In a {{Search}} for a {{Structure}}, {{Part}} 1},
  abstract = {Mathematics is about ''interesting structures''. What make a structure interesting is an abundance of interesting problems; we study a structure by solving these problems.},
  language = {en},
  booktitle = {European {{Congress}} of {{Mathematics Krak{\'o}w}}, 2 \textendash{} 7 {{July}}, 2012},
  publisher = {{European Mathematical Society Publishing House}},
  doi = {10.4171/120-1/4},
  author = {Gromov, Misha},
  editor = {Lata{\l}a, Rafa{\l} and Ruci{\'n}ski, Andrzej and Strzelecki, Pawe{\l} and {\'S}wi{\k{a}}tkowski, Jacek and Wrzosek, Dariusz and Zakrzewski, Piotr},
  month = nov,
  year = {2013},
  pages = {51-78},
  file = {/Users/lancelotdacosta/Zotero/storage/5EMDVT7R/Gromov - 2013 - In a Search for a Structure, Part 1 On Entropy.pdf;/Users/lancelotdacosta/Zotero/storage/AIK8NQZD/Gromov - 2013 - In a Search for a Structure, Part 1 On Entropy.pdf;/Users/lancelotdacosta/Zotero/storage/GHWA44D8/Gromov - 2013 - In a Search for a Structure, Part 1 On Entropy.pdf;/Users/lancelotdacosta/Zotero/storage/UZ5MVRW3/Gromov - 2013 - In a Search for a Structure, Part 1 On Entropy.pdf}
}

@article{pougetInferenceComputationPopulation2003,
  title = {Inference and Computation with Population Codes},
  volume = {26},
  issn = {0147-006X, 1545-4126},
  shorttitle = {I},
  language = {en},
  number = {1},
  journal = {Annual Review of Neuroscience},
  doi = {10.1146/annurev.neuro.26.041002.131112},
  author = {Pouget, Alexandre and Dayan, Peter and Zemel, Richard S.},
  month = mar,
  year = {2003},
  pages = {381-410},
  file = {/Users/lancelotdacosta/Zotero/storage/5MVEC3J9/Pouget et al. - 2003 - I span style=font-variantsmall-caps\;NFERENCE .pdf}
}

@article{bayerMidbrainDopamineNeurons2005,
  title = {Midbrain {{Dopamine Neurons Encode}} a {{Quantitative Reward Prediction Error Signal}}},
  volume = {47},
  issn = {08966273},
  abstract = {The midbrain dopamine neurons are hypothesized to provide a physiological correlate of the reward prediction error signal required by current models of reinforcement learning. We examined the activity of single dopamine neurons during a task in which subjects learned by trial and error when to make an eye movement for a juice reward. We found that these neurons encoded the difference between the current reward and a weighted average of previous rewards, a reward prediction error, but only for outcomes that were better than expected. Thus, the firing rate of midbrain dopamine neurons is quantitatively predicted by theoretical descriptions of the reward prediction error signal used in reinforcement learning models for circumstances in which this signal has a positive value. We also found that the dopamine system continued to compute the reward prediction error even when the behavioral policy of the animal was only weakly influenced by this computation.},
  language = {en},
  number = {1},
  journal = {Neuron},
  doi = {10.1016/j.neuron.2005.05.020},
  author = {Bayer, Hannah M. and Glimcher, Paul W.},
  month = jul,
  year = {2005},
  pages = {129-141},
  file = {/Users/lancelotdacosta/Zotero/storage/4VMSDPMZ/Bayer and Glimcher - 2005 - Midbrain Dopamine Neurons Encode a Quantitative Re.pdf;/Users/lancelotdacosta/Zotero/storage/5VIBR5D5/Bayer and Glimcher - 2005 - Midbrain Dopamine Neurons Encode a Quantitative Re.pdf;/Users/lancelotdacosta/Zotero/storage/GVGK3PTQ/Bayer and Glimcher - 2005 - Midbrain Dopamine Neurons Encode a Quantitative Re.pdf;/Users/lancelotdacosta/Zotero/storage/YSELJPMA/Bayer and Glimcher - 2005 - Midbrain Dopamine Neurons Encode a Quantitative Re.pdf}
}

@article{chenWassersteinNaturalGradient2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.08380},
  primaryClass = {cs, math},
  title = {Wasserstein Natural Gradient in Statistical Manifolds with Continuous Sample Space},
  abstract = {We study the Wasserstein natural gradient in parametric statistical models with continuous sample space. Our approach is to pull back the L2-Wasserstein metric tensor in probability density space to parameter space, under which the parameter space become a Riemannian manifold, named the Wasserstein statistical manifold. The gradient flow and natural gradient descent method in parameter space are then derived. When parameterized densities lie in R, we show the induced metric tensor establishes an explicit formula. Computationally, optimization problems can be accelerated by the proposed Wasserstein natural gradient descent, if the objective function is the Wasserstein distance. Examples are presented to demonstrate its effectiveness in several parametric statistical models.},
  language = {en},
  journal = {arXiv:1805.08380 [cs, math]},
  author = {Chen, Yifan and Li, Wuchen},
  month = may,
  year = {2018},
  keywords = {Computer Science - Information Theory,Mathematics - Optimization and Control},
  file = {/Users/lancelotdacosta/Zotero/storage/HAK8RS3M/Chen and Li - 2018 - Wasserstein natural gradient in statistical manifo.pdf}
}

@article{mantonVARIOUSGENERALISATIONSOPTIMISATION,
  title = {{{ON THE VARIOUS GENERALISATIONS OF OPTIMISATION ALGORITHMS TO MANIFOLDS}}},
  abstract = {Numerical minimisation of a cost function on Euclidean space is a well studied problem. Sometimes though, the most appropriate formulation of an optimisation problem is not in Euclidean space, but rather, it is required to minimise a cost function defined on a (not necessarily Riemannian) manifold. A natural question is how best to generalise existing optimisation algorithms to the manifold setting. This paper reviews and draws connections between existing techniques. It also proposes several new ideas. These ideas are in their infancy; they are intended to motivate further research in this area.},
  language = {en},
  author = {Manton, Jonathan H},
  pages = {6},
  file = {/Users/lancelotdacosta/Zotero/storage/CHBIZUYX/Manton - ON THE VARIOUS GENERALISATIONS OF OPTIMISATION ALG.pdf;/Users/lancelotdacosta/Zotero/storage/SGN9J5FG/Manton - ON THE VARIOUS GENERALISATIONS OF OPTIMISATION ALG.pdf;/Users/lancelotdacosta/Zotero/storage/UZPBWBE4/Manton - ON THE VARIOUS GENERALISATIONS OF OPTIMISATION ALG.pdf;/Users/lancelotdacosta/Zotero/storage/WCAZXEWU/Manton - ON THE VARIOUS GENERALISATIONS OF OPTIMISATION ALG.pdf}
}

@article{hauserPrinciplesRiemannianGeometry,
  title = {Principles of {{Riemannian Geometry}}  in {{Neural Networks}}},
  abstract = {This study deals with neural networks in the sense of geometric transformations acting on the coordinate representation of the underlying data manifold which the data is sampled from. It forms part of an attempt to construct a formalized general theory of neural networks in the setting of Riemannian geometry. From this perspective, the following theoretical results are developed and proven for feedforward networks. First it is shown that residual neural networks are nite di erence approximations to dynamical systems of rst order di erential equations, as opposed to ordinary networks that are static. This implies that the network is learning systems of di erential equations governing the coordinate transformations that represent the data. Second it is shown that a closed form solution of the metric tensor on the underlying data manifold can be found by backpropagating the coordinate representations learned by the neural network itself. This is formulated in a formal abstract sense as a sequence of Lie group actions on the metric bre space in the principal and associated bundles on the data manifold. Toy experiments were run to con rm parts of the proposed theory, as well as to provide intuitions as to how neural networks operate on data.},
  language = {en},
  author = {Hauser, Michael and Ray, Asok},
  pages = {10},
  file = {/Users/lancelotdacosta/Zotero/storage/KHFMKC2G/Hauser and Ray - Principles of Riemannian Geometry  in Neural Netwo.pdf}
}

@article{rollsProcessingSpeedCerebral1994,
  title = {Processing Speed in the Cerebral Cortex and the Neurophysiology of Visual Masking},
  volume = {257},
  issn = {0962-8452, 1471-2954},
  abstract = {In experiments to investigate the duration of the time for which cortical neurons respond when the identification of a visual stim ulus is ju st possible, we presented a test face stim ulus for 16 ms, and followed it at different intervals by a m asking stim ulus (either an N\textemdash{}O p attern or a face) while recording from single neurons in the tem poral visual cortex of m acaques. W hen there was no m ask the cells responded to the 16 ms of the test stim ulus for 200\textemdash{}300 ms, far longer th an the presentation time. W e suggest th a t this reflects the operation of a short-term memory system implemented in cortical circuitry. If the mask was a stim ulus w hich did not stim ulate the cells (either a non-face p a tte rn or a face w hich was a noneffective stimulus for that cell), then, as the interval between the onset of the test stimulus and the onset of the m ask stim ulus (the stim ulus onset asynchrony) was reduced, the length of tim e for w hich the cells fired in response to the test stim ulus was reduced. It is suggested th at this is due to the m ask stim ulating adjacent cells in the cortex w hich by lateral inhibition reduce the responses of the cells activated by the test stimulus. W hen the stimulus onset asynchrony was 20 ms, face-selective neurons in the inferior tem poral cortex of m acaques responded for a period of 20\textemdash{}30 ms before their firing was in terru p ted by the mask. W ith the same test-mask stimulus onset asynchrony of 20 ms, hum ans could just identify which of six faces was shown. These results provide evidence th a t a cortical area can perform its com putation necessary for the recognition of a visual stim ulus in 20\textemdash{}30 ms, and provide a fundam ental constraint which must be accounted for in any theory of cortical com putation.},
  language = {en},
  number = {1348},
  journal = {Proceedings of the Royal Society of London. Series B: Biological Sciences},
  doi = {10.1098/rspb.1994.0087},
  author = {Rolls, E T and Tovee, M J},
  month = jul,
  year = {1994},
  pages = {9-15},
  file = {/Users/lancelotdacosta/Zotero/storage/DU2GK2SZ/1994 - Processing speed in the cerebral cortex and the ne.pdf;/Users/lancelotdacosta/Zotero/storage/MBW82T68/1994 - Processing speed in the cerebral cortex and the ne.pdf;/Users/lancelotdacosta/Zotero/storage/PFH75P9H/1994 - Processing speed in the cerebral cortex and the ne.pdf;/Users/lancelotdacosta/Zotero/storage/VJUPT6BJ/1994 - Processing speed in the cerebral cortex and the ne.pdf}
}

@article{brownReviewMemoryEvolutive,
  title = {Review: {{Memory Evolutive Systems}}},
  abstract = {This is a review of the book `Memory Evolutive Systems; Hierarchy, Emergence, Cognition', by A Ehresmann and J.P. Vanbremeersch. I welcome the use of category theory and the notion of colimit as a way of describing how complex hierarchical systems can be organised, and the notion of categories varying with time to give a notion of an evolving system. In this review I also point out the relation of the notion of colimit to ideas of communication; the necessity of communications to be symbolic representations; and the use of an analogy with mathematics to spell out some of the necessities of such a mode of communication to be powerful, robust and efficient.},
  language = {en},
  author = {Brown, Ronald},
  pages = {9},
  file = {/Users/lancelotdacosta/Zotero/storage/DYR3ST77/Brown - Review Memory Evolutive Systems.pdf}
}

@article{frankSimpleUnityFundamental2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1904.00825},
  primaryClass = {cond-mat, q-bio},
  title = {Simple Unity among the Fundamental Equations of Science},
  abstract = {The Price equation describes the change in populations. Change concerns some value, such as biological fitness, information or physical work. The Price equation reveals universal aspects for the nature of change, independently of the meaning ascribed to values. By understanding those universal aspects, we can see more clearly why fundamental mathematical results in different disciplines often share a common form. We can also interpret more clearly the meaning of key results within each discipline. For example, the mathematics of natural selection in biology has a form closely related to information theory and physical entropy. Does that mean that natural selection is about information or entropy? Or do natural selection, information and entropy arise as interpretations of a common underlying abstraction? The Price equation suggests the latter. The Price equation achieves its abstract generality by partitioning change into two terms. The first term naturally associates with the direct forces that cause change. The second term naturally associates with the changing frame of reference. In the Price equation's canonical form, total change remains zero because the conservation of total probability requires that all probabilities invariantly sum to one. Much of the shared common form for the mathematics of different disciplines may arise from that seemingly trivial invariance of total probability, which leads to the partitioning of total change into equal and opposite components of the direct forces and the changing frame of reference.},
  language = {en},
  journal = {arXiv:1904.00825 [cond-mat, q-bio]},
  author = {Frank, Steven A.},
  month = mar,
  year = {2019},
  keywords = {Condensed Matter - Statistical Mechanics,Computer Science - Information Theory,Quantitative Biology - Populations and Evolution},
  file = {/Users/lancelotdacosta/Zotero/storage/UVGJ5WUH/Frank - 2019 - Simple unity among the fundamental equations of sc.pdf}
}

@article{bassettSmallWorldBrainNetworks2006,
  title = {Small-{{World Brain Networks}}},
  volume = {12},
  issn = {1073-8584, 1089-4098},
  language = {en},
  number = {6},
  journal = {The Neuroscientist},
  doi = {10.1177/1073858406293182},
  author = {Bassett, Danielle Smith and Bullmore, Ed},
  month = dec,
  year = {2006},
  pages = {512-523},
  file = {/Users/lancelotdacosta/Zotero/storage/K4CGFZVC/Bassett and Bullmore - 2006 - Small-World Brain Networks.pdf}
}

@article{seymourHigherorderLearningHumans2004,
  title = {Higher-Order Learning in Humans},
  volume = {429},
  language = {en},
  author = {Seymour, Ben and O'Doherty, John P and Dayan, Peter and Koltzenburg, Martin and Jones, Anthony K and Dolan, Raymond J and Friston, Karl J and Frackowiak, Richard S},
  year = {2004},
  pages = {4},
  file = {/Users/lancelotdacosta/Zotero/storage/5N7MMW2N/Seymour et al. - 2004 - higher-order learning in humans.pdf;/Users/lancelotdacosta/Zotero/storage/838S22RK/Seymour et al. - 2004 - higher-order learning in humans.pdf;/Users/lancelotdacosta/Zotero/storage/EJJQWQ9F/Seymour et al. - 2004 - higher-order learning in humans.pdf;/Users/lancelotdacosta/Zotero/storage/Z2NRGDK7/Seymour et al. - 2004 - higher-order learning in humans.pdf}
}

@book{oprisPhysicsMindBrain2017,
  address = {{New York, NY}},
  title = {The Physics of the Mind and Brain Disorders: Integrated Neural Circuits Supporting the Emergence of Mind},
  isbn = {978-3-319-29672-2},
  shorttitle = {The Physics of the Mind and Brain Disorders},
  language = {en},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Opris, Ioan},
  year = {2017},
  file = {/Users/lancelotdacosta/Zotero/storage/57DETWIF/Opris - 2017 - The physics of the mind and brain disorders integ.pdf;/Users/lancelotdacosta/Zotero/storage/9DNEYXFS/Opris - 2017 - The physics of the mind and brain disorders integ.pdf;/Users/lancelotdacosta/Zotero/storage/DPHCATBB/Opris - 2017 - The physics of the mind and brain disorders integ.pdf;/Users/lancelotdacosta/Zotero/storage/E6JLBJHM/Opris - 2017 - The physics of the mind and brain disorders integ.pdf}
}

@article{fiorilloGeneralTheoryNeural2008,
  title = {Towards a {{General Theory}} of {{Neural Computation Based}} on {{Prediction}} by {{Single Neurons}}},
  volume = {3},
  issn = {1932-6203},
  abstract = {Although there has been tremendous progress in understanding the mechanics of the nervous system, there has not been a general theory of its computational function. Here I present a theory that relates the established biophysical properties of single generic neurons to principles of Bayesian probability theory, reinforcement learning and efficient coding. I suggest that this theory addresses the general computational problem facing the nervous system. Each neuron is proposed to mirror the function of the whole system in learning to predict aspects of the world related to future reward. According to the model, a typical neuron receives current information about the state of the world from a subset of its excitatory synaptic inputs, and prior information from its other inputs. Prior information would be contributed by synaptic inputs representing distinct regions of space, and by different types of non-synaptic, voltage-regulated channels representing distinct periods of the past. The neuron's membrane voltage is proposed to signal the difference between current and prior information (``prediction error'' or ``surprise''). A neuron would apply a Hebbian plasticity rule to select those excitatory inputs that are the most closely correlated with reward but are the least predictable, since unpredictable inputs provide the neuron with the most ``new'' information about future reward. To minimize the error in its predictions and to respond only when excitation is ``new and surprising,'' the neuron selects amongst its prior information sources through an anti-Hebbian rule. The unique inputs of a mature neuron would therefore result from learning about spatial and temporal patterns in its local environment, and by extension, the external world. Thus the theory describes how the structure of the mature nervous system could reflect the structure of the external world, and how the complexity and intelligence of the system might develop from a population of undifferentiated neurons, each implementing similar learning algorithms.},
  language = {en},
  number = {10},
  journal = {PLoS ONE},
  doi = {10.1371/journal.pone.0003298},
  author = {Fiorillo, Christopher D.},
  editor = {Sporns, Olaf},
  month = oct,
  year = {2008},
  pages = {e3298},
  file = {/Users/lancelotdacosta/Zotero/storage/A2UGPDLF/Fiorillo - 2008 - Towards a General Theory of Neural Computation Bas.PDF}
}

@article{georgeMathematicalTheoryCortical2009,
  title = {Towards a {{Mathematical Theory}} of {{Cortical Micro}}-Circuits},
  volume = {5},
  issn = {1553-7358},
  abstract = {The theoretical setting of hierarchical Bayesian inference is gaining acceptance as a framework for understanding cortical computation. In this paper, we describe how Bayesian belief propagation in a spatio-temporal hierarchical model, called Hierarchical Temporal Memory (HTM), can lead to a mathematical model for cortical circuits. An HTM node is abstracted using a coincidence detector and a mixture of Markov chains. Bayesian belief propagation equations for such an HTM node define a set of functional constraints for a neuronal implementation. Anatomical data provide a contrasting set of organizational constraints. The combination of these two constraints suggests a theoretically derived interpretation for many anatomical and physiological features and predicts several others. We describe the pattern recognition capabilities of HTM networks and demonstrate the application of the derived circuits for modeling the subjective contour effect. We also discuss how the theory and the circuit can be extended to explain cortical features that are not explained by the current model and describe testable predictions that can be derived from the model.},
  language = {en},
  number = {10},
  journal = {PLoS Computational Biology},
  doi = {10.1371/journal.pcbi.1000532},
  author = {George, Dileep and Hawkins, Jeff},
  editor = {Friston, Karl J.},
  month = oct,
  year = {2009},
  pages = {e1000532},
  file = {/Users/lancelotdacosta/Zotero/storage/DNSQ6L7A/George and Hawkins - 2009 - Towards a Mathematical Theory of Cortical Micro-ci.PDF;/Users/lancelotdacosta/Zotero/storage/TC2E37EQ/George and Hawkins - 2009 - Towards a Mathematical Theory of Cortical Micro-ci.PDF;/Users/lancelotdacosta/Zotero/storage/XFF8D2SA/George and Hawkins - 2009 - Towards a Mathematical Theory of Cortical Micro-ci.PDF;/Users/lancelotdacosta/Zotero/storage/ZJ7NSXQA/George and Hawkins - 2009 - Towards a Mathematical Theory of Cortical Micro-ci.PDF}
}

@article{kulkarniUnsupervisedLearningObject2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.11883},
  primaryClass = {cs},
  title = {Unsupervised {{Learning}} of {{Object Keypoints}} for {{Perception}} and {{Control}}},
  abstract = {The study of object representations in computer vision has primarily focused on developing representations that are useful for image classification, object detection, or semantic segmentation as downstream tasks. In this work we aim to learn object representations that are useful for control and reinforcement learning (RL). To this end, we introduce Transporter, a neural network architecture for discovering concise geometric object representations in terms of keypoints or image-space coordinates. Our method learns from raw video frames in a fully unsupervised manner, by transporting learnt image features between video frames using a keypoint bottleneck. The discovered keypoints track objects and object parts across long time-horizons more accurately than recent similar methods. Furthermore, consistent long-term tracking enables two notable results in control domains \textendash{} (1) using the keypoint co-ordinates and corresponding image features as inputs enables highly sample-efficient reinforcement learning; (2) learning to explore by controlling keypoint locations drastically reduces the search space, enabling deep exploration (leading to states unreachable through random action exploration) without any extrinsic rewards.},
  language = {en},
  journal = {arXiv:1906.11883 [cs]},
  author = {Kulkarni, Tejas and Gupta, Ankush and Ionescu, Catalin and Borgeaud, Sebastian and Reynolds, Malcolm and Zisserman, Andrew and Mnih, Volodymyr},
  month = jun,
  year = {2019},
  keywords = {Computer Science - Machine Learning,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/lancelotdacosta/Zotero/storage/34KA66Q7/Kulkarni et al. - 2019 - Unsupervised Learning of Object Keypoints for Perc.pdf;/Users/lancelotdacosta/Zotero/storage/PUK64633/Kulkarni et al. - 2019 - Unsupervised Learning of Object Keypoints for Perc.pdf;/Users/lancelotdacosta/Zotero/storage/SF9S2QFY/Kulkarni et al. - 2019 - Unsupervised Learning of Object Keypoints for Perc.pdf;/Users/lancelotdacosta/Zotero/storage/XHBQMPTT/Kulkarni et al. - 2019 - Unsupervised Learning of Object Keypoints for Perc.pdf}
}

@article{fristonValuedependentSelectionBrain1994,
  title = {Value-Dependent Selection in the Brain: {{Simulation}} in a Synthetic Neural Model},
  volume = {59},
  issn = {03064522},
  shorttitle = {Value-Dependent Selection in the Brain},
  abstract = {Many forms of learning depend on the ability of an organism to sense and react to the adaptive value of its behavior. Such value, if reflected in the activity of specific neural structures (neural value systems), can selectively increase the probability of adaptive behaviors by modulating synaptic changes in the circuits relevant to those behaviors. Neuromodulatory systems in the brain are well suited to carry out this process since they respond to evolutionarily important cues (innate value), broadcast their responses to widely distributed areas of the brain through diffuse projections, and release substances that can modulate changes in synaptic strength.},
  language = {en},
  number = {2},
  journal = {Neuroscience},
  doi = {10.1016/0306-4522(94)90592-4},
  author = {Friston, K.J. and Tononi, G. and Reeke, G.N. and Sporns, O. and Edelman, G.M.},
  month = mar,
  year = {1994},
  pages = {229-243},
  file = {/Users/lancelotdacosta/Zotero/storage/5SPT8S4G/Friston et al. - 1994 - Value-dependent selection in the brain Simulation.pdf;/Users/lancelotdacosta/Zotero/storage/DBLC4TKQ/Friston et al. - 1994 - Value-dependent selection in the brain Simulation.pdf;/Users/lancelotdacosta/Zotero/storage/GSEVITH5/Friston et al. - 1994 - Value-dependent selection in the brain Simulation.pdf;/Users/lancelotdacosta/Zotero/storage/QIZ5HNIC/Friston et al. - 1994 - Value-dependent selection in the brain Simulation.pdf}
}

@article{houthooftVIMEVariationalInformation2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.09674},
  primaryClass = {cs, stat},
  title = {{{VIME}}: {{Variational Information Maximizing Exploration}}},
  shorttitle = {{{VIME}}},
  abstract = {Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as -greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.},
  language = {en},
  journal = {arXiv:1605.09674 [cs, stat]},
  author = {Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
  month = may,
  year = {2016},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Robotics},
  file = {/Users/lancelotdacosta/Zotero/storage/EYMRJWHU/Houthooft et al. - 2016 - VIME Variational Information Maximizing Explorati.pdf;/Users/lancelotdacosta/Zotero/storage/J5ZPWXPP/Houthooft et al. - 2016 - VIME Variational Information Maximizing Explorati.pdf;/Users/lancelotdacosta/Zotero/storage/KNAWMTZD/Houthooft et al. - 2016 - VIME Variational Information Maximizing Explorati.pdf;/Users/lancelotdacosta/Zotero/storage/LRW3RWQ8/Houthooft et al. - 2016 - VIME Variational Information Maximizing Explorati.pdf}
}

@article{hubelReceptiveFieldsSingle1959,
  title = {Receptive Fields of Single Neurones in the Cat's Striate Cortex},
  volume = {148},
  issn = {00223751},
  language = {en},
  number = {3},
  journal = {The Journal of Physiology},
  doi = {10.1113/jphysiol.1959.sp006308},
  author = {Hubel, D. H. and Wiesel, T. N.},
  month = oct,
  year = {1959},
  pages = {574-591},
  file = {/Users/lancelotdacosta/Zotero/storage/T8KAQFG5/Hubel and Wiesel - 1959 - Receptive fields of single neurones in the cat's s.pdf}
}

@article{yedidiaConstructingFreeEnergyApproximations2005,
  title = {Constructing {{Free}}-{{Energy Approximations}} and {{Generalized Belief Propagation Algorithms}}},
  volume = {51},
  issn = {0018-9448},
  abstract = {Important inference problems in statistical physics, computer vision, error-correcting coding theory, and artificial intelligence can all be reformulated as the computation of marginal probabilities on factor graphs. The belief propagation (BP) algorithm is an efficient way to solve these problems that is exact when the factor graph is a tree, but only approximate when the factor graph has cycles. We show that BP fixed points correspond to the stationary points of the Bethe approximation of the free energy for a factor graph. We explain how to obtain regionbased free energy approximations that improve the Bethe approximation, and corresponding generalized belief propagation (GBP) algorithms. We emphasize the conditions a free energy approximation must satisfy in order to be a v\textasciidieresis{}alido\textasciidieresis{}r m\textasciidieresis{} axent-normala\textasciidieresis{}pproximation. We describe the relationship between four difference methods that can be used to generate valid approximations: the B\textasciidieresis{} ethe method,\textasciidieresis{}the \textasciidieresis{}junction graph method,\textasciidieresis{}the c\textasciidieresis{}luster variation method,a\textasciidieresis{}nd the \textasciidieresis{}region graph method.F\textasciidieresis{} inally, we explain how to tell whether a region-based approximation, and its corresponding GBP algorithm, is likely to be accurate, and describe empirical results showing that GBP can significantly outperform BP.},
  language = {en},
  number = {7},
  journal = {IEEE Transactions on Information Theory},
  doi = {10.1109/TIT.2005.850085},
  author = {Yedidia, J.S. and Freeman, W.T. and Weiss, Y.},
  month = jul,
  year = {2005},
  pages = {2282-2312},
  file = {/Users/lancelotdacosta/Zotero/storage/WH7JFN8C/Yedidia et al. - 2005 - Constructing Free-Energy Approximations and Genera.pdf}
}

@article{raoDecisionMakingUncertainty2010,
  title = {Decision {{Making Under Uncertainty}}: {{A Neural Model Based}} on {{Partially Observable Markov Decision Processes}}},
  volume = {4},
  issn = {1662-5188},
  shorttitle = {Decision {{Making Under Uncertainty}}},
  abstract = {A fundamental problem faced by animals is learning to select actions based on noisy sensory information and incomplete knowledge of the world. It has been suggested that the brain engages in Bayesian inference during perception but how such probabilistic representations are used to select actions has remained unclear. Here we propose a neural model of action selection and decision making based on the theory of partially observable Markov decision processes (POMDPs). Actions are selected based not on a single ``optimal'' estimate of state but on the posterior distribution over states (the ``belief'' state). We show how such a model provides a unified framework for explaining experimental results in decision making that involve both information gathering and overt actions. The model utilizes temporal difference (TD) learning for maximizing expected reward.The resulting neural architecture posits an active role for the neocortex in belief computation while ascribing a role to the basal ganglia in belief representation, value computation, and action selection. When applied to the random dots motion discrimination task, model neurons representing belief exhibit responses similar to those of LIP neurons in primate neocortex. The appropriate threshold for switching from information gathering to overt actions emerges naturally during reward maximization. Additionally, the time course of reward prediction error in the model shares similarities with dopaminergic responses in the basal ganglia during the random dots task. For tasks with a deadline, the model learns a decision making strategy that changes with elapsed time, predicting a collapsing decision threshold consistent with some experimental studies. The model provides a new framework for understanding neural decision making and suggests an important role for interactions between the neocortex and the basal ganglia in learning the mapping between probabilistic sensory representations and actions that maximize rewards.},
  language = {en},
  journal = {Frontiers in Computational Neuroscience},
  doi = {10.3389/fncom.2010.00146},
  author = {Rao, Rajesh P. N.},
  year = {2010},
  file = {/Users/lancelotdacosta/Zotero/storage/T72D3BUZ/Rao - 2010 - Decision Making Under Uncertainty A Neural Model .pdf}
}

@article{maassComputationalPowerWinnerTakeAll2000,
  title = {On the {{Computational Power}} of {{Winner}}-{{Take}}-{{All}}},
  volume = {12},
  issn = {0899-7667, 1530-888X},
  language = {en},
  number = {11},
  journal = {Neural Computation},
  doi = {10.1162/089976600300014827},
  author = {Maass, Wolfgang},
  month = nov,
  year = {2000},
  pages = {2519-2535},
  file = {/Users/lancelotdacosta/Zotero/storage/EB6RIR44/Maass - 2000 - On the Computational Power of Winner-Take-All.pdf}
}

@article{fristonFreeEnergyPrinciple2019,
  title = {A Free Energy Principle for a Particular Physics},
  abstract = {This monograph attempts a theory of every `thing' that can be distinguished from other `things' in a statistical sense. The ensuing statistical independencies, mediated by Markov blankets, speak to a recursive composition of ensembles (of things) at increasingly higher spatiotemporal scales. This decomposition provides a description of small things; e.g., quantum mechanics \textendash{} via the Schr{\"o}dinger equation, ensembles of small things \textendash{} via statistical mechanics and related fluctuation theorems, through to big things \textendash{} via classical mechanics. These descriptions are complemented with a Bayesian mechanics for autonomous or active things. Although this work provides a formulation of every `thing', its main contribution is to examine the implications of Markov blankets for selforganisation to nonequilibrium steady-state. In brief, we recover an information geometry and accompanying free energy principle that allows one to interpret the internal states of something as representing or making inferences about its external states. The ensuing Bayesian mechanics is compatible with quantum, statistical and classical mechanics and may offer a formal description of lifelike particles.},
  language = {en},
  journal = {BioArxiv},
  author = {Friston, Karl},
  year = {2019},
  pages = {148},
  file = {/Users/lancelotdacosta/Zotero/storage/UIZFS5MT/Friston - A FREE ENERGY PRINCIPLE FOR A PARTICULAR PHYSICS.pdf}
}

@article{fristonTheoryCorticalResponses2005,
  title = {A Theory of Cortical Responses},
  volume = {360},
  issn = {0962-8436, 1471-2970},
  language = {en},
  number = {1456},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  doi = {10.1098/rstb.2005.1622},
  author = {Friston, Karl},
  month = apr,
  year = {2005},
  pages = {815-836},
  file = {/Users/lancelotdacosta/Zotero/storage/W3P99RI2/Friston - 2005 - A theory of cortical responses.pdf}
}

@incollection{pennyBayesianModelSelection2007,
  title = {Bayesian Model Selection and Averaging},
  isbn = {978-0-12-372560-8},
  language = {en},
  booktitle = {Statistical {{Parametric Mapping}}},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-012372560-8/50035-8},
  author = {Penny, W.D. and Mattout, J. and {Trujillo-Barreto}, N.},
  year = {2007},
  pages = {454-467},
  file = {/Users/lancelotdacosta/Zotero/storage/8GXG3QNP/Penny et al. - 2007 - Bayesian model selection and averaging.pdf}
}

@article{bastosCanonicalMicrocircuitsPredictive2012,
  title = {Canonical {{Microcircuits}} for {{Predictive Coding}}},
  volume = {76},
  issn = {08966273},
  language = {en},
  number = {4},
  journal = {Neuron},
  doi = {10.1016/j.neuron.2012.10.038},
  author = {Bastos, Andre M. and Usrey, W. Martin and Adams, Rick A. and Mangun, George R. and Fries, Pascal and Friston, Karl J.},
  month = nov,
  year = {2012},
  pages = {695-711},
  file = {/Users/lancelotdacosta/Zotero/storage/2BWTWN3M/Bastos et al. - 2012 - Canonical Microcircuits for Predictive Coding.pdf}
}

@article{fristonHierarchicalModelsBrain2008,
  title = {Hierarchical {{Models}} in the {{Brain}}},
  volume = {4},
  issn = {1553-7358},
  abstract = {This paper describes a general model that subsumes many parametric models for continuous data. The model comprises hidden layers of state-space or dynamic causal models, arranged so that the output of one provides input to another. The ensuing hierarchy furnishes a model for many types of data, of arbitrary complexity. Special cases range from the general linear model for static data to generalised convolution models, with system noise, for nonlinear time-series analysis. Crucially, all of these models can be inverted using exactly the same scheme, namely, dynamic expectation maximization. This means that a single model and optimisation scheme can be used to invert a wide range of models. We present the model and a brief review of its inversion to disclose the relationships among, apparently, diverse generative models of empirical data. We then show that this inversion can be formulated as a simple neural network and may provide a useful metaphor for inference and learning in the brain.},
  language = {en},
  number = {11},
  journal = {PLoS Computational Biology},
  doi = {10.1371/journal.pcbi.1000211},
  author = {Friston, Karl},
  editor = {Sporns, Olaf},
  month = nov,
  year = {2008},
  pages = {e1000211},
  file = {/Users/lancelotdacosta/Zotero/storage/62ULM7RU/Friston - 2008 - Hierarchical Models in the Brain.PDF}
}

@inproceedings{attiasPlanningProbabilisticInference2003,
  title = {Planning by {{Probabilistic Inference}}},
  abstract = {This paper presents and demonstrates a new approach to the problem of planning under uncertainty. Actions are treated as hidden variables, with their own prior distributions, in a probabilistic generative model involving actions and states. Planning is done by computing the posterior distribution over actions, conditioned on reaching the goal state within a specified number of steps. Under the new formulation, the toolbox of inference techniques be brought to bear on the planning problem. This paper focuses on problems with discrete actions and states, and discusses some extensions.},
  language = {en},
  booktitle = {9th {{Int}}. {{Workshop}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Attias, Hagai},
  year = {2003},
  pages = {8},
  file = {/Users/lancelotdacosta/Zotero/storage/YGBYBEL3/Attias - Planning by Probabilistic Inference.pdf}
}

@article{hintonWakesleepAlgorithmUnsupervised1995,
  title = {The "Wake-Sleep" Algorithm for Unsupervised Neural Networks},
  volume = {268},
  issn = {0036-8075, 1095-9203},
  language = {en},
  number = {5214},
  journal = {Science},
  doi = {10.1126/science.7761831},
  author = {Hinton, G. and Dayan, P and Frey, B. and Neal, R.},
  month = may,
  year = {1995},
  pages = {1158-1161},
  file = {/Users/lancelotdacosta/Zotero/storage/YY2CKNRB/Hinton et al. - 1995 - The wake-sleep algorithm for unsupervised neural.pdf}
}

@article{fristonHistoryFutureBayesian2012,
  title = {The History of the Future of the {{Bayesian}} Brain},
  volume = {62},
  issn = {10538119},
  abstract = {The slight perversion of the original title of this piece (The Future of the Bayesian Brain) reflects my attempt to write prospectively about `Science and Stories' over the past 20 years. I will meet this challenge by dealing with the future and then turning to its history. The future of the Bayesian brain (in neuroimaging) is clear: it is the application of dynamic causal modeling to understand how the brain conforms to the free energy principle. In this context, the Bayesian brain is a corollary of the free energy principle, which says that any self organizing system (like a brain or neuroimaging community) must maximize the evidence for its own existence, which means it must minimize its free energy using a model of its world. Dynamic causal modeling involves finding models of the brain that have the greatest evidence or the lowest free energy. In short, the future of imaging neuroscience is to refine models of the brain to minimize free energy, where the brain refines models of the world to minimize free energy. This endeavor itself minimizes free energy because our community is itself a self organizing system. I cannot imagine an alternative future that has the same beautiful self consistency as mine. Having dispensed with the future, we can now focus on the past, which is much more interesting: \textcopyright{} 2011 Elsevier Inc. Open access under CC BY license.},
  language = {en},
  number = {2},
  journal = {NeuroImage},
  doi = {10.1016/j.neuroimage.2011.10.004},
  author = {Friston, Karl},
  month = aug,
  year = {2012},
  pages = {1230-1233},
  file = {/Users/lancelotdacosta/Zotero/storage/SC5PYTPH/Friston - 2012 - The history of the future of the Bayesian brain.pdf}
}

@article{hohwySelfEvidencingBrain2016,
  title = {The {{Self}}-{{Evidencing Brain}}},
  volume = {50},
  issn = {00294624},
  shorttitle = {The {{Self}}-{{Evidencing Brain}}},
  abstract = {An exciting theory in neuroscience is that the brain is an organ for prediction error minimization (PEM). This theory is rapidly gaining influence and is set to dominate the science of mind and brain in the years to come. PEM has extreme explanatory ambition, and profound philosophical implications. Here, I assume the theory, briefly explain it, and then I argue that PEM implies that the brain is essentially self-evidencing. This means it is imperative to identify an evidentiary boundary between the brain and its environment. This boundary defines the mindworld relation, opens the door to skepticism, and makes the mind transpire as more inferentially secluded and neurocentrically skull-bound than many would nowadays think. Therefore, PEM somewhat deflates contemporary hypotheses that cognition is extended, embodied and enactive; however, it can nevertheless accommodate the kinds of cases that fuel these hypotheses.},
  language = {en},
  number = {2},
  journal = {No{\^u}s},
  doi = {10.1111/nous.12062},
  author = {Hohwy, Jakob},
  month = jun,
  year = {2016},
  pages = {259-285},
  file = {/Users/lancelotdacosta/Zotero/storage/CE3X69GA/Hohwy - 2016 - The Self-Evidencing Brain The Self-Evidencing Bra.pdf}
}

@article{aitchisonYouPredictiveCoding2017,
  title = {With or without You: Predictive Coding and {{Bayesian}} Inference in the Brain},
  volume = {46},
  issn = {09594388},
  shorttitle = {With or without You},
  language = {en},
  journal = {Current Opinion in Neurobiology},
  doi = {10.1016/j.conb.2017.08.010},
  author = {Aitchison, Laurence and Lengyel, M{\'a}t{\'e}},
  month = oct,
  year = {2017},
  pages = {219-227},
  file = {/Users/lancelotdacosta/Zotero/storage/XLCNHHMF/Aitchison and Lengyel - 2017 - With or without you predictive coding and Bayesia.pdf}
}

@article{chenHeaddirectionCellsRat1994,
  title = {Head-Direction Cells in the Rat Posterior Cortex},
  language = {en},
  journal = {Experimental brain research},
  author = {Chen, Longtang L and Lin, Lie-Huey and Green, Edward J},
  year = {1994},
  pages = {16},
  file = {/Users/lancelotdacosta/Zotero/storage/6UXE3N8S/Chen et al. - Head-direction cells in the rat posterior cortex.pdf}
}

@article{taubeHeaddirectionCellsRecorded1990,
  title = {Head-Direction Cells Recorded from the Postsubiculum in Freely Moving Rats. {{I}}. {{Description}} and Quantitative Analysis},
  volume = {10},
  issn = {0270-6474, 1529-2401},
  language = {en},
  number = {2},
  journal = {The Journal of Neuroscience},
  doi = {10.1523/JNEUROSCI.10-02-00420.1990},
  author = {Taube, Js and Muller, Ru and Ranck, Jb},
  month = feb,
  year = {1990},
  pages = {420-435},
  file = {/Users/lancelotdacosta/Zotero/storage/H7GFXSJ8/Taube et al. - 1990 - Head-direction cells recorded from the postsubicul.pdf}
}

@article{raoPredictiveCodingVisual1999,
  title = {Predictive Coding in the Visual Cortex: A Functional Interpretation of Some Extra-Classical Receptive-Field Effects},
  volume = {2},
  issn = {1097-6256, 1546-1726},
  shorttitle = {Predictive Coding in the Visual Cortex},
  language = {en},
  number = {1},
  journal = {Nature Neuroscience},
  doi = {10.1038/4580},
  author = {Rao, Rajesh P. N. and Ballard, Dana H.},
  month = jan,
  year = {1999},
  pages = {79-87},
  file = {/Users/lancelotdacosta/Zotero/storage/P6ZCARJW/Rao and Ballard - 1999 - Predictive coding in the visual cortex a function.pdf}
}

@article{wagenaarStatespaceDecodingPrimary2011,
  title = {State-Space Decoding of Primary Afferent Neuron Firing Rates},
  volume = {8},
  issn = {1741-2560, 1741-2552},
  abstract = {Kinematic state feedback is important for neuroprostheses to generate stable and adaptive movements of an extremity. State information, represented in the firing rates of populations of primary afferent neurons, can be recorded at the level of the dorsal root ganglia (DRG). Previous work in cats showed the feasibility of using DRG recordings to predict the kinematic state of the hind limb using reverse regression. Although accurate decoding results were attained, reverse regression does not make efficient use of the information embedded in the firing rates of the neural population. In this paper, we present decoding results based on state-space modeling, and show that it is a more principled and more efficient method for decoding the firing rates in an ensemble of primary afferent neurons. In particular, we show that we can extract confounded information from neurons that respond to multiple kinematic parameters, and that including velocity components in the firing rate models significantly increases the accuracy of the decoded trajectory. We show that, on average, state-space decoding is twice as efficient as reverse regression for decoding joint and endpoint kinematics.},
  language = {en},
  number = {1},
  journal = {Journal of Neural Engineering},
  doi = {10.1088/1741-2560/8/1/016002},
  author = {Wagenaar, J B and Ventura, V and Weber, D J},
  month = feb,
  year = {2011},
  pages = {016002},
  file = {/Users/lancelotdacosta/Zotero/storage/NCSDPKV8/Wagenaar et al. - 2011 - State-space decoding of primary afferent neuron fi.pdf}
}

@article{martinez-cantinBayesianExplorationexploitationApproach2009,
  title = {A {{Bayesian}} Exploration-Exploitation Approach for Optimal Online Sensing and Planning with a Visually Guided Mobile Robot},
  volume = {27},
  issn = {0929-5593, 1573-7527},
  language = {en},
  number = {2},
  journal = {Autonomous Robots},
  doi = {10.1007/s10514-009-9130-2},
  author = {{Martinez-Cantin}, Ruben and {de Freitas}, Nando and Brochu, Eric and Castellanos, Jos{\'e} and Doucet, Arnaud},
  month = aug,
  year = {2009},
  pages = {93-103},
  file = {/Users/lancelotdacosta/Zotero/storage/7J5RXWG8/Martinez-Cantin et al. - 2009 - A Bayesian exploration-exploitation approach for o.pdf;/Users/lancelotdacosta/Zotero/storage/KPGPEAES/Martinez-Cantin et al. - 2009 - A Bayesian exploration-exploitation approach for o.pdf}
}

@article{obrienCautionRegardingRules2007,
  title = {A {{Caution Regarding Rules}} of {{Thumb}} for {{Variance Inflation Factors}}},
  volume = {41},
  issn = {0033-5177, 1573-7845},
  abstract = {The Variance Inflation Factor (VIF) and tolerance are both widely used measures of the degree of multi-collinearity of the ith independent variable with the other independent variables in a regression model. Unfortunately, several rules of thumb \textendash{} most commonly the rule of 10 \textendash{} associated with VIF are regarded by many practitioners as a sign of severe or serious multi-collinearity (this rule appears in both scholarly articles and advanced statistical textbooks). When VIF reaches these threshold values researchers often attempt to reduce the collinearity by eliminating one or more variables from their analysis; using Ridge Regression to analyze their data; or combining two or more independent variables into a single index. These techniques for curing problems associated with multi-collinearity can create problems more serious than those they solve. Because of this, we examine these rules of thumb and find that threshold values of the VIF (and tolerance) need to be evaluated in the context of several other factors that influence the variance of regression coefficients. Values of the VIF of 10, 20, 40, or even higher do not, by themselves, discount the results of regression analyses, call for the elimination of one or more independent variables from the analysis, suggest the use of ridge regression, or require combining of independent variable into a single index.},
  language = {en},
  number = {5},
  journal = {Quality \& Quantity},
  doi = {10.1007/s11135-006-9018-6},
  author = {O'brien, Robert M.},
  month = sep,
  year = {2007},
  pages = {673-690},
  file = {/Users/lancelotdacosta/Zotero/storage/SITCR8SM/O’brien - 2007 - A Caution Regarding Rules of Thumb for Variance In.pdf;/Users/lancelotdacosta/Zotero/storage/VQMLUV7B/O’brien - 2007 - A Caution Regarding Rules of Thumb for Variance In.pdf}
}

@article{leeFasterCuttingPlane2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1508.04874},
  primaryClass = {cs, math},
  title = {A {{Faster Cutting Plane Method}} and Its {{Implications}} for {{Combinatorial}} and {{Convex Optimization}}},
  abstract = {In this paper we improve upon the running time for finding a point in a convex set given a separation oracle. In particular, given a separation oracle for a convex set K {$\subset$} Rn that is contained in a box of radius R we show how to either compute a point in K or prove that K does not contain a ball of radius using an expected O(n log(nR/ )) evaluations of the oracle and additional time O(n3 logO(1)(nR/ )). This matches the oracle complexity and improves upon the O(n{$\omega$}+1 log(nR/ )) additional time of the previous fastest algorithm achieved over 25 years ago by Vaidya [103] for the current value of the matrix multiplication constant {$\omega$} {$<$} 2.373 [110, 41] when R/ = O(poly(n)).},
  language = {en},
  journal = {arXiv:1508.04874 [cs, math]},
  author = {Lee, Yin Tat and Sidford, Aaron and Wong, Sam Chiu-wai},
  month = aug,
  year = {2015},
  keywords = {Mathematics - Optimization and Control,Computer Science - Data Structures and Algorithms,Computer Science - Discrete Mathematics,Mathematics - Numerical Analysis},
  file = {/Users/lancelotdacosta/Zotero/storage/APELQ5YV/Lee et al. - 2015 - A Faster Cutting Plane Method and its Implications.pdf;/Users/lancelotdacosta/Zotero/storage/YCJU528A/Lee et al. - 2015 - A Faster Cutting Plane Method and its Implications.pdf}
}

@article{fristonFreeEnergyPrinciple2006,
  title = {A Free Energy Principle for the Brain},
  volume = {100},
  issn = {09284257},
  abstract = {By formulating Helmholtz's ideas about perception, in terms of modern-day theories, one arrives at a model of perceptual inference and learning that can explain a remarkable range of neurobiological facts: using constructs from statistical physics, the problems of inferring the causes of sensory input and learning the causal structure of their generation can be resolved using exactly the same principles. Furthermore, inference and learning can proceed in a biologically plausible fashion. The ensuing scheme rests on Empirical Bayes and hierarchical models of how sensory input is caused. The use of hierarchical models enables the brain to construct prior expectations in a dynamic and context-sensitive fashion. This scheme provides a principled way to understand many aspects of cortical organisation and responses.},
  language = {en},
  number = {1-3},
  journal = {Journal of Physiology-Paris},
  doi = {10.1016/j.jphysparis.2006.10.001},
  author = {Friston, Karl and Kilner, James and Harrison, Lee},
  month = jul,
  year = {2006},
  pages = {70-87},
  file = {/Users/lancelotdacosta/Zotero/storage/3W6T86KI/Friston et al. - 2006 - A free energy principle for the brain.pdf;/Users/lancelotdacosta/Zotero/storage/9RK9WMTT/Friston et al. - 2006 - A free energy principle for the brain.pdf}
}

@article{anderssonGeneralPurposeSoftwareFramework,
  title = {A {{General}}-{{Purpose Software Framework}} for {{Dynamic Optimization}}},
  language = {en},
  author = {Andersson, Joel},
  pages = {186},
  file = {/Users/lancelotdacosta/Zotero/storage/JMVVXHCJ/Andersson - A General-Purpose Software Framework for Dynamic O.pdf}
}

@article{eliasmithLargeScaleModelFunctioning2012,
  title = {A {{Large}}-{{Scale Model}} of the {{Functioning Brain}}},
  volume = {338},
  issn = {0036-8075, 1095-9203},
  language = {en},
  number = {6111},
  journal = {Science},
  doi = {10.1126/science.1225266},
  author = {Eliasmith, C. and Stewart, T. C. and Choo, X. and Bekolay, T. and DeWolf, T. and Tang, Y. and Rasmussen, D.},
  month = nov,
  year = {2012},
  pages = {1202-1205},
  file = {/Users/lancelotdacosta/Zotero/storage/AP958KKH/Eliasmith et al. - 2012 - A Large-Scale Model of the Functioning Brain.pdf}
}

@article{ziaPossibleClassificationNonequilibrium2006,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {cond-mat/0605301},
  title = {A Possible Classification of Nonequilibrium Steady States},
  volume = {39},
  issn = {0305-4470, 1361-6447},
  abstract = {We propose a general classification of nonequilibrium steady states in terms of their stationary probability distribution and the associated probability currents. The stationary probabilities can be represented graph-theoretically as directed labelled trees; closing a single loop in such a graph leads to a representation of probability currents. This classification allows us to identify all choices of transition rates, based on a master equation, which generate the same nonequilibrium steady state. We explore the implications of this freedom, e.g., for entropy production.},
  language = {en},
  number = {24},
  journal = {Journal of Physics A: Mathematical and General},
  doi = {10.1088/0305-4470/39/24/L04},
  author = {Zia, R. K. P. and Schmittmann, B.},
  month = jun,
  year = {2006},
  keywords = {Condensed Matter - Statistical Mechanics},
  pages = {L407-L413},
  file = {/Users/lancelotdacosta/Zotero/storage/256ANNUE/Zia and Schmittmann - 2006 - A possible classification of nonequilibrium steady.pdf}
}

@article{cangTopologicalApproachProtein2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1510.00953},
  primaryClass = {q-bio},
  title = {A Topological Approach for Protein Classification},
  abstract = {Protein function and dynamics are closely related to its sequence and structure. However prediction of protein function and dynamics from its sequence and structure is still a fundamental challenge in molecular biology. Protein classification, which is typically done through measuring the similarity be- tween proteins based on protein sequence or physical information, serves as a crucial step toward the understanding of protein function and dynamics. Persistent homology is a new branch of algebraic topology that has found its success in the topological data analysis in a variety of disciplines, including molecular biology. The present work explores the potential of using persistent homology as an indepen- dent tool for protein classification. To this end, we propose a molecular topological fingerprint based support vector machine (MTF-SVM) classifier. Specifically, we construct machine learning feature vectors solely from protein topological fingerprints, which are topological invariants generated during the filtration process. To validate the present MTF-SVM approach, we consider four types of problems. First, we study protein-drug binding by using the M2 channel protein of influenza A virus. We achieve 96\% accuracy in discriminating drug bound and unbound M2 channels. Additionally, we examine the use of MTF-SVM for the classification of hemoglobin molecules in their relaxed and taut forms and obtain about 80\% accuracy. The identification of all alpha, all beta, and alpha-beta protein domains is carried out in our next study using 900 proteins. We have found a 85\% success in this identifica- tion. Finally, we apply the present technique to 55 classification tasks of protein superfamilies over 1357 samples. An average accuracy of 82\% is attained. The present study establishes computational topology as an independent and effective alternative for protein classification.},
  language = {en},
  journal = {arXiv:1510.00953 [q-bio]},
  author = {Cang, Zixuan and Mu, Lin and Wu, Kedi and Opron, Kristopher and Xia, Kelin and Wei, Guo-Wei},
  month = oct,
  year = {2015},
  keywords = {Quantitative Biology - Biomolecules},
  file = {/Users/lancelotdacosta/Zotero/storage/YHX6C7DV/Cang et al. - 2015 - A topological approach for protein classification.pdf}
}

@article{bogaczTutorialFreeenergyFramework2017,
  title = {A Tutorial on the Free-Energy Framework for Modelling Perception and Learning},
  volume = {76},
  issn = {00222496},
  abstract = {This paper provides an easy to follow tutorial on the free-energy framework for modelling perception developed by Friston, which extends the predictive coding model of Rao and Ballard. These models assume that the sensory cortex infers the most likely values of attributes or features of sensory stimuli from the noisy inputs encoding the stimuli. Remarkably, these models describe how this inference could be implemented in a network of very simple computational elements, suggesting that this inference could be performed by biological networks of neurons. Furthermore, learning about the parameters describing the features and their uncertainty is implemented in these models by simple rules of synaptic plasticity based on Hebbian learning. This tutorial introduces the free-energy framework using very simple examples, and provides step-by-step derivations of the model. It also discusses in more detail how the model could be implemented in biological neural circuits. In particular, it presents an extended version of the model in which the neurons only sum their inputs, and synaptic plasticity only depends on activity of pre-synaptic and post-synaptic neurons.},
  language = {en},
  journal = {Journal of Mathematical Psychology},
  doi = {10.1016/j.jmp.2015.11.003},
  author = {Bogacz, Rafal},
  month = feb,
  year = {2017},
  pages = {198-211},
  file = {/Users/lancelotdacosta/Zotero/storage/XSX5X9JJ/Bogacz - 2017 - A tutorial on the free-energy framework for modell.pdf;/Users/lancelotdacosta/Zotero/storage/YEM78SHV/Bogacz - 2017 - A tutorial on the free-energy framework for modell.pdf}
}

@inproceedings{osborneActiveDataSelection2010,
  address = {{Perth, Australia}},
  title = {Active {{Data Selection}} for {{Sensor Networks}} with {{Faults}} and {{Changepoints}}},
  isbn = {978-1-4244-6695-5},
  abstract = {We describe a Bayesian formalism for the intelligent selection of observations from sensor networks that may intermittently undergo faults or changepoints. Such active data selection is performed with the goal of taking as few observations as necessary in order to maintain a reasonable level of uncertainty about the variables of interest. The presence of faults/changepoints is not always obvious and therefore our algorithm must first detect their occurrence. Having done so, our selection of observations must be appropriately altered. Faults corrupt our observations, reducing their impact; changepoints (abrupt changes in the characteristics of data) may require the transition to an entirely different sampling schedule. Our solution is to employ a Gaussian process formalism that allows for sequential time-series prediction about variables of interest along with a decision theoretic approach to the problem of selecting observations.},
  language = {en},
  booktitle = {2010 24th {{IEEE International Conference}} on {{Advanced Information Networking}} and {{Applications}}},
  publisher = {{IEEE}},
  doi = {10.1109/AINA.2010.36},
  author = {Osborne, Michael A. and Garnett, Roman and Roberts, Stephen J.},
  year = {2010},
  pages = {533-540},
  file = {/Users/lancelotdacosta/Zotero/storage/43PJ95LG/Osborne et al. - 2010 - Active Data Selection for Sensor Networks with Fau.pdf;/Users/lancelotdacosta/Zotero/storage/44K3B5ZW/Osborne et al. - 2010 - Active Data Selection for Sensor Networks with Fau.pdf}
}

@article{fristonActiveInferenceLearning2016,
  title = {Active Inference and Learning},
  volume = {68},
  issn = {01497634},
  abstract = {This paper offers an active inference account of choice behaviour and learning. It focuses on the distinction between goal-directed and habitual behaviour and how they contextualise each other. We show that habits emerge naturally (and autodidactically) from sequential policy optimisation when agents are equipped with state-action policies. In active inference, behaviour has explorative (epistemic) and exploitative (pragmatic) aspects that are sensitive to ambiguity and risk respectively, where epistemic (ambiguity-resolving) behaviour enables pragmatic (reward-seeking) behaviour and the subsequent emergence of habits. Although goal-directed and habitual policies are usually associated with model-based and model-free schemes, we find the more important distinction is between belief-free and belief-based schemes. The underlying (variational) belief updating provides a comprehensive (if metaphorical) process theory for several phenomena, including the transfer of dopamine responses, reversal learning, habit formation and devaluation. Finally, we show that active inference reduces to a classical (Bellman) scheme, in the absence of ambiguity.},
  language = {en},
  journal = {Neuroscience \& Biobehavioral Reviews},
  doi = {10.1016/j.neubiorev.2016.06.022},
  author = {Friston, Karl and FitzGerald, Thomas and Rigoli, Francesco and Schwartenbeck, Philipp and O'Doherty, John and Pezzulo, Giovanni},
  month = sep,
  year = {2016},
  pages = {862-879},
  file = {/Users/lancelotdacosta/Zotero/storage/AA9ZJEYP/Friston et al. - 2016 - Active inference and learning.pdf;/Users/lancelotdacosta/Zotero/storage/QMHQNGC6/Friston et al. - 2016 - Active inference and learning.pdf}
}

@article{cullenActiveInferenceOpenAI2018,
  title = {Active {{Inference}} in {{OpenAI Gym}}: {{A Paradigm}} for {{Computational Investigations Into Psychiatric Illness}}},
  volume = {3},
  issn = {24519022},
  shorttitle = {Active {{Inference}} in {{OpenAI Gym}}},
  abstract = {BACKGROUND: Artificial intelligence has recently attained humanlike performance in a number of gamelike domains. These advances have been spurred by brain-inspired architectures and algorithms such as hierarchical filtering and reinforcement learning. OpenAI Gym is an open-source platform in which to train, test, and benchmark algorithms\textemdash{}it provides a range of tasks, including those of classic arcade games such as Doom. Here we describe how the platform might be used as a simulation, test, and diagnostic paradigm for psychiatric conditions.
METHODS: To illustrate how active inference models of game play could be used to test mechanistic and algorithmic properties of psychiatric disorders, we provide two exemplar analyses. The first speaks to the impact of aging on cognition, examining game-play behaviors in a model of aging in which we compared age-dependent changes of younger (n = 9, 22 6 1 years of age) and older (n = 7, 56 6 5 years of age) adult players. The second is an illustration of a putative feature of anhedonia in which we simulated diminished sensitivity to reward.
RESULTS: These simulations demonstrate how active inference can be used to test predicted changes in both neurobiology and beliefs in psychiatric cohorts. We show that, as well as behavioral measures, putative neural correlates of active inference can be simulated, and hypothesized (model-based) differences in local field potentials and blood oxygen level\textendash{}dependent responses can be produced.
CONCLUSIONS: We show that active inference, through epistemic and value-based goals, enables simulated subjects to actively develop detailed representations of gaming environments, and we demonstrate the use of a principled algorithmic and neurobiological framework for testing hypotheses in psychiatric illness.},
  language = {en},
  number = {9},
  journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
  doi = {10.1016/j.bpsc.2018.06.010},
  author = {Cullen, Maell and Davey, Ben and Friston, Karl J. and Moran, Rosalyn J.},
  month = sep,
  year = {2018},
  pages = {809-818},
  file = {/Users/lancelotdacosta/Zotero/storage/WRDRW3NQ/Cullen et al. - 2018 - Active Inference in OpenAI Gym A Paradigm for Com.pdf}
}

@article{fristonActiveInferenceProcess2017,
  title = {Active {{Inference}}: {{A Process Theory}}},
  volume = {29},
  issn = {0899-7667, 1530-888X},
  shorttitle = {Active {{Inference}}},
  language = {en},
  number = {1},
  journal = {Neural Computation},
  doi = {10.1162/NECO_a_00912},
  author = {Friston, Karl and FitzGerald, Thomas and Rigoli, Francesco and Schwartenbeck, Philipp and Pezzulo, Giovanni},
  month = jan,
  year = {2017},
  pages = {1-49},
  file = {/Users/lancelotdacosta/Zotero/storage/PQM2HY6N/Friston et al. - 2017 - Active Inference A Process Theory.pdf}
}

@article{fristonActiveInferenceCuriosity2017,
  title = {Active {{Inference}}, {{Curiosity}} and {{Insight}}},
  volume = {29},
  issn = {0899-7667, 1530-888X},
  language = {en},
  number = {10},
  journal = {Neural Computation},
  doi = {10.1162/neco_a_00999},
  author = {Friston, Karl J. and Lin, Marco and Frith, Christopher D. and Pezzulo, Giovanni and Hobson, J. Allan and Ondobaka, Sasha},
  month = oct,
  year = {2017},
  pages = {2633-2683},
  file = {/Users/lancelotdacosta/Zotero/storage/QJ5X5FKK/Friston et al. - 2017 - Active Inference, Curiosity and Insight.pdf}
}

@article{kingmaAdamMethodStochastic2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6980},
  primaryClass = {cs},
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  language = {en},
  journal = {arXiv:1412.6980 [cs]},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  month = dec,
  year = {2014},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/lancelotdacosta/Zotero/storage/GMH7Q6UC/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf}
}

@article{bediAddictionBrainDisease2017,
  title = {Addiction as a Brain Disease Does Not Promote Injustice},
  volume = {1},
  issn = {2397-3374},
  language = {en},
  number = {9},
  journal = {Nature Human Behaviour},
  doi = {10.1038/s41562-017-0203-5},
  author = {Bedi, Gillinder and Martinez, Diana and Levin, Frances R. and Comer, Sandra and Haney, Margaret},
  month = sep,
  year = {2017},
  pages = {610-610},
  file = {/Users/lancelotdacosta/Zotero/storage/NHD32AQR/Bedi et al. - 2017 - Addiction as a brain disease does not promote inju.pdf}
}

@article{tacchinoArtificialNeuronImplemented2019,
  title = {An Artificial Neuron Implemented on an Actual Quantum Processor},
  volume = {5},
  issn = {2056-6387},
  language = {en},
  number = {1},
  journal = {npj Quantum Information},
  doi = {10.1038/s41534-019-0140-4},
  author = {Tacchino, Francesco and Macchiavello, Chiara and Gerace, Dario and Bajoni, Daniele},
  month = dec,
  year = {2019},
  pages = {26},
  file = {/Users/lancelotdacosta/Zotero/storage/7FYQDGFU/Tacchino et al. - 2019 - An artificial neuron implemented on an actual quan.pdf;/Users/lancelotdacosta/Zotero/storage/L96Y2KV2/Tacchino et al. - 2019 - An artificial neuron implemented on an actual quan.pdf}
}

@incollection{yeangInformationGeometricPerspective2002,
  address = {{Berlin, Heidelberg}},
  title = {An {{Information Geometric Perspective}} on {{Active Learning}}},
  volume = {2430},
  isbn = {978-3-540-44036-9 978-3-540-36755-0},
  abstract = {The Fisher information matrix plays a very important role in both active learning and information geometry. In a special case of active learning (nonlinear regression with Gaussian noise), the inverse of the Fisher information matrix \textendash{} the dispersion matrix of parameters \textendash{}induces a variety of criteria for optimal experiment design. In information geometry, the Fisher information matrix defines the metric tensor on model manifolds. In this paper, I explore the intrinsic relations of these two fields. The conditional distributions which belong to exponential families are known to be dually flat. Moreover, the author proves for a certain type of conditional models, the embedding curvature in terms of true parameters also vanishes. The expected Riemannian distance between current parameters and the next update is proposed to be the loss function for active learning. Examples of nonlinear and logistic regressions are given in order to elucidate this active learning scheme.},
  language = {en},
  booktitle = {Machine {{Learning}}: {{ECML}} 2002},
  publisher = {{Springer Berlin Heidelberg}},
  doi = {10.1007/3-540-36755-1_40},
  author = {Yeang, Chen-Hsiang},
  editor = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Elomaa, Tapio and Mannila, Heikki and Toivonen, Hannu},
  year = {2002},
  pages = {480-492},
  file = {/Users/lancelotdacosta/Zotero/storage/S8LK5RJX/Yeang - 2002 - An Information Geometric Perspective on Active Lea.pdf}
}

@article{brettIntroductionRandomField,
  title = {An {{Introduction}} to {{Random Field Theory}}},
  language = {en},
  author = {Brett, Matthew and Penny, Will and Kiebel, Stefan},
  pages = {23},
  file = {/Users/lancelotdacosta/Zotero/storage/RMRCAVEC/Brett et al. - An Introduction to Random Field Theory.pdf}
}

@incollection{jordanIntroductionVariationalMethods1998,
  address = {{Dordrecht}},
  title = {An {{Introduction}} to {{Variational Methods}} for {{Graphical Models}}},
  isbn = {978-94-010-6104-9 978-94-011-5014-9},
  abstract = {This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.},
  language = {en},
  booktitle = {Learning in {{Graphical Models}}},
  publisher = {{Springer Netherlands}},
  doi = {10.1007/978-94-011-5014-9_5},
  author = {Jordan, Michael I. and Ghahramani, Zoubin and Jaakkola, Tommi S. and Saul, Lawrence K.},
  editor = {Jordan, Michael I.},
  year = {1998},
  pages = {105-161},
  file = {/Users/lancelotdacosta/Zotero/storage/2PYRBY8G/Jordan et al. - 1998 - An Introduction to Variational Methods for Graphic.pdf;/Users/lancelotdacosta/Zotero/storage/D2M4469F/Jordan et al. - 1998 - An Introduction to Variational Methods for Graphic.pdf}
}

@inproceedings{senguptaApproximateBayesianInference2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.06614},
  title = {Approximate {{Bayesian}} Inference as a Gauge Theory},
  volume = {14},
  language = {en},
  booktitle = {Computational {{Biology Workshop}}},
  doi = {10.1371/journal.pbio.1002400},
  author = {Sengupta, Biswa and Friston, Karl},
  month = mar,
  year = {2016},
  keywords = {Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  pages = {e1002400},
  file = {/Users/lancelotdacosta/Zotero/storage/38PXHXEW/Sengupta and Friston - 2016 - Approximate Bayesian inference as a gauge theory.pdf;/Users/lancelotdacosta/Zotero/storage/J6HPTNM2/Sengupta and Friston - 2016 - Approximate Bayesian inference as a gauge theory.pdf}
}

@article{cybenkoApproximationSuperpositionsSigmoidal1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  volume = {2},
  issn = {0932-4194, 1435-568X},
  language = {en},
  number = {4},
  journal = {Mathematics of Control, Signals, and Systems},
  doi = {10.1007/BF02551274},
  author = {Cybenko, G.},
  month = dec,
  year = {1989},
  pages = {303-314},
  file = {/Users/lancelotdacosta/Zotero/storage/5F497Q3G/Cybenko - 1989 - Approximation by superpositions of a sigmoidal fun.pdf}
}

@article{calandraBayesianOptimizationLearning2016,
  title = {Bayesian Optimization for Learning Gaits under Uncertainty: {{An}} Experimental Comparison on a Dynamic Bipedal Walker},
  volume = {76},
  issn = {1012-2443, 1573-7470},
  shorttitle = {Bayesian Optimization for Learning Gaits under Uncertainty},
  abstract = {Designing gaits and corresponding control policies is a key challenge in robot locomotion. Even with a viable controller parametrization, finding near-optimal parameters can be daunting. Typically, this kind of parameter optimization requires specific expert knowledge and extensive robot experiments. Automatic black-box gait optimization methods greatly reduce the need for human expertise and time-consuming design processes. Many different approaches for automatic gait optimization have been suggested to date. However, no extensive comparison among them has yet been performed. In this article, we thoroughly discuss multiple automatic optimization methods in the context of gait optimization. We extensively evaluate Bayesian optimization, a model-based approach to black-box optimization under uncertainty, on both simulated problems and real robots. This evaluation demonstrates that Bayesian optimization is particularly suited for robotic applications, where it is crucial to find a good set of gait parameters in a small number of experiments.},
  language = {en},
  number = {1-2},
  journal = {Annals of Mathematics and Artificial Intelligence},
  doi = {10.1007/s10472-015-9463-9},
  author = {Calandra, Roberto and Seyfarth, Andr{\'e} and Peters, Jan and Deisenroth, Marc Peter},
  month = feb,
  year = {2016},
  pages = {5-23},
  file = {/Users/lancelotdacosta/Zotero/storage/CRL3QKEM/Calandra et al. - 2016 - Bayesian optimization for learning gaits under unc.pdf}
}

@book{mockusBayesianApproachGlobal1989,
  address = {{Dordrecht}},
  title = {Bayesian {{Approach}} to {{Global Optimization}}: {{Theory}} and {{Applications}}},
  isbn = {978-94-009-0909-0},
  shorttitle = {Bayesian {{Approach}} to {{Global Optimization}}},
  language = {en},
  publisher = {{Springer Netherlands}},
  author = {Mockus, Jonas},
  year = {1989},
  file = {/Users/lancelotdacosta/Zotero/storage/J9Y4CQZI/Mockus - 1989 - Bayesian Approach to Global Optimization Theory a.pdf;/Users/lancelotdacosta/Zotero/storage/L6SRLWVG/Mockus - 1989 - Bayesian Approach to Global Optimization Theory a.pdf},
  note = {OCLC: 879623613}
}

@article{volkowBrainDiseaseModel2015,
  title = {Brain Disease Model of Addiction: Why Is It so Controversial?},
  volume = {2},
  issn = {22150366},
  shorttitle = {Brain Disease Model of Addiction},
  language = {en},
  number = {8},
  journal = {The Lancet Psychiatry},
  doi = {10.1016/S2215-0366(15)00236-9},
  author = {Volkow, Nora D and Koob, George},
  month = aug,
  year = {2015},
  pages = {677-679},
  file = {/Users/lancelotdacosta/Zotero/storage/5TRTZQLA/Volkow and Koob - 2015 - Brain disease model of addiction why is it so con.pdf;/Users/lancelotdacosta/Zotero/storage/C3SD7Q2G/Volkow and Koob - 2015 - Brain disease model of addiction why is it so con.pdf}
}

@article{louieCategoricalSystemTheory1983,
  title = {Categorical System Theory and the Phenomenological Calculus},
  language = {en},
  journal = {Bulletin of Mathematical Biology},
  author = {Louie, A H},
  year = {1983},
  pages = {17},
  file = {/Users/lancelotdacosta/Zotero/storage/GLQ358IV/Louie - CATEGORICAL SYSTEM THEORY AND THE PHENOMENOLOGICAL.pdf;/Users/lancelotdacosta/Zotero/storage/Y3XM9EQ8/Louie - CATEGORICAL SYSTEM THEORY AND THE PHENOMENOLOGICAL.pdf}
}

@article{lambeCategoriesMathematicsSystems,
  title = {Categories, {{Mathematics}}, and {{Systems}}},
  language = {en},
  author = {Lambe, Larry A and Scientist, Chief},
  pages = {58},
  file = {/Users/lancelotdacosta/Zotero/storage/EGQ2ZF3B/Lambe and Scientist - Categories, Mathematics, and Systems.pdf;/Users/lancelotdacosta/Zotero/storage/FNQ7LL7V/Lambe and Scientist - Categories, Mathematics, and Systems.pdf}
}

@article{brownCategoryTheoryHigher,
  title = {Category {{Theory}} and {{Higher Dimensional Algebra}}: Potential Descriptive Tools in Neuroscience},
  abstract = {We explain the notion of colimit in category theory as a potential tool for describing structures and their communication, and the notion of higher dimensional algebra as potential yoga for dealing with processes and processes of processes.},
  language = {en},
  author = {Brown, Ronald and Porter, Timothy},
  pages = {13},
  file = {/Users/lancelotdacosta/Zotero/storage/BKUSJ3EX/Brown and Porter - Category Theory and Higher Dimensional Algebra po.pdf}
}

@article{vreeswijkChaosNeuronalNetworks1996,
  title = {Chaos in {{Neuronal Networks}} with {{Balanced Excitatory}} and {{Inhibitory Activity}}},
  volume = {274},
  issn = {0036-8075, 1095-9203},
  language = {en},
  number = {5293},
  journal = {Science},
  doi = {10.1126/science.274.5293.1724},
  author = {v. Vreeswijk, C. and Sompolinsky, H.},
  month = dec,
  year = {1996},
  pages = {1724-1726},
  file = {/Users/lancelotdacosta/Zotero/storage/22EKG7AJ/Vreeswijk and Sompolinsky - 1996 - Chaos in Neuronal Networks with Balanced Excitator.pdf;/Users/lancelotdacosta/Zotero/storage/Y9UR6N6H/Vreeswijk and Sompolinsky - 1996 - Chaos in Neuronal Networks with Balanced Excitator.pdf}
}

@article{fristonCognitiveDynamicsAttractors2014,
  title = {Cognitive {{Dynamics}}: {{From Attractors}} to {{Active Inference}}},
  volume = {102},
  issn = {0018-9219, 1558-2256},
  shorttitle = {Cognitive {{Dynamics}}},
  language = {en},
  number = {4},
  journal = {Proceedings of the IEEE},
  doi = {10.1109/JPROC.2014.2306251},
  author = {Friston, Karl and Sengupta, Biswa and Auletta, Gennaro},
  month = apr,
  year = {2014},
  pages = {427-445},
  file = {/Users/lancelotdacosta/Zotero/storage/KPB9BBGQ/Friston et al. - 2014 - Cognitive Dynamics From Attractors to Active Infe.pdf;/Users/lancelotdacosta/Zotero/storage/RHUDR8B8/Friston et al. - 2014 - Cognitive Dynamics From Attractors to Active Infe.pdf}
}

@techreport{millidgeCombiningActiveInference2019,
  type = {Preprint},
  title = {Combining {{Active Inference}} and {{Hierarchical Predictive Coding}}: {{A Tutorial Introduction}} and {{Case Study}}},
  shorttitle = {Combining {{Active Inference}} and {{Hierarchical Predictive Coding}}},
  abstract = {This paper combines the active inference formulation of action (Friston et al., 2009) with hierarchical predictive coding models (Friston, 2003) to provide a proof-of-concept implementation of an active inference agent able to solve a common reinforcement learning baseline \textendash{} the cart-pole environment in OpenAI gym. It demonstrates empirically that predictive coding and active inference approaches can be successfully scaled up to tasks more challenging than the mountain car (Friston et al., 2009, 2012). We show that hierarchical predictive coding models can be learned from scratch during the task, and can successfully drive action selection via active inference. To our knowledge, it is the first implemented active inference agent to combine active inference with a hierarchical predictive coding perceptual model. We also provide a tutorial walk-through of the free-energy principle, hierarchical predictive coding, and active inference, including an in-depth derivation of our agent.},
  language = {en},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/kf6wc},
  author = {Millidge, Beren},
  month = mar,
  year = {2019},
  file = {/Users/lancelotdacosta/Zotero/storage/3MY63MW4/Millidge - 2019 - Combining Active Inference and Hierarchical Predic.pdf;/Users/lancelotdacosta/Zotero/storage/C32UFGLP/Millidge - 2019 - Combining Active Inference and Hierarchical Predic.pdf}
}

@article{linCriticalityFormalLanguages2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.06737},
  title = {Criticality in {{Formal Languages}} and {{Statistical Physics}}},
  volume = {19},
  issn = {1099-4300},
  abstract = {We show that the mutual information between two symbols, as a function of the number of symbols between the two, decays exponentially in any probabilistic regular grammar, but can decay like a power law for a context-free grammar. This result about formal languages is closely related to a well-known result in classical statistical mechanics that there are no phase transitions in dimensions fewer than two. It is also related to the emergence of power-law correlations in turbulence and cosmological inflation through recursive generative processes. We elucidate these physics connections and comment on potential applications of our results to machine learning tasks like training artificial recurrent neural networks. Along the way, we introduce a useful quantity which we dub the rational mutual information and discuss generalizations of our claims involving more complicated Bayesian networks.},
  language = {en},
  number = {7},
  journal = {Entropy},
  doi = {10.3390/e19070299},
  author = {Lin, Henry W. and Tegmark, Max},
  month = jun,
  year = {2017},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Computer Science - Computation and Language},
  pages = {299},
  file = {/Users/lancelotdacosta/Zotero/storage/3MKGMSMW/Lin and Tegmark - 2017 - Criticality in Formal Languages and Statistical Ph.pdf;/Users/lancelotdacosta/Zotero/storage/XMWJIWUA/Lin and Tegmark - 2017 - Criticality in Formal Languages and Statistical Ph.pdf}
}

@article{kordingDecisionTheoryWhat2007,
  title = {Decision {{Theory}}: {{What}} "{{Should}}" the {{Nervous System Do}}?},
  volume = {318},
  issn = {0036-8075, 1095-9203},
  shorttitle = {Decision {{Theory}}},
  abstract = {The purpose of our nervous system is to allow us to successfully interact with our environment. This normative idea is formalized by decision theory that defines which choices would be most beneficial. We live in an uncertain world, and each decision may have many possible outcomes; choosing the best decision is thus complicated. Bayesian decision theory formalizes these problems in the presence of uncertainty and often provides compact models that predict observed behavior. With its elegant formalization of the problems faced by the nervous system, it promises to become a major inspiration for studies in neuroscience.},
  language = {en},
  number = {5850},
  journal = {Science},
  doi = {10.1126/science.1142998},
  author = {K{\"o}rding, Konrad},
  month = oct,
  year = {2007},
  pages = {606-610},
  file = {/Users/lancelotdacosta/Zotero/storage/A88WIF23/Körding - 2007 - Decision Theory What Should the Nervous System .pdf;/Users/lancelotdacosta/Zotero/storage/D6U6IDRJ/Körding - 2007 - Decision Theory What Should the Nervous System .pdf}
}

@article{dayanDecisionTheoryReinforcement2008,
  title = {Decision Theory, Reinforcement Learning, and the Brain},
  volume = {8},
  issn = {1530-7026, 1531-135X},
  language = {en},
  number = {4},
  journal = {Cognitive, Affective, \& Behavioral Neuroscience},
  doi = {10.3758/CABN.8.4.429},
  author = {Dayan, P. and Daw, N. D.},
  month = dec,
  year = {2008},
  pages = {429-453},
  file = {/Users/lancelotdacosta/Zotero/storage/4UHCAXPJ/Dayan and Daw - 2008 - Decision theory, reinforcement learning, and the b.pdf;/Users/lancelotdacosta/Zotero/storage/G58LEAQ2/Dayan and Daw - 2008 - Decision theory, reinforcement learning, and the b.pdf}
}

@article{ueltzhofferDeepActiveInference2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.02341},
  title = {Deep {{Active Inference}}},
  volume = {112},
  issn = {0340-1200, 1432-0770},
  abstract = {This work combines the free energy principle from cognitive neuroscience and the ensuing active inference dynamics with recent advances in variational inference on deep generative models and evolution strategies as efficient large-scale black-box optimisation technique, to introduce the ''deep active inference'' agent. This agent tries to minimize a variational free energy bound on the average surprise of its sensations, which is motivated by a homeostatic argument. It does so by changing the parameters of its generative model, together with a variational density approximating the posterior distribution over latent variables, given its observations, and by acting on its environment to actively sample input that is likely under its generative model. The internal dynamics of the agent are implemented using deep neural networks, as used in machine learning, and recurrent dynamics, making the deep active inference agent a scalable and very flexible class of active inference agents. Using the mountaincar problem, we show how goal-directed behaviour can be implemented by defining sensible prior expectations on the latent states in the agent's model, that it will try to fulfil. Furthermore, we show that the deep active inference agent can learn a generative model of the environment, which can be sampled from to understand the agent's beliefs about the environment and its interaction with it.},
  language = {en},
  number = {6},
  journal = {Biological Cybernetics},
  doi = {10.1007/s00422-018-0785-7},
  author = {Ueltzh{\"o}ffer, Kai},
  month = dec,
  year = {2018},
  keywords = {Quantitative Biology - Neurons and Cognition},
  pages = {547-573},
  file = {/Users/lancelotdacosta/Zotero/storage/5B9KSGDQ/Ueltzhöffer - 2018 - Deep Active Inference.pdf;/Users/lancelotdacosta/Zotero/storage/GVI2S2CI/Ueltzhöffer - 2018 - Deep Active Inference.pdf}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  volume = {521},
  issn = {0028-0836, 1476-4687},
  language = {en},
  number = {7553},
  journal = {Nature},
  doi = {10.1038/nature14539},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  month = may,
  year = {2015},
  pages = {436-444},
  file = {/Users/lancelotdacosta/Zotero/storage/BHNEFCYC/LeCun et al. - 2015 - Deep learning.pdf;/Users/lancelotdacosta/Zotero/storage/HDJDS6HY/LeCun et al. - 2015 - Deep learning.pdf}
}

@article{fristonDeepTemporalModels2018,
  title = {Deep Temporal Models and Active Inference},
  volume = {90},
  issn = {01497634},
  abstract = {How do we navigate a deeply structured world? Why are you reading this sentence first \textendash{} and did you actually look at the fifth word? This review offers some answers by appealing to active inference based on deep temporal models. It builds on previous formulations of active inference to simulate behavioural and electrophysiological responses under hierarchical generative models of state transitions. Inverting these models corresponds to sequential inference, such that the state at any hierarchical level entails a sequence of transitions in the level below. The deep temporal aspect of these models means that evidence is accumulated over nested time scales, enabling inferences about narratives (i.e., temporal scenes). We illustrate this behaviour with Bayesian belief updating \textendash{} and neuronal process theories \textendash{} to simulate the epistemic foraging seen in reading. These simulations reproduce perisaccadic delay period activity and local field potentials seen empirically. Finally, we exploit the deep structure of these models to simulate responses to local (e.g., font type) and global (e.g., semantic) violations; reproducing mismatch negativity and P300 responses respectively.},
  language = {en},
  journal = {Neuroscience \& Biobehavioral Reviews},
  doi = {10.1016/j.neubiorev.2018.04.004},
  author = {Friston, Karl J. and Rosch, Richard and Parr, Thomas and Price, Cathy and Bowman, Howard},
  month = jul,
  year = {2018},
  pages = {486-501},
  file = {/Users/lancelotdacosta/Zotero/storage/TF7NDIDZ/Friston et al. - 2018 - Deep temporal models and active inference.pdf;/Users/lancelotdacosta/Zotero/storage/TQQ3I944/Friston et al. - 2018 - Deep temporal models and active inference.pdf}
}

@article{haspelDetectingIntermediateProtein2017,
  title = {Detecting Intermediate Protein Conformations Using Algebraic Topology},
  volume = {18},
  issn = {1471-2105},
  abstract = {Background: Understanding protein structure and dynamics is essential for understanding their function. This is a challenging task due to the high complexity of the conformational landscapes of proteins and their rugged energy levels. In particular, it is important to detect highly populated regions which could correspond to intermediate structures or local minima.
Results: We present a hierarchical clustering and algebraic topology based method that detects regions of interest in protein conformational space. The method is based on several techniques. We use coarse grained protein conformational search, efficient robust dimensionality reduction and topological analysis via persistent homology as the main tools. We use two dimensionality reduction methods as well, robust Principal Component Analysis (PCA) and Isomap, to generate a reduced representation of the data while preserving most of the variance in the data.
Conclusions: Our hierarchical clustering method was able to produce compact, well separated clusters for all the tested examples.},
  language = {en},
  number = {S15},
  journal = {BMC Bioinformatics},
  doi = {10.1186/s12859-017-1918-z},
  author = {Haspel, Nurit and Luo, Dong and Gonz{\'a}lez, Eduardo},
  month = dec,
  year = {2017},
  pages = {502},
  file = {/Users/lancelotdacosta/Zotero/storage/6PST4PEQ/Haspel et al. - 2017 - Detecting intermediate protein conformations using.pdf;/Users/lancelotdacosta/Zotero/storage/PP7R7F9V/Haspel et al. - 2017 - Detecting intermediate protein conformations using.pdf}
}

@article{fristonDoesPredictiveCoding2018,
  title = {Does Predictive Coding Have a Future?},
  volume = {21},
  issn = {1097-6256, 1546-1726},
  language = {en},
  number = {8},
  journal = {Nature Neuroscience},
  doi = {10.1038/s41593-018-0200-7},
  author = {Friston, Karl},
  month = aug,
  year = {2018},
  pages = {1019-1021},
  file = {/Users/lancelotdacosta/Zotero/storage/6QS52HV7/Friston - 2018 - Does predictive coding have a future.pdf;/Users/lancelotdacosta/Zotero/storage/KSDIXKV5/Friston - 2018 - Does predictive coding have a future.pdf}
}

@article{jansonDynamicalSystemPlastic2017,
  title = {Dynamical System with Plastic Self-Organized Velocity Field as an Alternative Conceptual Model of a Cognitive System},
  volume = {7},
  issn = {2045-2322},
  language = {en},
  number = {1},
  journal = {Scientific Reports},
  doi = {10.1038/s41598-017-16994-y},
  author = {Janson, Natalia B. and Marsden, Christopher J.},
  month = dec,
  year = {2017},
  pages = {17007},
  file = {/Users/lancelotdacosta/Zotero/storage/DU5TDFEJ/Janson and Marsden - 2017 - Dynamical system with plastic self-organized veloc.pdf;/Users/lancelotdacosta/Zotero/storage/PXM55W5K/Janson and Marsden - 2017 - Dynamical system with plastic self-organized veloc.pdf}
}

@article{jonesEfficientGlobalOptimization,
  title = {Efficient {{Global Optimization}} of {{Expensive Black}}-{{Box Functions}}},
  abstract = {In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome.},
  language = {en},
  author = {Jones, Donald R and Schonlau, Matthias},
  pages = {38},
  file = {/Users/lancelotdacosta/Zotero/storage/5H2J4UJE/Jones and Schonlau - Efficient Global Optimization of Expensive Black-B.pdf;/Users/lancelotdacosta/Zotero/storage/VJUYLGPK/Jones and Schonlau - Efficient Global Optimization of Expensive Black-B.pdf}
}

@article{zwaanEmbodimentLanguageComprehension2014,
  title = {Embodiment and Language Comprehension: Reframing the Discussion},
  volume = {18},
  issn = {13646613},
  shorttitle = {Embodiment and Language Comprehension},
  language = {en},
  number = {5},
  journal = {Trends in Cognitive Sciences},
  doi = {10.1016/j.tics.2014.02.008},
  author = {Zwaan, Rolf A.},
  month = may,
  year = {2014},
  pages = {229-234},
  file = {/Users/lancelotdacosta/Zotero/storage/L829QH8K/Zwaan - 2014 - Embodiment and language comprehension reframing t.pdf;/Users/lancelotdacosta/Zotero/storage/T9B8URPN/Zwaan - 2014 - Embodiment and language comprehension reframing t.pdf}
}

@article{shivamEntropyGeometryQuantum2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.07279},
  title = {Entropy and {{Geometry}} of {{Quantum States}}},
  volume = {16},
  issn = {0219-7499, 1793-6918},
  abstract = {We compare the roles of the Bures-Helstrom (BH) and Bogoliubov-Kubo-Mori (BKM) metrics in the subject of quantum information geometry. We note that there are two limits involved in state discrimination, which we call the "thermodynamic" limit (of \$N\$, the number of realizations going to infinity) and the infinitesimal limit (of the separation of states tending to zero). We show that these two limits do not commute in the quantum case. Taking the infinitesimal limit first leads to the BH metric and the corresponding Cram\textbackslash{}'er-Rao bound, which is widely accepted in this subject. Taking limits in the opposite order leads to the BKM metric, which results in a weaker Cram\textbackslash{}'er-Rao bound. This lack of commutation of limits is a purely quantum phenomenon arising from quantum entanglement. We can exploit this phenomenon to gain a quantum advantage in state discrimination and get around the limitation imposed by the Bures-Helstrom Cram\textbackslash{}'er-Rao (BHCR) bound. We propose a technologically feasible experiment with cold atoms to demonstrate the quantum advantage in the simple case of two qubits.},
  language = {en},
  number = {04},
  journal = {International Journal of Quantum Information},
  doi = {10.1142/S0219749918500326},
  author = {Shivam, Kumar and Reddy, Anirudh and Samuel, Joseph and Sinha, Supurna},
  month = jun,
  year = {2018},
  keywords = {Quantum Physics},
  pages = {1850032},
  file = {/Users/lancelotdacosta/Zotero/storage/5S99JBWP/Shivam et al. - 2018 - Entropy and Geometry of Quantum States.pdf;/Users/lancelotdacosta/Zotero/storage/QHGQ83LI/Shivam et al. - 2018 - Entropy and Geometry of Quantum States.pdf}
}

@inproceedings{vikharEvolutionaryAlgorithmsCritical2016,
  address = {{Jalgaon, India}},
  title = {Evolutionary Algorithms: {{A}} Critical Review and Its Future Prospects},
  isbn = {978-1-5090-0467-6},
  shorttitle = {Evolutionary Algorithms},
  abstract = {Evolutionary algorithm (EA) emerges as an important optimization and search technique in the last decade. EA is a subset of Evolutionary Computations (EC) and belongs to set of modern heuristics based search method. Due to flexible nature and robust behavior inherited from Evolutionary Computation, it becomes efficient means of problem solving method for widely used global optimization problems. It can be used successfully in many applications of high complexity.},
  language = {en},
  booktitle = {2016 {{International Conference}} on {{Global Trends}} in {{Signal Processing}}, {{Information Computing}} and {{Communication}} ({{ICGTSPICC}})},
  publisher = {{IEEE}},
  doi = {10.1109/ICGTSPICC.2016.7955308},
  author = {Vikhar, Pradnya A.},
  month = dec,
  year = {2016},
  pages = {261-265},
  file = {/Users/lancelotdacosta/Zotero/storage/5IY5SJDF/Vikhar - 2016 - Evolutionary algorithms A critical review and its.pdf;/Users/lancelotdacosta/Zotero/storage/82X2PC62/Vikhar - 2016 - Evolutionary algorithms A critical review and its.pdf}
}

@article{khanFastSimpleNaturalGradient2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.04489},
  primaryClass = {cs, math, stat},
  title = {Fast yet {{Simple Natural}}-{{Gradient Descent}} for {{Variational Inference}} in {{Complex Models}}},
  abstract = {Bayesian inference plays an important role in advancing machine learning, but faces computational challenges when applied to complex models such as deep neural networks. Variational inference circumvents these challenges by formulating Bayesian inference as an optimization problem and solving it using gradient-based optimization. In this paper, we argue in favor of natural-gradient approaches which, unlike their gradientbased counterparts, can improve convergence by exploiting the information geometry of the solutions. We show how to derive fast yet simple natural-gradient updates by using a duality associated with exponential-family distributions. An attractive feature of these methods is that, by using natural-gradients, they are able to extract accurate local approximations for individual model components. We summarize recent results for Bayesian deep learning showing the superiority of natural-gradient approaches over their gradient counterparts.},
  language = {en},
  journal = {arXiv:1807.04489 [cs, math, stat]},
  author = {Khan, Mohammad Emtiyaz and Nielsen, Didrik},
  month = jul,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Computation,Computer Science - Information Theory},
  file = {/Users/lancelotdacosta/Zotero/storage/55QNV57X/Khan and Nielsen - 2018 - Fast yet Simple Natural-Gradient Descent for Varia.pdf;/Users/lancelotdacosta/Zotero/storage/CTBDEWEN/Khan and Nielsen - 2018 - Fast yet Simple Natural-Gradient Descent for Varia.pdf}
}

@article{paulFisherInformationMatrix2005,
  title = {Fisher {{Information Matrix}} of the {{Dirichlet}}-Multinomial {{Distribution}}},
  volume = {47},
  issn = {0323-3847, 1521-4036},
  abstract = {In this paper we derive explicit expressions for the elements of the exact Fisher information matrix of the Dirichlet-multinomial distribution. We show that exact calculation is based on the beta-binomial probability function rather than that of the Dirichlet-multinomial and this makes the exact calculation quite easy. The exact results are expected to be useful for the calculation of standard errors of the maximum likelihood estimates of the beta-binomial parameters and those of the Dirichlet-multinomial parameters for data that arise in practice in toxicology and other similar fields. Standard errors of the maximum likelihood estimates of the beta-binomial parameters and those of the Dirichlet-multinomial parameters, based on the exact and the asymptotic Fisher information matrix based on the Dirichlet distribution, are obtained for a set of data from Haseman and Soares (1976), a dataset from Mosimann (1962) and a more recent dataset from Chen, Kodell, Howe and Gaylor (1991). There is substantial difference between the standard errors of the estimates based on the exact Fisher information matrix and those based on the asymptotic Fisher information matrix.},
  language = {en},
  number = {2},
  journal = {Biometrical Journal},
  doi = {10.1002/bimj.200410103},
  author = {Paul, Sudhir R. and Balasooriya, Uditha and Banerjee, Tathagata},
  month = apr,
  year = {2005},
  pages = {230-236},
  file = {/Users/lancelotdacosta/Zotero/storage/K7YS9U3I/Paul et al. - 2005 - Fisher Information Matrix of the Dirichlet-multino.pdf;/Users/lancelotdacosta/Zotero/storage/NH375Z8Y/Paul et al. - 2005 - Fisher Information Matrix of the Dirichlet-multino.pdf}
}

@article{fristonGeneralisedFiltering2010,
  title = {Generalised {{Filtering}}},
  volume = {2010},
  issn = {1024-123X, 1563-5147},
  abstract = {We describe a Bayesian filtering scheme for nonlinear state-space models in continuous time. This scheme is called Generalised Filtering and furnishes posterior (conditional) densities on hidden states and unknown parameters generating observed data. Crucially, the scheme operates online, assimilating data to optimize the conditional density on time-varying states and time-invariant parameters. In contrast to Kalman and Particle smoothing, Generalised Filtering does not require a backwards pass. In contrast to variational schemes, it does not assume conditional independence between the states and parameters. Generalised Filtering optimises the conditional density with respect to a free-energy bound on the model's log-evidence. This optimisation uses the generalised motion of hidden states and parameters, under the prior assumption that the motion of the parameters is small. We describe the scheme, present comparative evaluations with a fixed-form variational version, and conclude with an illustrative application to a nonlinear state-space model of brain imaging time-series.},
  language = {en},
  journal = {Mathematical Problems in Engineering},
  doi = {10.1155/2010/621670},
  author = {Friston, Karl and Stephan, Klaas and Li, Baojuan and Daunizeau, Jean},
  year = {2010},
  pages = {1-34},
  file = {/Users/lancelotdacosta/Zotero/storage/EJM4VCVV/Friston et al. - 2010 - Generalised Filtering.pdf;/Users/lancelotdacosta/Zotero/storage/SS8Q7LV8/Friston et al. - 2010 - Generalised Filtering.pdf}
}

@article{ehresmannHierarchicalEvolutiveSystems,
  title = {Hierarchical Evolutive Systems: {{A}} Mathematical Model for Complex Systems},
  language = {en},
  author = {Ehresmann, A C and {de Picardie}},
  pages = {38},
  file = {/Users/lancelotdacosta/Zotero/storage/E4ZZKVFT/Ehresmann and de Picardie - Hierarchical evolutive systems A mathematical mod.pdf}
}

@article{stringerHighdimensionalGeometryPopulation2019,
  title = {High-Dimensional Geometry of Population Responses in Visual Cortex},
  volume = {571},
  issn = {0028-0836, 1476-4687},
  language = {en},
  number = {7765},
  journal = {Nature},
  doi = {10.1038/s41586-019-1346-5},
  author = {Stringer, Carsen and Pachitariu, Marius and Steinmetz, Nicholas and Carandini, Matteo and Harris, Kenneth D.},
  month = jul,
  year = {2019},
  pages = {361-365},
  file = {/Users/lancelotdacosta/Zotero/storage/MRSVR9IV/Stringer et al. - 2019 - High-dimensional geometry of population responses .pdf;/Users/lancelotdacosta/Zotero/storage/Z8UC7LZW/Stringer et al. - 2019 - High-dimensional geometry of population responses .pdf}
}

@article{senguptaHowRobustAre2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1804.11313},
  primaryClass = {cs, math, stat},
  title = {How {{Robust}} Are {{Deep Neural Networks}}?},
  abstract = {Convolutional and Recurrent, deep neural networks have been successful in machine learning systems for computer vision, reinforcement learning, and other allied fields. However, the robustness of such neural networks is seldom apprised, especially after high classification accuracy has been attained. In this paper, we evaluate the robustness of three recurrent neural networks to tiny perturbations, on three widely used datasets, to argue that high accuracy does not always mean a stable and a robust (to bounded perturbations, adversarial attacks, etc.) system. Especially, normalizing the spectrum of the discrete recurrent network to bound the spectrum (using power method, Rayleigh quotient, etc.) on a unit disk produces stable, albeit highly nonrobust neural networks. Furthermore, using the -pseudo-spectrum, we show that training of recurrent networks, say using gradient-based methods, often result in non-normal matrices that may or may not be diagonalizable. Therefore, the open problem lies in constructing methods that optimize not only for accuracy but also for the stability and the robustness of the underlying neural network, a criterion that is distinct from the other.},
  language = {en},
  journal = {arXiv:1804.11313 [cs, math, stat]},
  author = {Sengupta, Biswa and Friston, Karl J.},
  month = apr,
  year = {2018},
  keywords = {Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Dynamical Systems},
  file = {/Users/lancelotdacosta/Zotero/storage/BFXIA7VU/Sengupta and Friston - 2018 - How Robust are Deep Neural Networks.pdf;/Users/lancelotdacosta/Zotero/storage/YPT3RW7S/Sengupta and Friston - 2018 - How Robust are Deep Neural Networks.pdf}
}

@article{alonHowChooseGood2009,
  title = {How {{To Choose}} a {{Good Scientific Problem}}},
  volume = {35},
  issn = {10972765},
  language = {en},
  number = {6},
  journal = {Molecular Cell},
  doi = {10.1016/j.molcel.2009.09.013},
  author = {Alon, Uri},
  month = sep,
  year = {2009},
  pages = {726-728},
  file = {/Users/lancelotdacosta/Zotero/storage/U6BLGLNE/Alon - 2009 - How To Choose a Good Scientific Problem.pdf}
}

@techreport{millidgeImplementingPredictiveProcessing2019,
  type = {Preprint},
  title = {Implementing {{Predictive Processing}} and {{Active Inference}}: {{Preliminary Steps}} and {{Results}}},
  shorttitle = {Implementing {{Predictive Processing}} and {{Active Inference}}},
  abstract = {Initial and preliminary implementations of predictive processing and active inference models are presented. These include the baseline hierarchical predictive coding models of (Friston, 2003, 2005), and dynamical predictive coding models using generalised coordinates (Friston, 2008a,b; Friston et al., 2010; Buckley et al., 2017). Additionally, we re-implement and experiment with the active inference thermostat presented in (Buckley et al., 2017) and also implement an active inference agent with a hierarchical predictive coding perceptual model on the more challenging cart-pole task from OpanAI gym. We discuss the initial performance, capabilities, and limitations of these models in their preliminary stages and consider how they might be further scaled up to tackle more challenging tasks.},
  language = {en},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/4hb58},
  author = {Millidge, Beren},
  month = mar,
  year = {2019},
  file = {/Users/lancelotdacosta/Zotero/storage/9S77XCII/Millidge - 2019 - Implementing Predictive Processing and Active Infe.pdf;/Users/lancelotdacosta/Zotero/storage/WB5HQMXM/Millidge - 2019 - Implementing Predictive Processing and Active Infe.pdf}
}

@article{mirzaImpulsivityActiveInference2019,
  title = {Impulsivity and {{Active Inference}}},
  volume = {31},
  issn = {0898-929X, 1530-8898},
  language = {en},
  number = {2},
  journal = {Journal of Cognitive Neuroscience},
  doi = {10.1162/jocn_a_01352},
  author = {Mirza, M. Berk and Adams, Rick A. and Parr, Thomas and Friston, Karl},
  month = feb,
  year = {2019},
  pages = {202-220},
  file = {/Users/lancelotdacosta/Zotero/storage/M396LDV8/Mirza et al. - 2019 - Impulsivity and Active Inference.pdf;/Users/lancelotdacosta/Zotero/storage/WE3LAQRS/Mirza et al. - 2019 - Impulsivity and Active Inference.pdf}
}

@article{mackayInformationBasedObjectiveFunctions1992,
  title = {Information-{{Based Objective Functions}} for {{Active Data Selection}}},
  volume = {4},
  issn = {0899-7667, 1530-888X},
  language = {en},
  number = {4},
  journal = {Neural Computation},
  doi = {10.1162/neco.1992.4.4.590},
  author = {MacKay, David J. C.},
  month = jul,
  year = {1992},
  pages = {590-604},
  file = {/Users/lancelotdacosta/Zotero/storage/N6JAVUEU/MacKay - 1992 - Information-Based Objective Functions for Active D.pdf;/Users/lancelotdacosta/Zotero/storage/UZXPZ8NB/MacKay - 1992 - Information-Based Objective Functions for Active D.pdf}
}

@article{fristonLearningInferenceBrain2003,
  title = {Learning and Inference in the Brain},
  volume = {16},
  issn = {08936080},
  abstract = {This article is about how the brain data mines its sensory inputs. There are several architectural principles of functional brain anatomy that have emerged from careful anatomic and physiologic studies over the past century. These principles are considered in the light of representational learning to see if they could have been predicted a priori on the basis of purely theoretical considerations. We first review the organisation of hierarchical sensory cortices, paying special attention to the distinction between forward and backward connections. We then review various approaches to representational learning as special cases of generative models, starting with supervised learning and ending with learning based upon empirical Bayes. The latter predicts many features, such as a hierarchical cortical system, prevalent top-down backward influences and functional asymmetries between forward and backward connections that are seen in the real brain.},
  language = {en},
  number = {9},
  journal = {Neural Networks},
  doi = {10.1016/j.neunet.2003.06.005},
  author = {Friston, Karl},
  month = nov,
  year = {2003},
  pages = {1325-1352},
  file = {/Users/lancelotdacosta/Zotero/storage/AN78EU44/Friston - 2003 - Learning and inference in the brain.pdf;/Users/lancelotdacosta/Zotero/storage/NUHY7YCW/Friston - 2003 - Learning and inference in the brain.pdf}
}

@article{silverMasteringGameGo2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  volume = {529},
  issn = {0028-0836, 1476-4687},
  language = {en},
  number = {7587},
  journal = {Nature},
  doi = {10.1038/nature16961},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  month = jan,
  year = {2016},
  pages = {484-489},
  file = {/Users/lancelotdacosta/Zotero/storage/AHBLA6TZ/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf;/Users/lancelotdacosta/Zotero/storage/MEZ87IWJ/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf}
}

@article{parrNeuronalMessagePassing2019,
  title = {Neuronal Message Passing Using {{Mean}}-Field, {{Bethe}}, and {{Marginal}} Approximations},
  volume = {9},
  issn = {2045-2322},
  language = {en},
  number = {1},
  journal = {Scientific Reports},
  doi = {10.1038/s41598-018-38246-3},
  author = {Parr, Thomas and Markovic, Dimitrije and Kiebel, Stefan J. and Friston, Karl J.},
  month = dec,
  year = {2019},
  pages = {1889},
  file = {/Users/lancelotdacosta/Zotero/storage/KT3A39N5/Parr et al. - 2019 - Neuronal message passing using Mean-field, Bethe, .pdf;/Users/lancelotdacosta/Zotero/storage/UEL8REA8/Parr et al. - 2019 - Neuronal message passing using Mean-field, Bethe, .pdf}
}

@article{frazierNewOneStepBayesOptimal,
  title = {New {{One}}-{{Step Bayes}}-{{Optimal}}   {{Algorithms}} for {{Global Optimization}}:   {{Parallel Computing}} and {{Common Random Numbers}}},
  language = {en},
  author = {Frazier, Peter I},
  pages = {34},
  file = {/Users/lancelotdacosta/Zotero/storage/9W6JZBBF/Frazier - New One-Step Bayes-Optimal   Algorithms for Global.pdf;/Users/lancelotdacosta/Zotero/storage/GSJWRV84/Frazier - New One-Step Bayes-Optimal   Algorithms for Global.pdf}
}

@article{hoydalObjectvectorCodingMedial2019,
  title = {Object-Vector Coding in the Medial Entorhinal Cortex},
  volume = {568},
  issn = {0028-0836, 1476-4687},
  language = {en},
  number = {7752},
  journal = {Nature},
  doi = {10.1038/s41586-019-1077-7},
  author = {H{\o}ydal, {\O}yvind Arne and Skyt{\o}en, Emilie Ranheim and Andersson, Sebastian Ola and Moser, May-Britt and Moser, Edvard I.},
  month = apr,
  year = {2019},
  pages = {400-404},
  file = {/Users/lancelotdacosta/Zotero/storage/IKJV8QT9/Høydal et al. - 2019 - Object-vector coding in the medial entorhinal cort.pdf;/Users/lancelotdacosta/Zotero/storage/NDMYYCUL/Høydal et al. - 2019 - Object-vector coding in the medial entorhinal cort.pdf}
}

@article{mokusBAYESIANETHODSSEEKING,
  title = {{{ON BAYESIAN I}}\textasciitilde{{ETHODS FOR SEEKING THE EXTREMUM}}},
  language = {en},
  author = {Mokus, J},
  pages = {5},
  file = {/Users/lancelotdacosta/Zotero/storage/7ICVPSSH/Mokus - ON BAYESIAN I~ETHODS FOR SEEKING THE EXTREMUM.pdf;/Users/lancelotdacosta/Zotero/storage/MRYFVC8Z/Mokus - ON BAYESIAN I~ETHODS FOR SEEKING THE EXTREMUM.pdf}
}

@article{gussCharacterizingCapacityNeural2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.04443},
  primaryClass = {cs, math, stat},
  title = {On {{Characterizing}} the {{Capacity}} of {{Neural Networks}} Using {{Algebraic Topology}}},
  abstract = {The learnability of different neural architectures can be characterized directly by computable measures of data complexity. In this paper, we reframe the problem of architecture selection as understanding how data determines the most expressive and generalizable architectures suited to that data, beyond inductive bias. After suggesting algebraic topology as a measure for data complexity, we show that the power of a network to express the topological complexity of a dataset in its decision region is a strictly limiting factor in its ability to generalize. We then provide the first empirical characterization of the topological capacity of neural networks. Our empirical analysis shows that at every level of dataset complexity, neural networks exhibit topological phase transitions. This observation allowed us to connect existing theory to empirically driven conjectures on the choice of architectures for fully-connected neural networks.},
  language = {en},
  journal = {arXiv:1802.04443 [cs, math, stat]},
  author = {Guss, William H. and Salakhutdinov, Ruslan},
  month = feb,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Computational Geometry,Mathematics - Algebraic Topology},
  file = {/Users/lancelotdacosta/Zotero/storage/BEZVWECG/Guss and Salakhutdinov - 2018 - On Characterizing the Capacity of Neural Networks .pdf;/Users/lancelotdacosta/Zotero/storage/DSM4TN2S/Guss and Salakhutdinov - 2018 - On Characterizing the Capacity of Neural Networks .pdf}
}

@article{ekelandVariationalPrinciple1974,
  title = {On the Variational Principle},
  volume = {47},
  issn = {0022247X},
  language = {en},
  number = {2},
  journal = {Journal of Mathematical Analysis and Applications},
  doi = {10.1016/0022-247X(74)90025-0},
  author = {Ekeland, I.},
  month = aug,
  year = {1974},
  pages = {324-353},
  file = {/Users/lancelotdacosta/Zotero/storage/GHDR9A96/Ekeland - 1974 - On the variational principle.pdf}
}

@article{kappenOptimalControlGraphical2012,
  title = {Optimal Control as a Graphical Model Inference Problem},
  volume = {87},
  issn = {0885-6125, 1573-0565},
  abstract = {We reformulate a class of non-linear stochastic optimal control problems introduced by Todorov (in Advances in Neural Information Processing Systems, vol. 19, pp. 1369\textendash{}1376, 2007) as a Kullback-Leibler (KL) minimization problem. As a result, the optimal control computation reduces to an inference computation and approximate inference methods can be applied to efficiently compute approximate optimal controls. We show how this KL control theory contains the path integral control method as a special case. We provide an example of a block stacking task and a multi-agent cooperative game where we demonstrate how approximate inference can be successfully applied to instances that are too complex for exact computation. We discuss the relation of the KL control approach to other inference approaches to control.},
  language = {en},
  number = {2},
  journal = {Machine Learning},
  doi = {10.1007/s10994-012-5278-7},
  author = {Kappen, Hilbert J. and G{\'o}mez, Vicen{\c c} and Opper, Manfred},
  month = may,
  year = {2012},
  pages = {159-182},
  file = {/Users/lancelotdacosta/Zotero/storage/KGWKYMDW/Kappen et al. - 2012 - Optimal control as a graphical model inference pro.pdf;/Users/lancelotdacosta/Zotero/storage/L9RLUAMH/Kappen et al. - 2012 - Optimal control as a graphical model inference pro.pdf}
}

@article{liParticleRoboticsBased2019,
  title = {Particle Robotics Based on Statistical Mechanics of Loosely Coupled Components},
  volume = {567},
  issn = {0028-0836, 1476-4687},
  language = {en},
  number = {7748},
  journal = {Nature},
  doi = {10.1038/s41586-019-1022-9},
  author = {Li, Shuguang and Batra, Richa and Brown, David and Chang, Hyun-Dong and Ranganathan, Nikhil and Hoberman, Chuck and Rus, Daniela and Lipson, Hod},
  month = mar,
  year = {2019},
  pages = {361-365},
  file = {/Users/lancelotdacosta/Zotero/storage/C9T4WMDB/Li et al. - 2019 - Particle robotics based on statistical mechanics o.pdf;/Users/lancelotdacosta/Zotero/storage/SYXZCYZU/Li et al. - 2019 - Particle robotics based on statistical mechanics o.pdf}
}

@article{botvinickPlanningInference2012,
  title = {Planning as Inference},
  volume = {16},
  issn = {13646613},
  language = {en},
  number = {10},
  journal = {Trends in Cognitive Sciences},
  doi = {10.1016/j.tics.2012.08.006},
  author = {Botvinick, Matthew and Toussaint, Marc},
  month = oct,
  year = {2012},
  pages = {485-488},
  file = {/Users/lancelotdacosta/Zotero/storage/LE5JCKVR/Botvinick and Toussaint - 2012 - Planning as inference.pdf;/Users/lancelotdacosta/Zotero/storage/YM67UXCF/Botvinick and Toussaint - 2012 - Planning as inference.pdf}
}

@article{brochuPortfolioAllocationBayesian2010,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1009.5419},
  primaryClass = {cs},
  title = {Portfolio {{Allocation}} for {{Bayesian Optimization}}},
  abstract = {Bayesian optimization with Gaussian processes has become an increasingly popular tool in the machine learning community. It is efficient and can be used when very little is known about the objective function, making it popular in expensive black-box optimization scenarios. It uses Bayesian methods to sample the objective efficiently using an acquisition function which incorporates the model's estimate of the objective and the uncertainty at any given point. However, there are several different parameterized acquisition functions in the literature, and it is often unclear which one to use. Instead of using a single acquisition function, we adopt a portfolio of acquisition functions governed by an online multi-armed bandit strategy. We propose several portfolio strategies, the best of which we call GP-Hedge, and show that this method outperforms the best individual acquisition function. We also provide a theoretical bound on the algorithm's performance.},
  language = {en},
  journal = {arXiv:1009.5419 [cs]},
  author = {Brochu, Eric and Hoffman, Matthew W. and {de Freitas}, Nando},
  month = sep,
  year = {2010},
  keywords = {Computer Science - Machine Learning,G.1.6,G.3,I.2.6},
  file = {/Users/lancelotdacosta/Zotero/storage/WD7LP2UA/Brochu et al. - 2010 - Portfolio Allocation for Bayesian Optimization.pdf}
}

@article{snoekPracticalBayesianOptimization,
  title = {Practical {{Bayesian Optimization}} of {{Machine Learning Algorithms}}},
  abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a ``black art'' requiring expert experience, rules of thumb, or sometimes bruteforce search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
  language = {en},
  author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  pages = {9},
  file = {/Users/lancelotdacosta/Zotero/storage/BDG88ULD/Snoek et al. - Practical Bayesian Optimization of Machine Learnin.pdf}
}

@article{parrPrefrontalComputationActive2019,
  title = {Prefrontal {{Computation}} as {{Active Inference}}},
  issn = {1047-3211, 1460-2199},
  abstract = {The prefrontal cortex is vital for a range of cognitive processes, including working memory, attention, and decision-making. Notably, its absence impairs the performance of tasks requiring the maintenance of information through a delay period. In this paper, we formulate a rodent task\textemdash{}which requires maintenance of delay-period activity\textemdash{}as a Markov decision process and treat optimal task performance as an (active) inference problem. We simulate the behavior of a Bayes optimal mouse presented with 1 of 2 cues that instructs the selection of concurrent visual and auditory targets on a trial-by-trial basis. Formulating inference as message passing, we reproduce features of neuronal coupling within and between prefrontal regions engaged by this task. We focus on the micro-circuitry that underwrites delay-period activity and relate it to functional specialization within the prefrontal cortex in primates. Finally, we simulate the electrophysiological correlates of inference and demonstrate the consequences of lesions to each part of our in silico prefrontal cortex. In brief, this formulation suggests that recurrent excitatory connections\textemdash{}which support persistent neuronal activity\textemdash{}encode beliefs about transition probabilities over time. We argue that attentional modulation can be understood as the contextualization of sensory input by these persistent beliefs.},
  language = {en},
  journal = {Cerebral Cortex},
  doi = {10.1093/cercor/bhz118},
  author = {Parr, Thomas and Rikhye, Rajeev Vijay and Halassa, Michael M and Friston, Karl J},
  month = jul,
  year = {2019},
  pages = {bhz118 e0190429 e12112 e37454},
  file = {/Users/lancelotdacosta/Zotero/storage/2CFV8I3B/Parr et al. - 2019 - Prefrontal Computation as Active Inference.pdf;/Users/lancelotdacosta/Zotero/storage/UTNLSAH7/Parr et al. - 2019 - Prefrontal Computation as Active Inference.pdf}
}

@article{fristonReinforcementLearningActive2009,
  title = {Reinforcement {{Learning}} or {{Active Inference}}?},
  volume = {4},
  issn = {1932-6203},
  abstract = {This paper questions the need for reinforcement learning or control theory when optimising behaviour. We show that it is fairly simple to teach an agent complicated and adaptive behaviours using a free-energy formulation of perception. In this formulation, agents adjust their internal states and sampling of the environment to minimize their free-energy. Such agents learn causal structure in the environment and sample it in an adaptive and self-supervised fashion. This results in behavioural policies that reproduce those optimised by reinforcement learning and dynamic programming. Critically, we do not need to invoke the notion of reward, value or utility. We illustrate these points by solving a benchmark problem in dynamic programming; namely the mountain-car problem, using active perception or inference under the free-energy principle. The ensuing proof-of-concept may be important because the free-energy formulation furnishes a unified account of both action and perception and may speak to a reappraisal of the role of dopamine in the brain.},
  language = {en},
  number = {7},
  journal = {PLoS ONE},
  doi = {10.1371/journal.pone.0006421},
  author = {Friston, Karl J. and Daunizeau, Jean and Kiebel, Stefan J.},
  editor = {Sporns, Olaf},
  month = jul,
  year = {2009},
  pages = {e6421},
  file = {/Users/lancelotdacosta/Zotero/storage/NVM6SZEI/Friston et al. - 2009 - Reinforcement Learning or Active Inference.pdf}
}

@article{falorsiReparameterizingDistributionsLie2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1903.02958},
  primaryClass = {cs, math, stat},
  title = {Reparameterizing {{Distributions}} on {{Lie Groups}}},
  abstract = {Reparameterizable densities are an important way to learn probability distributions in a deep learning setting. For many distributions it is possible to create low-variance gradient estimators by utilizing a `reparameterization trick'. Due to the absence of a general reparameterization trick, much research has recently been devoted to extend the number of reparameterizable distributional families. Unfortunately, this research has primarily focused on distributions defined in Euclidean space, ruling out the usage of one of the most influential class of spaces with non-trivial topologies: Lie groups. In this work we define a general framework to create reparameterizable densities on arbitrary Lie groups, and provide a detailed practitioners guide to further the ease of usage. We demonstrate how to create complex and multimodal distributions on the well known oriented group of 3D rotations, SO(3), using normalizing flows. Our experiments on applying such distributions in a Bayesian setting for pose estimation on objects with discrete and continuous symmetries, showcase their necessity in achieving realistic uncertainty estimates.},
  language = {en},
  journal = {arXiv:1903.02958 [cs, math, stat]},
  author = {Falorsi, Luca and {de Haan}, Pim and Davidson, Tim R. and Forr{\'e}, Patrick},
  month = mar,
  year = {2019},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Computational Geometry,Mathematics - Probability,Mathematics - Representation Theory},
  file = {/Users/lancelotdacosta/Zotero/storage/U2G63HQX/Falorsi et al. - 2019 - Reparameterizing Distributions on Lie Groups.pdf}
}

@incollection{venterReviewOptimizationTechniques2010,
  address = {{Chichester, UK}},
  title = {Review of {{Optimization Techniques}}},
  isbn = {978-0-470-75440-5 978-0-470-68665-2},
  abstract = {A basic overview of optimization techniques is provided. The standard form of the general non-linear, constrained optimization problem is presented, and various techniques for solving the resulting optimization problem are discussed. The techniques are classified as either local (typically gradient-based) or global (typically nongradient based or evolutionary) algorithms. A great many optimization techniques exist and it is not possible to provide a complete review in the limited space available here. Instead, an effort is made to concentrate on techniques that are commonly used in engineering optimization applications. The review is kept general in nature, without considering special cases like linear programming, convex problems, multi-objective optimization, multidisciplinary optimization, etc. The advantages and disadvantages of the different techniques are highlighted, and suggestions are made to aid the designer in selecting an appropriate technique for a specific problem at hand. Where possible, a short overview of a representative method is presented to aid the discussion of that particular class of algorithms.},
  language = {en},
  booktitle = {Encyclopedia of {{Aerospace Engineering}}},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470686652.eae495},
  author = {Venter, Gerhard},
  editor = {Blockley, Richard and Shyy, Wei},
  month = dec,
  year = {2010},
  pages = {eae495},
  file = {/Users/lancelotdacosta/Zotero/storage/U9EF3RFD/Venter - 2010 - Review of Optimization Techniques.pdf}
}

@article{milanoSelfOrganizingNetsOptimization2004,
  title = {Self-{{Organizing Nets}} for {{Optimization}}},
  volume = {15},
  issn = {1045-9227},
  abstract = {Given some optimization problem and a series of typically expensive trials of solution candidates sampled from a search space, how can we efficiently select the next candidate? We address this fundamental problem by embedding simple optimization strategies in learning algorithms inspired by Kohonen's selforganizing maps and neural gas networks. Our adaptive nets or grids are used to identify and exploit search space regions that maximize the probability of generating points closer to the optima. Net nodes are attracted by candidates that lead to improved evaluations, thus quickly biasing the active data selection process towards promising regions, without loss of ability to escape from local optima. On standard benchmark functions our techniques perform more reliably than the widely used covariance matrix adaptation evolution strategy. The proposed algorithm is also applied to the problem of drag reduction in a flow past an actively controlled circular cylinder, leading to unprecedented drag reduction.},
  language = {en},
  number = {3},
  journal = {IEEE Transactions on Neural Networks},
  doi = {10.1109/TNN.2004.826132},
  author = {Milano, M. and Koumoutsakos, P. and Schmidhuber, J.},
  month = may,
  year = {2004},
  pages = {758-765},
  file = {/Users/lancelotdacosta/Zotero/storage/NF3JTJJT/Milano et al. - 2004 - Self-Organizing Nets for Optimization.pdf}
}

@article{amaranSimulationOptimizationReview2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.08591},
  title = {Simulation Optimization: {{A}} Review of Algorithms and Applications},
  volume = {240},
  issn = {0254-5330, 1572-9338},
  shorttitle = {Simulation Optimization},
  abstract = {Simulation Optimization (SO) refers to the optimization of an objective function subject to constraints, both of which can be evaluated through a stochastic simulation. To address specific features of a particular simulation\textemdash{}discrete or continuous decisions, expensive or cheap simulations, single or multiple outputs, homogeneous or heterogeneous noise\textemdash{}various algorithms have been proposed in the literature. As one can imagine, there exist several competing algorithms for each of these classes of problems. This document emphasizes the difficulties in simulation optimization as compared to mathematical programming, makes reference to state-of-the-art algorithms in the field, examines and contrasts the different approaches used, reviews some of the diverse applications that have been tackled by these methods, and speculates on future directions in the field.},
  language = {en},
  number = {1},
  journal = {Annals of Operations Research},
  doi = {10.1007/s10479-015-2019-x},
  author = {Amaran, Satyajith and Sahinidis, Nikolaos V. and Sharda, Bikram and Bury, Scott J.},
  month = may,
  year = {2016},
  keywords = {Mathematics - Optimization and Control,Computer Science - Data Structures and Algorithms},
  pages = {351-380},
  file = {/Users/lancelotdacosta/Zotero/storage/23BL4VNZ/Amaran et al. - 2016 - Simulation optimization A review of algorithms an.pdf}
}

@article{lengletStatisticsManifoldMultivariate2006,
  title = {Statistics on the {{Manifold}} of {{Multivariate Normal Distributions}}: {{Theory}} and {{Application}} to {{Diffusion Tensor MRI Processing}}},
  volume = {25},
  issn = {0924-9907, 1573-7683},
  shorttitle = {Statistics on the {{Manifold}} of {{Multivariate Normal Distributions}}},
  abstract = {This paper is dedicated to the statistical analysis of the space of multivariate normal distributions with an application to the processing of Diffusion Tensor Images (DTI). It relies on the differential geometrical properties of the underlying parameters space, endowed with a Riemannian metric, as well as on recent works that led to the generalization of the normal law on Riemannian manifolds. We review the geometrical properties of the space of multivariate normal distributions with zero mean vector and focus on an original characterization of the mean, covariance matrix and generalized normal law on that manifold. We extensively address the derivation of accurate and efficient numerical schemes to estimate these statistical parameters. A major application of the present work is related to the analysis and processing of DTI datasets and we show promising results on synthetic and real examples.},
  language = {en},
  number = {3},
  journal = {Journal of Mathematical Imaging and Vision},
  doi = {10.1007/s10851-006-6897-z},
  author = {Lenglet, Christophe and Rousson, Mika{\"e}l and Deriche, Rachid and Faugeras, Olivier},
  month = oct,
  year = {2006},
  pages = {423-444},
  file = {/Users/lancelotdacosta/Zotero/storage/APPVNDY2/Lenglet et al. - 2006 - Statistics on the Manifold of Multivariate Normal .pdf;/Users/lancelotdacosta/Zotero/storage/RGQT4AVL/Lenglet et al. - 2006 - Statistics on the Manifold of Multivariate Normal .pdf}
}

@article{gorbanStochasticSeparationTheorems2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.01203},
  title = {Stochastic {{Separation Theorems}}},
  volume = {94},
  issn = {08936080},
  abstract = {The problem of non-iterative one-shot and non-destructive correction of unavoidable mistakes arises in all Artificial Intelligence applications in the real world. Its solution requires robust separation of samples with errors from samples where the system works properly. We demonstrate that in (moderately) high dimension this separation could be achieved with probability close to one by linear discriminants. Based on fundamental properties of measure concentration, we show that for M {$<$} a exp(bn) random Melement sets in Rn are linearly separable with probability p, p {$>$} 1 - \textvartheta, where 1 {$>$} \textvartheta{} {$>$} 0 is a given small constant. Exact values of a, b {$>$} 0 depend on the probability distribution that determines how the random M-element sets are drawn, and on the constant \textvartheta. These stochastic separation theorems provide a new instrument for the development, analysis, and assessment of machine learning methods and algorithms in high dimension. Theoretical statements are illustrated with numerical examples.},
  language = {en},
  journal = {Neural Networks},
  doi = {10.1016/j.neunet.2017.07.014},
  author = {Gorban, A. N. and Tyukin, I. Y.},
  month = oct,
  year = {2017},
  keywords = {Computer Science - Machine Learning,Computer Science - Artificial Intelligence,I.2.6,68T10},
  pages = {255-259},
  file = {/Users/lancelotdacosta/Zotero/storage/8CQPI8EZ/Gorban and Tyukin - 2017 - Stochastic Separation Theorems.pdf;/Users/lancelotdacosta/Zotero/storage/FZ6DZCAY/Gorban and Tyukin - 2017 - Stochastic Separation Theorems.pdf}
}

@article{brownSuperhumanAIMultiplayer2019,
  title = {Superhuman {{AI}} for Multiplayer Poker},
  issn = {0036-8075, 1095-9203},
  abstract = {In recent years there have been great strides in artificial intelligence (AI), with games often serving as challenge problems, benchmarks, and milestones for progress. Poker has served for decades as such a challenge problem. Past successes in such benchmarks, including poker, have been limited to two-player games. However, poker in particular is traditionally played with more than two players. Multiplayer games present fundamental additional issues beyond those in two-player games, and multiplayer poker is a recognized AI milestone. In this paper we present Pluribus, an AI that we show is stronger than top human professionals in six-player no-limit Texas hold'em poker, the most popular form of poker played by humans.},
  language = {en},
  journal = {Science},
  doi = {10.1126/science.aay2400},
  author = {Brown, Noam and Sandholm, Tuomas},
  month = jul,
  year = {2019},
  pages = {eaay2400},
  file = {/Users/lancelotdacosta/Zotero/storage/BK5H2PXH/Brown and Sandholm - 2019 - Superhuman AI for multiplayer poker.pdf}
}

@article{mumfordDawningAgeStochasticity,
  title = {The {{Dawning}} of the {{Age}} of {{Stochasticity}}},
  abstract = {For over two millennia, Aristotle's logic has ruled over the thinking of western intellectuals. All precise theories, all scientific models, even models of the process of thinking itself, have in principle conformed to the straight- jacket of logic. But from its shady beginnings devising gambling strategies and counting corpses in medieval London, probability theory and statistical inference now emerge as better foundations for scientific models, especially those of the process of thinking and as essential ingredients of theoretical mathematics, even the foundations of mathematics itself. We propose that this sea change in our perspective will affect virtually all of mathematics in the next century.},
  language = {en},
  author = {Mumford, David},
  pages = {22},
  file = {/Users/lancelotdacosta/Zotero/storage/6U4XBUMU/Mumford - The Dawning of the Age of Stochasticity.pdf;/Users/lancelotdacosta/Zotero/storage/RUSKQF8E/Mumford - The Dawning of the Age of Stochasticity.pdf}
}

@article{fristonFreeenergyPrincipleUnified2010,
  title = {The Free-Energy Principle: A Unified Brain Theory?},
  volume = {11},
  issn = {1471-003X, 1471-0048},
  shorttitle = {The Free-Energy Principle},
  abstract = {A free-energy principle has been proposed recently that accounts for action, perception and learning. This Review looks at some key brain theories in the biological (for example, neural Darwinism) and physical (for example, information theory and optimal control theory) sciences from the free-energy perspective. Crucially, one key theme runs through each of these theories \textemdash{} optimization. Furthermore, if we look closely at what is optimized, the same quantity keeps emerging, namely value (expected reward, expected utility) or its complement, surprise (prediction error, expected cost). This is the quantity that is optimized under the free-energy principle, which suggests that several global brain theories might be unified within a free-energy framework.},
  language = {en},
  number = {2},
  journal = {Nature Reviews Neuroscience},
  doi = {10.1038/nrn2787},
  author = {Friston, Karl},
  month = feb,
  year = {2010},
  pages = {127-138},
  file = {/Users/lancelotdacosta/Zotero/storage/3JRFS4LA/Friston - 2010 - The free-energy principle a unified brain theory.pdf;/Users/lancelotdacosta/Zotero/storage/K4AVGHFY/Friston - 2010 - The free-energy principle a unified brain theory.pdf}
}

@article{buckleyFreeEnergyPrinciple2017,
  title = {The Free Energy Principle for Action and Perception: {{A}} Mathematical Review},
  volume = {81},
  issn = {00222496},
  shorttitle = {The Free Energy Principle for Action and Perception},
  abstract = {The `free energy principle' (FEP) has been suggested to provide a unified theory of the brain, integrating data and theory relating to action, perception, and learning. The theory and implementation of the FEP combines insights from Helmholtzian `perception as inference', machine learning theory, and statistical thermodynamics. Here, we provide a detailed mathematical evaluation of a suggested biologically plausible implementation of the FEP that has been widely used to develop the theory. Our objectives are (i) to describe within a single article the mathematical structure of this implementation of the FEP; (ii) provide a simple but complete agent-based model utilising the FEP and (iii) to disclose the assumption structure of this implementation of the FEP to help elucidate its significance for the brain sciences.},
  language = {en},
  journal = {Journal of Mathematical Psychology},
  doi = {10.1016/j.jmp.2017.09.004},
  author = {Buckley, Christopher L. and Kim, Chang Sub and McGregor, Simon and Seth, Anil K.},
  month = dec,
  year = {2017},
  pages = {55-79},
  file = {/Users/lancelotdacosta/Zotero/storage/4E5Q986F/Buckley et al. - 2017 - The free energy principle for action and perceptio.pdf;/Users/lancelotdacosta/Zotero/storage/XITSATA2/Buckley et al. - 2017 - The free energy principle for action and perceptio.pdf}
}

@article{fristonGraphicalBrainBelief2017,
  title = {The Graphical Brain: {{Belief}} Propagation and Active Inference},
  volume = {1},
  issn = {2472-1751},
  shorttitle = {The Graphical Brain},
  abstract = {This paper considers functional integration in the brain from a computational perspective. We ask what sort of neuronal message passing is mandated by active inference\textemdash{}and what implications this has for context-sensitive connectivity at microscopic and macroscopic levels. In particular, we formulate neuronal processing as belief propagation under deep generative models. Crucially, these models can entertain both discrete and continuous states, leading to distinct schemes for belief updating that play out on the same (neuronal) architecture. Technically, we use Forney (normal) factor graphs to elucidate the requisite message passing in terms of its form and scheduling. To accommodate mixed generative models (of discrete and continuous states), one also has to consider link nodes or factors that enable discrete and continuous representations to talk to each other. When mapping the implicit computational architecture onto neuronal connectivity, several interesting features emerge. For example, Bayesian model averaging and comparison, which link discrete and continuous states, may be implemented in thalamocortical loops. These and other considerations speak to a computational connectome that is inherently state dependent and self-organizing in ways that yield to a principled (variational) account. We conclude with simulations of reading that illustrate the implicit neuronal message passing, with a special focus on how discrete (semantic) representations inform, and are informed by, continuous (visual) sampling of the sensorium.},
  language = {en},
  number = {4},
  journal = {Network Neuroscience},
  doi = {10.1162/NETN_a_00018},
  author = {Friston, Karl J. and Parr, Thomas and {de Vries}, Bert},
  month = dec,
  year = {2017},
  pages = {381-414},
  file = {/Users/lancelotdacosta/Zotero/storage/9C4GJ837/Friston et al. - 2017 - The graphical brain Belief propagation and active.pdf;/Users/lancelotdacosta/Zotero/storage/WLJSTUQC/Friston et al. - 2017 - The graphical brain Belief propagation and active.pdf;/Users/lancelotdacosta/Zotero/storage/WNCXNZB8/Friston et al. - 2017 - The graphical brain Belief propagation and active.pdf}
}

@article{dayanHelmholtzMachine1995,
  title = {The {{Helmholtz Machine}}},
  volume = {7},
  issn = {0899-7667, 1530-888X},
  language = {en},
  number = {5},
  journal = {Neural Computation},
  doi = {10.1162/neco.1995.7.5.889},
  author = {Dayan, Peter and Hinton, Geoffrey E. and Neal, Radford M. and Zemel, Richard S.},
  month = sep,
  year = {1995},
  pages = {889-904},
  file = {/Users/lancelotdacosta/Zotero/storage/N9NB7TVL/Dayan et al. - 1995 - The Helmholtz Machine.pdf}
}

@article{claxtonIrrelevanceInferenceDecisionmaking1999,
  title = {The Irrelevance of Inference: A Decision-Making Approach to the Stochastic Evaluation of Health Care Technologies},
  volume = {18},
  issn = {01676296},
  shorttitle = {The Irrelevance of Inference},
  language = {en},
  number = {3},
  journal = {Journal of Health Economics},
  doi = {10.1016/S0167-6296(98)00039-3},
  author = {Claxton, Karl},
  month = jun,
  year = {1999},
  pages = {341-364},
  file = {/Users/lancelotdacosta/Zotero/storage/5CN642TR/Claxton - 1999 - The irrelevance of inference a decision-making ap.pdf}
}

@unpublished{petersenMatrixCookbook2012,
  title = {The {{Matrix Cookbook}}},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  year = {2012},
  address = {{http://matrixcookbook.com}},
  file = {/Users/lancelotdacosta/Zotero/storage/IRXPHWFD/Petersen and Pedersen - [ httpmatrixcookbook.com ].pdf;/Users/lancelotdacosta/Zotero/storage/P7HHSVRV/Petersen and Pedersen - [ httpmatrixcookbook.com ].pdf},
  language = {en}
}



@article{petersenMatrixCookbook,
  title = {The {{Matrix Cookbook}}},
  language = {en},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  pages = {72},
  file = {/Users/lancelotdacosta/Zotero/storage/IRXPHWFD/Petersen and Pedersen - [ httpmatrixcookbook.com ].pdf;/Users/lancelotdacosta/Zotero/storage/P7HHSVRV/Petersen and Pedersen - [ httpmatrixcookbook.com ].pdf}
}

@article{mcfaddenOriginsQuantumBiology2018,
  title = {The Origins of Quantum Biology},
  volume = {474},
  issn = {1364-5021, 1471-2946},
  language = {en},
  number = {2220},
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  doi = {10.1098/rspa.2018.0674},
  author = {McFadden, Johnjoe and {Al-Khalili}, Jim},
  month = dec,
  year = {2018},
  pages = {20180674},
  file = {/Users/lancelotdacosta/Zotero/storage/QX3492LY/McFadden and Al-Khalili - 2018 - The origins of quantum biology.pdf;/Users/lancelotdacosta/Zotero/storage/URM7UHNL/McFadden and Al-Khalili - 2018 - The origins of quantum biology.pdf}
}

@article{rosenRepresentationBiologicalSystems1958,
  title = {The Representation of Biological Systems from the Standpoint of the Theory of Categories},
  volume = {20},
  issn = {0007-4985, 1522-9602},
  language = {en},
  number = {4},
  journal = {The Bulletin of Mathematical Biophysics},
  doi = {10.1007/BF02477890},
  author = {Rosen, Robert},
  month = dec,
  year = {1958},
  pages = {317-341},
  file = {/Users/lancelotdacosta/Zotero/storage/AAGPCANF/Rosen - 1958 - The representation of biological systems from the .pdf;/Users/lancelotdacosta/Zotero/storage/ZZWMADVY/Rosen - 1958 - The representation of biological systems from the .pdf}
}

@article{kantasSharpFlatShallow2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.04121},
  primaryClass = {cs, stat},
  title = {The Sharp, the Flat and the Shallow: {{Can}} Weakly Interacting Agents Learn to Escape Bad Minima?},
  shorttitle = {The Sharp, the Flat and the Shallow},
  abstract = {An open problem in machine learning is whether flat minima generalize better and how to compute such minima efficiently. This is a very challenging problem. As a first step towards understanding this question we formalize it as an optimization problem with weakly interacting agents. We review appropriate background material from the theory of stochastic processes and provide insights that are relevant to practitioners. We propose an algorithmic framework for an extended stochastic gradient Langevin dynamics and illustrate its potential. The paper is written as a tutorial, and presents an alternative use of multi-agent learning. Our primary focus is on the design of algorithms for machine learning applications; however the underlying mathematical framework is suitable for the understanding of large scale systems of agent based models that are popular in the social sciences, economics and finance.},
  language = {en},
  journal = {arXiv:1905.04121 [cs, stat]},
  author = {Kantas, Nikolas and Parpas, Panos and Pavliotis, Grigorios A.},
  month = may,
  year = {2019},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Multiagent Systems},
  file = {/Users/lancelotdacosta/Zotero/storage/IMME347K/Kantas et al. - 2019 - The sharp, the flat and the shallow Can weakly in.pdf;/Users/lancelotdacosta/Zotero/storage/KMFQ4BUX/Kantas et al. - 2019 - The sharp, the flat and the shallow Can weakly in.pdf}
}

@article{tshitoyanUnsupervisedWordEmbeddings2019,
  title = {Unsupervised Word Embeddings Capture Latent Knowledge from Materials Science Literature},
  volume = {571},
  issn = {0028-0836, 1476-4687},
  language = {en},
  number = {7763},
  journal = {Nature},
  doi = {10.1038/s41586-019-1335-8},
  author = {Tshitoyan, Vahe and Dagdelen, John and Weston, Leigh and Dunn, Alexander and Rong, Ziqin and Kononova, Olga and Persson, Kristin A. and Ceder, Gerbrand and Jain, Anubhav},
  month = jul,
  year = {2019},
  pages = {95-98},
  file = {/Users/lancelotdacosta/Zotero/storage/9K33CFGL/Tshitoyan et al. - 2019 - Unsupervised word embeddings capture latent knowle.pdf;/Users/lancelotdacosta/Zotero/storage/M687TCQX/Tshitoyan et al. - 2019 - Unsupervised word embeddings capture latent knowle.pdf}
}

@article{bealVariationalAlgorithmsApproximate2003,
  title = {Variational {{Algorithms}} for {{Approximate Bayesian Inference}}},
  language = {en},
  author = {Beal, Matthew James},
  year = {2003},
  pages = {281},
  file = {/Users/lancelotdacosta/Zotero/storage/9ERH2SVS/Beal - Variational Algorithms for Approximate Bayesian In.pdf}
}

@article{fristonVariationalFreeEnergy2007,
  title = {Variational Free Energy and the {{Laplace}} Approximation},
  volume = {34},
  issn = {10538119},
  language = {en},
  number = {1},
  journal = {NeuroImage},
  doi = {10.1016/j.neuroimage.2006.08.035},
  author = {Friston, Karl and Mattout, J{\'e}r{\'e}mie and {Trujillo-Barreto}, Nelson and Ashburner, John and Penny, Will},
  month = jan,
  year = {2007},
  pages = {220-234},
  file = {/Users/lancelotdacosta/Zotero/storage/FNPMRUNH/Friston et al. - 2007 - Variational free energy and the Laplace approximat.pdf;/Users/lancelotdacosta/Zotero/storage/WFEHBWVV/Friston et al. - 2007 - Variational free energy and the Laplace approximat.pdf}
}

@article{behrensWhatCognitiveMap2018,
  title = {What {{Is}} a {{Cognitive Map}}? {{Organizing Knowledge}} for {{Flexible Behavior}}},
  volume = {100},
  issn = {08966273},
  shorttitle = {What {{Is}} a {{Cognitive Map}}?},
  language = {en},
  number = {2},
  journal = {Neuron},
  doi = {10.1016/j.neuron.2018.10.002},
  author = {Behrens, Timothy E.J. and Muller, Timothy H. and Whittington, James C.R. and Mark, Shirley and Baram, Alon B. and Stachenfeld, Kimberly L. and {Kurth-Nelson}, Zeb},
  month = oct,
  year = {2018},
  pages = {490-509},
  file = {/Users/lancelotdacosta/Zotero/storage/ZB7BE6JD/Behrens et al. - 2018 - What Is a Cognitive Map Organizing Knowledge for .pdf}
}

@article{bradleyWhatAppliedCategory2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1809.05923},
  primaryClass = {math},
  title = {What Is {{Applied Category Theory}}?},
  abstract = {This is a collection of introductory, expository notes on applied category theory, inspired by the 2018 Applied Category Theory Workshop, and in these notes we take a leisurely stroll through two themes (functorial semantics and compositionality), two constructions (monoidal categories and decorated cospans) and two examples (chemical reaction networks and natural language processing) within the field.},
  language = {en},
  journal = {arXiv:1809.05923 [math]},
  author = {Bradley, Tai-Danae},
  month = sep,
  year = {2018},
  keywords = {Mathematics - Category Theory},
  file = {/Users/lancelotdacosta/Zotero/storage/9V2MS8WM/Bradley - 2018 - What is Applied Category Theory.pdf;/Users/lancelotdacosta/Zotero/storage/VDGBJMZF/Bradley - 2018 - What is Applied Category Theory.pdf}
}

@article{haWorldModels2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.10122},
  primaryClass = {cs, stat},
  title = {World {{Models}}},
  abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment.},
  language = {en},
  journal = {arXiv:1803.10122 [cs, stat]},
  doi = {10.5281/zenodo.1207631},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/lancelotdacosta/Zotero/storage/5FL7ESFS/Ha and Schmidhuber - 2018 - World Models.pdf;/Users/lancelotdacosta/Zotero/storage/9RPQ8G85/Ha and Schmidhuber - 2018 - World Models.pdf}
}

@book{bishopPatternRecognitionMachine2006,
  address = {{New York}},
  series = {Information Science and Statistics},
  title = {Pattern Recognition and Machine Learning},
  isbn = {978-0-387-31073-2},
  lccn = {Q327 .B52 2006},
  language = {en},
  publisher = {{Springer}},
  author = {Bishop, Christopher M.},
  year = {2006},
  keywords = {Machine learning,Pattern perception},
  file = {/Users/lancelotdacosta/Zotero/storage/KIXUTKNM/Bishop - 2006 - Pattern recognition and machine learning.pdf}
}

@article{wilsonMathematicalTheoryFunctional1973,
  title = {A Mathematical Theory of the Functional Dynamics of Cortical and Thalamic Nervous Tissue},
  volume = {13},
  issn = {0340-1200, 1432-0770},
  abstract = {It is proposed that distinct anatomical regions of cerebral cortex and of thalamic nuclei are functionally two-dimensional. On this view, the third (radial) dimension of cortical and thalamic structures is associated with a redundancy of circuits and functions so that reliable signal processing obtains in the presence of noisy or ambiguous stimuli.},
  language = {en},
  number = {2},
  journal = {Kybernetik},
  doi = {10.1007/BF00288786},
  author = {Wilson, H. R. and Cowan, J. D.},
  month = sep,
  year = {1973},
  pages = {55-80},
  file = {/Users/lancelotdacosta/Zotero/storage/URV6DZLF/Wilson and Cowan - 1973 - A mathematical theory of the functional dynamics o.pdf}
}

@article{marreirosPopulationDynamicsLaplace2009,
  title = {Population Dynamics under the {{Laplace}} Assumption},
  volume = {44},
  issn = {1053-8119},
  abstract = {In this paper, we describe a generic approach to modelling dynamics in neuronal populations. This approach models a full density on the states of neuronal populations but finesses this high-dimensional problem by re-formulating density dynamics in terms of ordinary differential equations on the sufficient statistics of the densities considered (c.f., the method of moments). The particular form for the population density we adopt is a Gaussian density (c.f., the Laplace assumption). This means population dynamics are described by equations governing the evolution of the population's mean and covariance. We derive these equations from the Fokker-Planck formalism and illustrate their application to a conductance-based model of neuronal exchanges. One interesting aspect of this formulation is that we can uncouple the mean and covariance to furnish a neural-mass model, which rests only on the populations mean. This enables us to compare equivalent mean-field and neural-mass models of the same populations and evaluate, quantitatively, the contribution of population variance to the expected dynamics. The mean-field model presented here will form the basis of a dynamic causal model of observed electromagnetic signals in future work.},
  number = {3},
  journal = {NeuroImage},
  doi = {10.1016/j.neuroimage.2008.10.008},
  author = {Marreiros, Andr{\'e} C. and Kiebel, Stefan J. and Daunizeau, Jean and Harrison, Lee M. and Friston, Karl J.},
  month = feb,
  year = {2009},
  keywords = {Laplace assumption,Mean-field,Modelling,Neural-mass models,Neuronal,Nonlinear},
  pages = {701-714},
  file = {/Users/lancelotdacosta/Zotero/storage/W8QWE5PS/S1053811908011063.html}
}

@article{frickerCellattachedMeasurementsFiring1999,
  title = {Cell-Attached Measurements of the Firing Threshold of Rat Hippocampal Neurones},
  volume = {517},
  issn = {0022-3751},
  abstract = {The cell-attached configuration of the patch-clamp technique was used to assess resting membrane potential and firing threshold of CA1 pyramidal cells and interneurones of rat hippocampal slices.
            
            
              Resting potential was inferred from the reversal potential of voltage-gated K+ currents with symmetrical intracellular and pipette K+ concentrations. Its mean value was -74 {$\pm$} 9 mV for silent interneurones (mean {$\pm$} s.d.; n = 17) and -84 {$\pm$} 7 mV for silent pyramidal cells (n = 8). Spontaneous action currents occurred in thirteen out of thirty-two interneurones and two out of ten pyramidal cells. In active cells, membrane potential values fluctuated by up to 20 mV, due in part to the large hyperpolarizations that followed an action current.
            
            
              Membrane potential values determined from K+ current reversal were 13 {$\pm$} 6 mV more hyperpolarized than those measured in whole-cell recordings from the same neurones (n = 8), probably due to a Donnan equilibrium potential between pipette and cytoplasm.
            
            
              Firing threshold of silent cells was determined by elevating external K+ until action currents were generated, while membrane potential was monitored from the cell-attached K+ current reversal. Spike threshold was attained at -49 {$\pm$} 8 mV for interneurones (n = 17) and at -60 {$\pm$} 8 mV for pyramidal cells (n = 8). Increasing external Ca2+ from 2 to 4 mM shifted the neuronal voltage threshold by +5 mV, without affecting resting potential.
            
            
              For comparison with these values, we examined how the rate of membrane polarization influenced firing threshold in whole-cell records. Ramp current injections, of duration 15\textendash{}1500 ms, revealed that current threshold followed a classical strength-duration relationship. In contrast voltage threshold, determined from current injection or by elevating extracellular K+, varied little with the rate of membrane polarization.
            
            
              The state of activation and inactivation of Na+ and K+ currents might contribute to the stability of the voltage threshold. Cell-attached records showed that 79 {$\pm$} 10 \% of Na+ channels and 64 {$\pm$} 10 \% of K+ channels were available for activation at resting potential in silent cells (n = 8). As cells were depolarized to threshold, Na+ current availability was reduced to 23 {$\pm$} 10 \%, and K+ current availability to 31 {$\pm$} 12 \%.
            
            
              The speed of transition into the inactivated states also appears to contribute to the invariance of threshold for all but the fastest depolarizations. At potentials close to threshold, the rate of inactivation of Na+ and K+ followed a double exponential time course, such that Na+ currents were 62 \% inactivated and K+ currents were 63 \% inactivated within 15 ms.},
  number = {Pt 3},
  journal = {The Journal of Physiology},
  doi = {10.1111/j.1469-7793.1999.0791s.x},
  author = {Fricker, Desdemona and Verheugen, Jos A H and Miles, Richard},
  month = jun,
  year = {1999},
  pages = {791-804},
  file = {/Users/lancelotdacosta/Zotero/storage/5X6MLDID/Fricker et al. - 1999 - Cell-attached measurements of the firing threshold.pdf},
  pmid = {10358119},
  pmcid = {PMC2269376}
}

@article{steinCodingPositionSimultaneously2004,
  title = {Coding of Position by Simultaneously Recorded Sensory Neurones in the Cat Dorsal Root Ganglion: {{Coding}} of Dorsal Root Ganglion Neurones},
  volume = {560},
  issn = {00223751},
  shorttitle = {Coding of Position by Simultaneously Recorded Sensory Neurones in the Cat Dorsal Root Ganglion},
  language = {en},
  number = {3},
  journal = {The Journal of Physiology},
  doi = {10.1113/jphysiol.2004.068668},
  author = {Stein, R. B. and Weber, D. J. and Aoyagi, Y. and Prochazka, A. and Wagenaar, J. B. M. and Shoham, S. and Normann, R. A.},
  month = nov,
  year = {2004},
  pages = {883-896},
  file = {/Users/lancelotdacosta/Zotero/storage/LGUNHBZ6/Stein et al. - 2004 - Coding of position by simultaneously recorded sens.pdf}
}

@article{weberDecodingSensoryFeedback2006,
  title = {Decoding {{Sensory Feedback From Firing Rates}} of {{Afferent Ensembles Recorded}} in {{Cat Dorsal Root Ganglia}} in {{Normal Locomotion}}},
  volume = {14},
  issn = {1534-4320},
  abstract = {Sensory feedback is required by biological motor control systems to maintain stability, respond to perturbations, and adapt. Similarly, motor neuroprostheses require feedback to provide natural and complete restoration of motor functions. In this paper, we show that ensemble firing rates from the body's mechanoreceptors can provide a natural source of kinematic state feedback and could be useful for prosthetic control. Single unit recordings from multiple primary afferent neurons were obtained during walking using multichannel electrode arrays implanted chronically in the L7 dorsal root ganglia of three cats. We typically recorded simultaneously from over 20\textendash{}30 neurons during the first 7\textendash{}14 days after surgery, but recordings gradually worsened thereafter. Histology indicates that a ring of inflammatory and connective tissues (100 m thick) develops around each microelectrode and likely contributes to the degradation in recording quality. Accurate estimates of the hindlimb trajectory were made using a linear filter with inputs from only a few neurons highly correlated with limb kinematics. The coefficients for the linear filter were identified in a least-squares fit with 5\textendash{}10 s of walking data (model training stage). The estimated and actual trajectories of separate walking data generally match well for walking at a range of speeds accounting for 63 22\% (mean S D for hip, knee, and ankle) of the variance in joint angle and 72 4\% of the variance in joint angular velocities. These results indicate that a neural interface with primary sensory neurons in the dorsal root ganglion can provide accurate kinematic state information that may be useful for closed loop control of a neuroprosthesis.},
  language = {en},
  number = {2},
  journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  doi = {10.1109/TNSRE.2006.875575},
  author = {Weber, D.J. and Stein, R.B. and Everaert, D.G. and Prochazka, A.},
  month = jun,
  year = {2006},
  pages = {240-243},
  file = {/Users/lancelotdacosta/Zotero/storage/SLF5IUVX/Weber et al. - 2006 - Decoding Sensory Feedback From Firing Rates of Aff.pdf}
}

@article{aronovMappingNonspatialDimension2017,
  title = {Mapping of a Non-Spatial Dimension by the Hippocampal\textendash{}Entorhinal Circuit},
  volume = {543},
  issn = {0028-0836, 1476-4687},
  abstract = {During spatial navigation, neural activity in the hippocampus and the medial entorhinal cortex (MEC) is correlated to navigational variables like location1,2, head direction3, speed4, and proximity to boundaries5. These activity patterns are thought to provide a map-like representation of physical space. However, the hippocampal/entorhinal circuit is involved not only in spatial navigation, but in a variety of memory-guided behaviors6. The relationship between this general function and the specialized spatial activity patterns is unclear. A conceptual framework reconciling these views is that spatial representation is just one example of a more general mechanism for encoding continuous, task-relevant variables7\textendash{}10. We tested this idea by recording hippocampal and entorhinal neurons in a task that required rats to use a joystick to manipulate sound along a continuous frequency axis. We found neural representation of the entire behavioral task, including activity that formed discrete firing fields at particular sound frequencies. Neurons involved in this representation overlapped with the known spatial cell types in the circuit like place cells and grid cells. These results suggest that common circuit mechanisms in the hippocampal/ entorhinal system are used for representations of diverse behavioral tasks, possibly supporting cognitive processes beyond spatial navigation.},
  language = {en},
  number = {7647},
  journal = {Nature},
  doi = {10.1038/nature21692},
  author = {Aronov, Dmitriy and Nevers, Rhino and Tank, David W.},
  month = mar,
  year = {2017},
  pages = {719-722},
  file = {/Users/lancelotdacosta/Zotero/storage/4XBGS6M6/Aronov et al. - 2017 - Mapping of a non-spatial dimension by the hippocam.pdf}
}

@article{haftingMicrostructureSpatialMap2005,
  title = {Microstructure of a Spatial Map in the Entorhinal Cortex},
  volume = {436},
  issn = {0028-0836, 1476-4687},
  language = {en},
  number = {7052},
  journal = {Nature},
  doi = {10.1038/nature03721},
  author = {Hafting, Torkel and Fyhn, Marianne and Molden, Sturla and Moser, May-Britt and Moser, Edvard I.},
  month = aug,
  year = {2005},
  pages = {801-806},
  file = {/Users/lancelotdacosta/Zotero/storage/N36A3RUA/Hafting et al. - 2005 - Microstructure of a spatial map in the entorhinal .pdf}
}

@article{stachenfeldHippocampusPredictiveMap2017,
  title = {The Hippocampus as a Predictive Map},
  volume = {20},
  issn = {1097-6256, 1546-1726},
  language = {en},
  number = {11},
  journal = {Nature Neuroscience},
  doi = {10.1038/nn.4650},
  author = {Stachenfeld, Kimberly L and Botvinick, Matthew M and Gershman, Samuel J},
  month = nov,
  year = {2017},
  pages = {1643-1653},
  file = {/Users/lancelotdacosta/Zotero/storage/2KWV8753/Stachenfeld et al. - 2017 - The hippocampus as a predictive map.pdf}
}

@article{okeefeHippocampusSpatialMap1971,
  title = {The Hippocampus as a Spatial Map. {{Preliminary}} Evidence from Unit Activity in the Freely-Moving Rat},
  volume = {34},
  issn = {00068993},
  language = {en},
  number = {1},
  journal = {Brain Research},
  doi = {10.1016/0006-8993(71)90358-1},
  author = {O'Keefe, J. and Dostrovsky, J.},
  month = nov,
  year = {1971},
  pages = {171-175},
  file = {/Users/lancelotdacosta/Zotero/storage/ECKTFPWH/O'Keefe and Dostrovsky - 1971 - The hippocampus as a spatial map. Preliminary evid.pdf}
}

@book{amariInformationGeometryIts2016,
  title = {Information Geometry and Its Applications},
  publisher = {{Springer}},
  author = {Amari, S.},
  year = {2016}
}

@article{giryCATEGORICALAPPROACHPROBABILITY,
  title = {A {{CATEGORICAL APPROACH TO PROBABILITY THEORY}}},
  language = {en},
  author = {Giry, Michle},
  pages = {18},
  file = {/Users/lancelotdacosta/Zotero/storage/AI5HZTN7/Giry - A CATEGORICAL APPROACH TO PROBABILITY THEORY.pdf}
}

@book{cencovStatisticalDecisionRules1982,
  series = {Translations of Mathematical Monographs},
  title = {Statistical {{Decision Rules}} and {{Optimal Inference}}},
  volume = {53},
  author = {Cencov, N. N.},
  year = {1982}
}

@article{bruinebergFreeenergyMinimizationJoint2018,
  title = {Free-Energy Minimization in Joint Agent-Environment Systems: {{A}} Niche Construction Perspective},
  volume = {455},
  issn = {0022-5193},
  shorttitle = {Free-Energy Minimization in Joint Agent-Environment Systems},
  abstract = {\textbullet{}
              Free-energy is developed as a measure for the `fit' between an agent and its niche.
            
            
              \textbullet{}
              Simulations show how the behavior of an agent is shaped by the structure of its niche.
            
            
              \textbullet{}
              Using computational methods, we show how niche-construction can improve the `fit' between an agent and its environment.
            
          
        , The free-energy principle is an attempt to explain the structure of the agent and its brain, starting from the fact that an agent exists (Friston and Stephan, 2007; Friston~et~al., 2010). More specifically, it can be regarded as a systematic attempt to understand the `fit' between an embodied agent and its niche, where the quantity of free-energy is a measure for the `misfit' or disattunement (Bruineberg and Rietveld, 2014) between agent and environment. This paper offers a proof-of-principle simulation of niche construction under the free-energy principle. Agent-centered treatments have so far failed to address situations where environments change alongside agents, often due to the action of agents themselves. The key point of this paper is that the minimum of free-energy is not at a point in which the agent is maximally adapted to the statistics of a static environment, but can better be conceptualized an attracting manifold within the joint agent-environment state-space as a whole, which the system tends toward through mutual interaction. We will provide a general introduction to active inference and the free-energy principle. Using Markov Decision Processes (MDPs), we then describe a canonical generative model and the ensuing update equations that minimize free-energy. We then apply these equations to simulations of foraging in an environment; in which an agent learns the most efficient path to a pre-specified location. In some of those simulations, unbeknownst to the agent, the `desire paths' emerge as a function of the activity of the agent (i.e. niche construction occurs). We will show how, depending on the relative inertia of the environment and agent, the joint agent-environment system moves to different attracting sets of jointly minimized free-energy.},
  journal = {Journal of Theoretical Biology},
  doi = {10.1016/j.jtbi.2018.07.002},
  author = {Bruineberg, Jelle and Rietveld, Erik and Parr, Thomas and {van Maanen}, Leendert and Friston, Karl J},
  month = oct,
  year = {2018},
  pages = {161-178},
  file = {/Users/lancelotdacosta/Zotero/storage/6AH8W5TX/Bruineberg et al. - 2018 - Free-energy minimization in joint agent-environmen.pdf},
  pmid = {30012517},
  pmcid = {PMC6117456}
}

@article{constantRegimesExpectationsActive2019,
  title = {Regimes of {{Expectations}}: {{An Active Inference Model}} of {{Social Conformity}} and {{Human Decision Making}}},
  volume = {10},
  issn = {1664-1078},
  shorttitle = {Regimes of {{Expectations}}},
  abstract = {How do humans come to acquire shared expectations about how they ought to behave in distinct normalised social settings? This paper offers a normative framework to answer this question. We introduce the computational construct of `deontic value' \textendash{} based on active inference and Markov decision processes \textendash{} to formalise conceptions of social conformity and human decision-making. Deontic value is an attribute of choices, behaviours, or action sequences that inherit directly from deontic cues in our econiche (e.g., red traffic lights); namely, cues that denote an obligatory social rule. Crucially, the prosocial aspect of deontic value rests upon a particular form of circular causality: deontic cues exist in the environment in virtue of the environment being modified by repeated actions, while action itself is contingent upon the deontic value of environmental cues. We argue that this construction of deontic cues enables the epistemic (i.e., information-seeking) and pragmatic (i.e., goal- seeking) values of any behaviour to be `cached' or `outsourced' to the environment, where the environment effectively `learns' about the behaviour of its denizens. We describe the process whereby this particular aspect of value enables learning of habitual behaviour over neurodevelopmental and transgenerational timescales.},
  language = {English},
  journal = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2019.00679},
  author = {Constant, Axel and Ramstead, Maxwell J. D. and Veissi{\`e}re, Samuel P. L. and Friston, Karl},
  year = {2019},
  keywords = {Active inference.,Decision - making,deonticity,Markov decision algorithm,niche construction,Social Conformity},
  file = {/Users/lancelotdacosta/Zotero/storage/NZVYA92N/Constant et al. - 2019 - Regimes of Expectations An Active Inference Model.pdf}
}

@article{kaplanPlanningNavigationActive2018,
  title = {Planning and Navigation as Active Inference},
  volume = {112},
  issn = {1432-0770},
  abstract = {This paper introduces an active inference formulation of planning and navigation. It illustrates how the exploitation\textendash{}exploration dilemma is dissolved by acting to minimise uncertainty (i.e. expected surprise or free energy). We use simulations of a maze problem to illustrate how agents can solve quite complicated problems using context sensitive prior preferences to form subgoals. Our focus is on how epistemic behaviour\textemdash{}driven by novelty and the imperative to reduce uncertainty about the world\textemdash{}contextualises pragmatic or goal-directed behaviour. Using simulations, we illustrate the underlying process theory with synthetic behavioural and electrophysiological responses during exploration of a maze and subsequent navigation to a target location. An interesting phenomenon that emerged from the simulations was a putative distinction between `place cells'\textemdash{}that fire when a subgoal is reached\textemdash{}and `path cells'\textemdash{}that fire until a subgoal is reached.},
  language = {en},
  number = {4},
  journal = {Biological Cybernetics},
  doi = {10.1007/s00422-018-0753-2},
  author = {Kaplan, Raphael and Friston, Karl J.},
  month = aug,
  year = {2018},
  keywords = {Active inference,Bayesian,Curiosity,Epistemic value,Exploitation,Exploration,Free energy,Novelty,Salience},
  pages = {323-343},
  file = {/Users/lancelotdacosta/Zotero/storage/Y2QS7TJL/Kaplan and Friston - 2018 - Planning and navigation as active inference.pdf}
}

@article{parrComputationalAnatomyVisual2018,
  title = {The {{Computational Anatomy}} of {{Visual Neglect}}},
  volume = {28},
  issn = {1460-2199},
  abstract = {Visual neglect is a debilitating neuropsychological phenomenon that has many clinical implications and-in cognitive neuroscience-offers an important lesion deficit model. In this article, we describe a computational model of visual neglect based upon active inference. Our objective is to establish a computational and neurophysiological process theory that can be used to disambiguate among the various causes of this important syndrome; namely, a computational neuropsychology of visual neglect. We introduce a Bayes optimal model based upon Markov decision processes that reproduces the visual searches induced by the line cancellation task (used to characterize visual neglect at the bedside). We then consider 3 distinct ways in which the model could be lesioned to reproduce neuropsychological (visual search) deficits. Crucially, these 3 levels of pathology map nicely onto the neuroanatomy of saccadic eye movements and the systems implicated in visual neglect.},
  language = {eng},
  number = {2},
  journal = {Cerebral Cortex (New York, N.Y.: 1991)},
  doi = {10.1093/cercor/bhx316},
  author = {Parr, Thomas and Friston, Karl J.},
  month = jan,
  year = {2018},
  keywords = {Bayesian,active inference,Attention,Computer Simulation,Eye Movements,Functional Laterality,Humans,Models; Anatomic,Neural Pathways,Neuropsychological Tests,neuropsychology,Perceptual Disorders,Photic Stimulation,saccades,visual neglect,Visual Perception},
  pages = {777-790},
  file = {/Users/lancelotdacosta/Zotero/storage/QAMNLASJ/Parr and Friston - 2018 - The Computational Anatomy of Visual Neglect.pdf;/Users/lancelotdacosta/Zotero/storage/QB2ST5K4/Parr and Friston - 2018 - The Computational Anatomy of Visual Neglect.pdf},
  pmid = {29190328},
  pmcid = {PMC6005118}
}

@article{parrUncertaintyEpistemicsActive2017,
  title = {Uncertainty, Epistemics and Active Inference},
  volume = {14},
  issn = {1742-5689},
  abstract = {Biological systems\textemdash{}like ourselves\textemdash{}are constantly faced with uncertainty. Despite noisy sensory data, and volatile environments, creatures appear to actively maintain their integrity. To account for this remarkable ability to make optimal decisions in the face of a capricious world, we propose a generative model that represents the beliefs an agent might possess about their own uncertainty. By simulating a noisy and volatile environment, we demonstrate how uncertainty influences optimal epistemic (visual) foraging. In our simulations, saccades were deployed less frequently to regions with a lower sensory precision, while a greater volatility led to a shorter inhibition of return. These simulations illustrate a principled explanation for some cardinal aspects of visual foraging\textemdash{}and allow us to propose a correspondence between the representation of uncertainty and ascending neuromodulatory systems, complementing that suggested by Yu \& Dayan (Yu \& Dayan 2005 Neuron
46, 681\textendash{}692. (doi:10.1016/j.neuron.2005.04.026)).},
  number = {136},
  journal = {Journal of the Royal Society Interface},
  doi = {10.1098/rsif.2017.0376},
  author = {Parr, Thomas and Friston, Karl J.},
  month = nov,
  year = {2017},
  file = {/Users/lancelotdacosta/Zotero/storage/YL33H6HD/Parr and Friston - 2017 - Uncertainty, epistemics and active inference.pdf},
  pmid = {29167370},
  pmcid = {PMC5721148}
}

@article{mirzaSceneConstructionVisual2016,
  title = {Scene {{Construction}}, {{Visual Foraging}}, and {{Active Inference}}},
  volume = {10},
  issn = {1662-5188},
  abstract = {This paper describes an active inference scheme for visual searches and the perceptual synthesis entailed by scene construction. Active inference assumes that perception and action minimize variational free energy, where actions are selected to minimize the free energy expected in the future. This assumption generalizes risk-sensitive control and expected utility theory to include epistemic value; namely, the value (or salience) of information inherent in resolving uncertainty about the causes of ambiguous cues or outcomes. Here, we apply active inference to saccadic searches of a visual scene. We consider the (difficult) problem of categorizing a scene, based on the spatial relationship among visual objects where, crucially, visual cues are sampled myopically through a sequence of saccadic eye movements. This means that evidence for competing hypotheses about the scene has to be accumulated sequentially, calling upon both prediction (planning) and postdiction (memory). Our aim is to highlight some simple but fundamental aspects of the requisite functional anatomy; namely, the link between approximate Bayesian inference under mean field assumptions and functional segregation in the visual cortex. This link rests upon the (neurobiologically plausible) process theory that accompanies the normative formulation of active inference for Markov decision processes. In future work, we hope to use this scheme to model empirical saccadic searches and identify the prior beliefs that underwrite intersubject variability in the way people forage for information in visual scenes (e.g., in schizophrenia).},
  language = {en},
  journal = {Frontiers in Computational Neuroscience},
  doi = {10.3389/fncom.2016.00056},
  author = {Mirza, M. Berk and Adams, Rick A. and Mathys, Christoph D. and Friston, Karl J.},
  month = jun,
  year = {2016},
  file = {/Users/lancelotdacosta/Zotero/storage/83IBVBWQ/Mirza et al. - 2016 - Scene Construction, Visual Foraging, and Active In.pdf;/Users/lancelotdacosta/Zotero/storage/X9GQYYY2/Mirza et al. - 2016 - Scene Construction, Visual Foraging, and Active In.pdf;/Users/lancelotdacosta/Zotero/storage/3DBJBJEV/27378899.html}
}

@article{parrActiveInferenceAnatomy2018,
  title = {Active Inference and the Anatomy of Oculomotion},
  volume = {111},
  issn = {0028-3932},
  abstract = {Given that eye movement control can be framed as an inferential process, how are the requisite forces generated to produce anticipated or desired fixation? Starting from a generative model based on simple Newtonian equations of motion, we derive a variational solution to this problem and illustrate the plausibility of its implementation in the oculomotor brainstem. We show, through simulation, that the Bayesian filtering equations that implement `planning as inference' can generate both saccadic and smooth pursuit eye movements. Crucially, the associated message passing maps well onto the known connectivity and neuroanatomy of the brainstem \textendash{} and the changes in these messages over time are strikingly similar to single unit recordings of neurons in the corresponding nuclei. Furthermore, we show that simulated lesions to axonal pathways reproduce eye movement patterns of neurological patients with damage to these tracts.},
  journal = {Neuropsychologia},
  doi = {10.1016/j.neuropsychologia.2018.01.041},
  author = {Parr, Thomas and Friston, Karl J.},
  month = mar,
  year = {2018},
  keywords = {Active inference,Free energy,Brainstem,Oculomotor,Predictive coding,Saccades},
  pages = {334-343},
  file = {/Users/lancelotdacosta/Zotero/storage/9BB4VTX5/Parr and Friston - 2018 - Active inference and the anatomy of oculomotion.pdf;/Users/lancelotdacosta/Zotero/storage/3DKGU9CT/S0028393218300472.html}
}

@article{constantVariationalApproachNiche2018,
  title = {A Variational Approach to Niche Construction},
  volume = {15},
  abstract = {In evolutionary biology, niche construction is sometimes described as a genuine evolutionary process whereby organisms, through their activities and regulatory mechanisms, modify their environment such as to steer their own evolutionary trajectory, and that of other species. There is ongoing debate, however, on the extent to which niche construction ought to be considered a bona fide evolutionary force, on a par with natural selection. Recent formulations of the variational free-energy principle as applied to the life sciences describe the properties of living systems, and their selection in evolution, in terms of variational inference. We argue that niche construction can be described using a variational approach. We propose new arguments to support the niche construction perspective, and to extend the variational approach to niche construction to current perspectives in various scientific fields.},
  number = {141},
  journal = {Journal of The Royal Society Interface},
  doi = {10.1098/rsif.2017.0685},
  author = {Constant, A and Ramstead, M and Veissi{\`e}re, Samuel P. L. and Campbell, J O and Friston, K.J.},
  month = apr,
  year = {2018},
  pages = {20170685},
  file = {/Users/lancelotdacosta/Zotero/storage/VXHSLKBP/Constant Axel et al. - 2018 - A variational approach to niche construction.pdf;/Users/lancelotdacosta/Zotero/storage/H8QT6N3V/rsif.2017.html}
}

@article{zhangDiscreteFixedResolutionRepresentations2008,
  title = {Discrete {{Fixed}}-{{Resolution Representations}} in {{Visual Working Memory}}},
  volume = {453},
  issn = {0028-0836},
  abstract = {Limits on the storage capacity of working memory significantly impact cognitive abilities in a wide range of domains, but the nature of these capacity limits has been elusive. Some researchers have proposed that working memory stores a limited set of discrete, fixed-resolution representations, whereas others have proposed that working memory consists of a pool of resources that can be allocated flexibly to provide either a small number of high-resolution representations or a large number of low-resolution representations. Here we resolve this controversy by providing independent measures of capacity and resolution. We show that, when presented with more than a few simple objects, observers store a high-resolution representation of a subset of the objects and retain no information about the others. Memory resolution varied over a narrow range that cannot be explained in terms of a general resource pool but can be well explained by a small set of discrete, fixed-resolution representations.},
  number = {7192},
  journal = {Nature},
  doi = {10.1038/nature06860},
  author = {Zhang, Weiwei and Luck, Steven J.},
  month = may,
  year = {2008},
  pages = {233-235},
  file = {/Users/lancelotdacosta/Zotero/storage/WFVBWVMI/Zhang and Luck - 2008 - Discrete Fixed-Resolution Representations in Visua.pdf},
  pmid = {18385672},
  pmcid = {PMC2588137}
}

@article{wuGeneralizationGuidesHuman2018,
  title = {Generalization Guides Human Exploration in Vast Decision Spaces},
  volume = {2},
  copyright = {2018 The Author(s), under exclusive licence to Springer Nature Limited},
  issn = {2397-3374},
  abstract = {When searching for rewards in complex, unfamiliar environments, it is often impossible to explore all options. Wu et al. show how a combination of generalization and optimistic sampling guides efficient human exploration in complex environments.},
  language = {en},
  number = {12},
  journal = {Nature Human Behaviour},
  doi = {10.1038/s41562-018-0467-4},
  author = {Wu, Charley M. and Schulz, Eric and Speekenbrink, Maarten and Nelson, Jonathan D. and Meder, Bj{\"o}rn},
  month = dec,
  year = {2018},
  pages = {915-924},
  file = {/Users/lancelotdacosta/Zotero/storage/C679EW2P/Wu et al. - 2018 - Generalization guides human exploration in vast de.pdf;/Users/lancelotdacosta/Zotero/storage/6GPBPQPB/s41562-018-0467-4.html}
}

@article{dawModelBasedInfluencesHumans2011,
  title = {Model-{{Based Influences}} on {{Humans}}' {{Choices}} and {{Striatal Prediction Errors}}},
  volume = {69},
  issn = {0896-6273},
  abstract = {Summary
The mesostriatal dopamine system is prominently implicated in model-free reinforcement learning, with fMRI BOLD signals in ventral striatum notably covarying with model-free prediction errors. However, latent learning and devaluation studies show that behavior also shows hallmarks of model-based planning, and the interaction between model-based and model-free values, prediction errors, and preferences is underexplored. We designed a multistep decision task in which model-based and model-free influences on human choice behavior could be distinguished. By showing that choices reflected both influences we could then test the purity of the ventral striatal BOLD signal as a model-free report. Contrary to expectations, the signal reflected both model-free and model-based predictions in proportions matching those that best explained choice behavior. These results challenge the notion of a separate model-free learner and suggest a more integrated computational architecture for high-level human decision-making.},
  number = {6},
  journal = {Neuron},
  doi = {10.1016/j.neuron.2011.02.027},
  author = {Daw, Nathaniel D. and Gershman, Samuel J. and Seymour, Ben and Dayan, Peter and Dolan, Raymond J.},
  month = mar,
  year = {2011},
  pages = {1204-1215},
  file = {/Users/lancelotdacosta/Zotero/storage/LXJY4M4V/Daw et al. - 2011 - Model-Based Influences on Humans' Choices and Stri.pdf;/Users/lancelotdacosta/Zotero/storage/4IDLCAE2/S0896627311001255.html}
}

@article{parrComputationalPharmacologyOculomotion2019,
  title = {The Computational Pharmacology of Oculomotion},
  issn = {1432-2072},
  abstract = {Many physiological and pathological changes in brain function manifest in eye-movement control. As such, assessment of oculomotion is an invaluable part of a clinical examination and affords a non-invasive window on several key aspects of neuronal computation. While oculomotion is often used to detect deficits of the sort associated with vascular or neoplastic events; subtler (e.g. pharmacological) effects on neuronal processing also induce oculomotor changes. We have previously framed oculomotor control as part of active vision, namely, a process of inference comprising two distinct but related challenges. The first is inferring where to look, and the second is inferring how to implement the selected action. In this paper, we draw from recent theoretical work on the neuromodulatory control of active inference. This allows us to simulate the sort of changes we would expect in oculomotor behaviour, following pharmacological enhancement or suppression of key neuromodulators\textemdash{}in terms of deciding where to look and the ensuing trajectory of the eye movement itself. We focus upon the influence of cholinergic and GABAergic agents on the speed of saccades, and consider dopaminergic and noradrenergic effects on more complex, memory-guided, behaviour. In principle, a computational approach to understanding the relationship between pharmacology and oculomotor behaviour affords the opportunity to estimate the influence of a given pharmaceutical upon neuronal function, and to use this to optimise therapeutic interventions on an individual basis.},
  language = {en},
  journal = {Psychopharmacology},
  doi = {10.1007/s00213-019-05240-0},
  author = {Parr, Thomas and Friston, Karl J.},
  month = apr,
  year = {2019},
  keywords = {Active inference,Bayesian,Computational pharmacology,Neuromodulation,Oculomotion},
  file = {/Users/lancelotdacosta/Zotero/storage/A8A634UL/Parr and Friston - 2019 - The computational pharmacology of oculomotion.pdf}
}

@unpublished{xitongUnderstandingVariationalLower2017,
  address = {{Institute for Advanced Computer Studies. University of Maryland.}},
  title = {Understanding the {{Variational Lower Bound}}},
  author = {Xitong, Yang},
  year = {2017}
}

@article{knillBayesianBrainRole2004,
  title = {The {{Bayesian}} Brain: The Role of Uncertainty in Neural Coding and Computation},
  volume = {27},
  issn = {01662236},
  shorttitle = {The {{Bayesian}} Brain},
  language = {en},
  number = {12},
  journal = {Trends in Neurosciences},
  doi = {10.1016/j.tins.2004.10.007},
  author = {Knill, David C. and Pouget, Alexandre},
  month = dec,
  year = {2004},
  pages = {712-719},
  file = {/Users/lancelotdacosta/Zotero/storage/XUEWW6BW/Knill and Pouget - 2004 - The Bayesian brain the role of uncertainty in neu.pdf}
}

@techreport{smithGreaterDecisionUncertainty2020,
  title = {Greater Decision Uncertainty Characterizes a Transdiagnostic Patient Sample during Approach-Avoidance Conflict: A Computational Modeling Approach},
  shorttitle = {Greater Decision Uncertainty Characterizes a Transdiagnostic Patient Sample during Approach-Avoidance Conflict},
  author = {Smith, Ryan and Kirlic, Namik and Stewart, Jennifer L. and Touthang, James and Kuplicki, Rayus and Khalsa, Sahib S. and Paulus, Martin P and Investigators, T and Aupperle, Robin},
  year = {2020},
  month = apr,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/t2dhn},
  abstract = {Background: Imbalances in approach-avoidance conflict (AAC) decision-making (e.g. sacrificing rewards to avoid negative outcomes) are considered central to multiple psychiatric disorders. We used computational modeling to examine two factors often not distinguished within model-free analyses of AAC: decision uncertainty (DU) and sensitivity to negative outcomes vs. reward (emotional conflict; EC).Methods: A previously-validated AAC task was completed by 477 participants, including healthy controls (HCs; N=59), individuals with substance use disorders (SUDs; N=159) and individuals with depression and/or anxiety (DEP/ANX; N=260) disorders without SUDs. Using an active inference model, we estimated individual-level values for a model parameter ({$\beta$}) reflecting DU as well as another reflecting EC. Analyses were also repeated in a subsample propensity matched on age and general intelligence.Results: The model showed high accuracy (73\%). As further validation, parameters correlated with reaction times and self-reported task motivations in expected directions. EC further correlated with self-reported anxiety during the task (r=0.32, p\&lt;0.001), while DU correlated with self-reported difficulty making decisions (r=0.45, p\&lt;0.001). Compared to HCs, both DEP/ANX and SUDs showed higher DU in the propensity matched sample (t=2.16, p = .03; and t=2.88, p = .005, respectively), with analogous results in the full sample; SUDs also showed lower EC in the full sample (t=3.17, p=0.002). Limitations: This study is limited by clinical sample heterogeneity and an inability to examine learning.Conclusions: These results suggest that reduced confidence in how to act, rather than increased emotional conflict, may explain maladaptive approach-avoidance behaviors in psychiatric disorders.},
  type = {Preprint}
}



@techreport{smithImpreciseActionSelection2020,
  title = {Imprecise {{Action Selection}} in {{Substance Use Disorder}}: {{Evidence}} for {{Active Learning Impairments When Solving}} the {{Explore}}-{{Exploit Dilemma}}},
  shorttitle = {Imprecise {{Action Selection}} in {{Substance Use Disorder}}},
  author = {Smith, Ryan and Schwartenbeck, Philipp and Stewart, Jennifer L. and Kuplicki, Rayus and Ekhtiari, Hamed and Investigators, T and Paulus, Martin P},
  year = {2020},
  month = apr,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/a794k},
  abstract = {Background: Substance use disorders (SUDs) are a major public health risk. While there is a growing literature attempting to characterize aberrant decision processes in SUDs, mechanisms accounting for continued patterns of poor choices in the face of negative life consequences remain poorly understood. Methods: In this paper, we use a computational (active inference) modeling approach to examine how treatment-seeking individuals with one or more SUDs (alcohol, cannabis, sedatives, stimulants, hallucinogens, and/or opioids; N = 147) and healthy controls (HCs; N = 54) make choices to resolve uncertainty within a three-armed bandit gambling task. A subset of SUDs (n = 49) and HCs (n = 51) propensity-matched on age, sex, and verbal IQ were also compared to replicate larger group findings. As SUDs endorsed higher comorbid depression and anxiety symptoms than HCs, correlations between these symptoms and computational parameters were also explored within SUDs. Findings: Results indicate that: (a) SUDs show poorer task performance than HCs, with model estimates revealing less precise action selection mechanisms, a lower learning rate from losses, and a greater learning rate from gains; (b) greater severity in comorbid depression and anxiety symptoms within SUDs is associated with parameters that promote greater information-seeking; and (c) groups do not differ significantly in goal-directed information seeking. Conclusions: Findings suggest a pattern of inconsistent behavior in response to positive outcomes in SUDs combined with a tendency to attribute negative outcomes to chance. Specifically, individuals with SUDs fail to settle on a behavior strategy despite sufficient evidence of its success. These learning impairments could help account for difficulties in adjusting behavior and maintaining optimal decision making during and after treatment.},
  type = {Preprint}
}



@unpublished{dacostaRelationshipDynamicProgramming2020,
  title = {The Relationship between Dynamic Programming and Active Inference: The Discrete, Finite Horizon Case},
  author = {Da Costa, Lancelot and Sajid, Noor and Parr, Thomas and Friston, Karl and Smith, Ryan},
  year = {2020}
}




@incollection{pearlGraphicalModelsProbabilistic1998,
  address = {{Dordrecht}},
  series = {Handbook of {{Defeasible Reasoning}} and {{Uncertainty Management Systems}}},
  title = {Graphical {{Models}} for {{Probabilistic}} and {{Causal Reasoning}}},
  isbn = {978-94-017-1735-9},
  abstract = {This chapter surveys the development of graphical models known as Bayesian networks, summarizes their semantical basis and assesses their properties and applications to reasoning and planning.},
  language = {en},
  booktitle = {Quantified {{Representation}} of {{Uncertainty}} and {{Imprecision}}},
  publisher = {{Springer Netherlands}},
  doi = {10.1007/978-94-017-1735-9_12},
  author = {Pearl, Judea},
  editor = {Smets, Philippe},
  year = {1998},
  keywords = {Bayesian Network,Causal Effect,Causal Model,Causal Theory,Graphical Model},
  pages = {367-389},
  file = {/Users/lancelotdacosta/Zotero/storage/WBDTQ54Z/Pearl - 1998 - Graphical Models for Probabilistic and Causal Reas.pdf}
}

@unpublished{allenbyHierarchicalBayesModels2005,
  address = {{Journal of Bayesian Applications in Marketing}},
  title = {Hierarchical {{Bayes Models}}: {{A Practitioners Guide}}},
  author = {Allenby, Greg M and Rossi, Peter E and McCulloch, Robert E},
  year = {2005}
}

@article{boxMultiparameterProblemsBayesian1965,
  title = {Multiparameter Problems from a {{Bayesian}} Point of View},
  journal = {The Annals of Mathematical Statistics},
  author = {Box, G E P and Tiao, George C},
  year = {1965}
}

@book{bartoReinforcementLearningIntroduction1992,
  title = {Reinforcement {{Learning}}: {{An Introduction}}},
  author = {Barto, Andrew and Sutton, Richard},
  year = {1992}
}

@book{stoneArtificialIntelligenceEngines2019,
  title = {Artificial {{Intelligence Engines}}: {{A Tutorial Introduction}} to the {{Mathematics}} of {{Deep Learning}}},
  author = {Stone, James V},
  year = {2019}
}

@article{astromOptimalControlMarkov1965,
  title = {Optimal {{Control}} of {{Markov Processes}} with {{Incomplete State Information}}},
  volume = {10},
  journal = {Journal of Mathematical Analysis and Applications},
  author = {Astr{\"o}m, K J},
  year = {1965}
}

@incollection{whiteMarkovDecisionProcesses2001,
  address = {{Boston, MA}},
  title = {Markov Decision Processes},
  isbn = {978-1-4020-0611-1},
  language = {en},
  booktitle = {Encyclopedia of {{Operations Research}} and {{Management Science}}},
  publisher = {{Springer US}},
  doi = {10.1007/1-4020-0611-X_580},
  author = {White, C. C.},
  year = {2001},
  pages = {484-486}
}

@book{pearlProbabilisticReasoningIntelligent1988,
  title = {Probabilistic {{Reasoning}} in {{Intelligent Systems}}},
  isbn = {978-1-55860-479-7},
  author = {Pearl, Judea},
  year = {1988}
}

@book{pavliotisStochasticProcessesApplications2014,
  address = {{New York}},
  series = {Texts in Applied Mathematics},
  title = {Stochastic Processes and Applications: Diffusion Processes, the {{Fokker}}-{{Planck}} and {{Langevin}} Equations},
  isbn = {978-1-4939-1322-0},
  lccn = {QA274 .P385 2014},
  shorttitle = {Stochastic Processes and Applications},
  abstract = {This book presents various results and techniques from the theory of stochastic processes that are useful in the study of stochastic problems in the natural sciences. The main focus is analytical methods, although numerical methods and statistical inference methodologies for studying diffusion processes are also presented. The goal is the development of techniques that are applicable to a wide variety of stochastic models that appear in physics, chemistry and other natural sciences. Applications such as stochastic resonance, Brownian motion in periodic potentials and Brownian motors are studied and the connection between diffusion processes and time-dependent statistical mechanics is elucidated. The book contains a large number of illustrations, examples, and exercises. It will be useful for graduate-level courses on stochastic processes for students in applied mathematics, physics and engineering. Many of the topics covered in this book (reversible diffusions, convergence to equilibrium for diffusion processes, inference methods for stochastic differential equations, derivation of the generalized Langevin equation, exit time problems) cannot be easily found in textbook form and will be useful to both researchers and students interested in the applications of stochastic processes.--},
  language = {en},
  number = {volume 60},
  publisher = {{Springer}},
  author = {Pavliotis, Grigorios A.},
  year = {2014},
  keywords = {Statistik,Stochastic models,Stochastic processes,Stochastik,Textbooks},
  file = {/Users/lancelotdacosta/Zotero/storage/BN6M6Z95/Pavliotis - 2014 - Stochastic processes and applications diffusion p.pdf},
  note = {OCLC: ocn898121925}
}

@article{loeligerIntroductionFactorGraphs2004,
  title = {An Introduction to Factor Graphs},
  journal = {IEEE Signal Processing Magazine},
  author = {Loeliger, Hans-Andrea},
  year = {2004}
}

@article{teeInformationBrainRepresented2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.01631},
  primaryClass = {cs, math, q-bio},
  title = {Is {{Information}} in the {{Brain Represented}} in {{Continuous}} or {{Discrete Form}}?},
  abstract = {The question of continuous-versus-discrete information representation in the brain is a fundamental yet unresolved physiological question. Historically, most analyses assume a continuous representation without considering the alternative possibility of a discrete representation. Our work explores the plausibility of both representations, and answers the question from a communications engineering perspective. Drawing on the well-established Shannon's communications theory, we posit that information in the brain is represented in a discrete form. Using a computer simulation, we show that information cannot be communicated reliably between neurons using a continuous representation, due to the presence of noise; neural information has to be in a discrete form. In addition, we designed 3 (human) behavioral experiments on probability estimation and analyzed the data using a novel discrete (quantized) model of probability. Under a discrete model of probability, two distinct probabilities (say, 0.57 and 0.58) are treated indifferently. We found that data from all participants were better fit to discrete models than continuous ones. Furthermore, we re-analyzed the data from a published (human) behavioral study on intertemporal choice using a novel discrete (quantized) model of intertemporal choice. Under such a model, two distinct time delays (say, 16 days and 17 days) are treated indifferently. We found corroborating results, showing that data from all participants were better fit to discrete models than continuous ones. In summary, all results reported here support our discrete hypothesis of information representation in the brain, which signifies a major demarcation from the current understanding of the brain's physiology.},
  journal = {arXiv:1805.01631 [cs, math, q-bio]},
  author = {Tee, James and Taylor, Desmond P.},
  month = may,
  year = {2018},
  keywords = {Computer Science - Information Theory,Quantitative Biology - Neurons and Cognition},
  file = {/Users/lancelotdacosta/Zotero/storage/2PU928YR/Tee and Taylor - 2018 - Is Information in the Brain Represented in Continu.pdf;/Users/lancelotdacosta/Zotero/storage/E2SIDI9E/1805.html}
}

@article{luckCapacityVisualWorking1997,
  title = {The Capacity of Visual Working Memory for Features and Conjunctions},
  volume = {390},
  issn = {0028-0836},
  abstract = {Short-term memory storage can be divided into separate subsystems for verbal information and visual information, and recent studies have begun to delineate the neural substrates of these working-memory systems. Although the verbal storage system has been well characterized, the storage capacity of visual working memory has not yet been established for simple, suprathreshold features or for conjunctions of features. Here we demonstrate that it is possible to retain information about only four colours or orientations in visual working memory at one time. However, it is also possible to retain both the colour and the orientation of four objects, indicating that visual working memory stores integrated objects rather than individual features. Indeed, objects defined by a conjunction of four features can be retained in working memory just as well as single-feature objects, allowing sixteen individual features to be retained when distributed across four objects. Thus, the capacity of visual working memory must be understood in terms of integrated objects rather than individual features, which places significant constraints on cognitive and neurobiological models of the temporary storage of visual information.},
  language = {eng},
  number = {6657},
  journal = {Nature},
  doi = {10.1038/36846},
  author = {Luck, S. J. and Vogel, E. K.},
  month = nov,
  year = {1997},
  keywords = {Humans,Visual Perception,Color Perception,Memory; Short-Term},
  pages = {279-281},
  pmid = {9384378}
}

@article{buschmanShiftingSpotlightAttention2010,
  title = {Shifting the {{Spotlight}} of {{Attention}}: {{Evidence}} for {{Discrete Computations}} in {{Cognition}}},
  volume = {4},
  issn = {1662-5161},
  shorttitle = {Shifting the {{Spotlight}} of {{Attention}}},
  abstract = {Our thoughts have a limited bandwidth; we can only fully process a few items in mind simultaneously. To compensate, the brain developed attention, the ability to select information relevant to the current task, while filtering out the rest. Therefore, by understanding the neural mechanisms of attention we hope to understand a core component of cognition. Here, we review our recent investigations of the neural mechanisms underlying the control of visual attention in frontal and parietal cortex. This includes the observation that the neural mechanisms that shift attention were synchronized to 25 Hz oscillatory brain rhythms, with each shift in attention falling within a single cycle of the oscillation. We generalize these findings to present a hypothesis that cognition relies on neural mechanisms that operate in discrete, periodic computations, as reflected in ongoing oscillations. We discuss the advantages of the model, experimental support, and make several testable hypotheses.},
  language = {English},
  journal = {Frontiers in Human Neuroscience},
  doi = {10.3389/fnhum.2010.00194},
  author = {Buschman, Timothy J. and Miller, Earl K.},
  year = {2010},
  keywords = {Attention,Cognition,oscillations,synchrony},
  file = {/Users/lancelotdacosta/Zotero/storage/9EZ8A6C4/Buschman and Miller - 2010 - Shifting the Spotlight of Attention Evidence for .pdf}
}

@article{duncanDirectMeasurementAttentional1994,
  title = {Direct Measurement of Attentional Dwell Time in Human Vision},
  volume = {369},
  issn = {0028-0836},
  abstract = {In vision, attentional limitations are reflected in interference or reduced accuracy when two objects must be identified at once in a brief display. In our experiments a brief temporal separation was introduced between the two objects to be identified. We measured how long the object continued to interfere with the second, and hence the time course of the first object's attentional demand. According to conventional serial models, attention is assigned rapidly to one object after another, with a dwell time of only a few dozen milliseconds per item. But we report here that interference lasts for several hundred milliseconds--an order of magnitude more than the prediction of conventional models. We suggest that visual attention is not a high-speed switching mechanism, but a sustained state during which relevant objects become available to influence behaviour. This conclusion is consistent with recent physiological results in the monkey.},
  language = {eng},
  number = {6478},
  journal = {Nature},
  doi = {10.1038/369313a0},
  author = {Duncan, J. and Ward, R. and Shapiro, K.},
  month = may,
  year = {1994},
  keywords = {Attention,Humans,Visual Perception,Adult,Cues,Task Performance and Analysis,Time Factors},
  pages = {313-315},
  pmid = {8183369}
}

@article{hanslmayrPrestimulusOscillatoryPhase2013,
  title = {Prestimulus {{Oscillatory Phase}} at 7 {{Hz Gates Cortical Information Flow}} and {{Visual Perception}}},
  volume = {23},
  issn = {0960-9822},
  abstract = {{$<$}h2{$>$}Summary{$<$}/h2{$><$}p{$>$}Although we have the impression that visual information flows continuously from our sensory channels, recent studies indicate that this is likely not the case. Rather, we sample visual stimuli rhythmically, oscillating at 5\textendash{}10 Hz [1\textendash{}3]. Electroencephalography (EEG) studies have demonstrated that this rhythmicity is reflected by the phase of ongoing brain oscillations in the same frequency [4\textendash{}6]. Theoretically, brain oscillations could underlie the rhythmic nature of perception by providing transient time windows for information exchange [7], but this question has not yet been systematically addressed. We recorded simultaneous EEG-fMRI while human participants performed a contour integration task and show that ongoing brain oscillations prior to stimulus onset predict functional connectivity between higher and lower level visual processing regions. Specifically, our results demonstrate that the phase of a 7 Hz oscillation prior to stimulus onset predicts perceptual performance and the bidirectional information flow between the left lateral occipital cortex and right intraparietal sulcus, as indicated by psychophysiological interaction and dynamic causal modeling. These findings suggest that human brain oscillations periodically gate visual perception at around 7 Hz by providing transient time windows for long-distance cortical information transfer. Such gating might be a general mechanism underlying the rhythmic nature of human perception.{$<$}/p{$>$}},
  language = {English},
  number = {22},
  journal = {Current Biology},
  doi = {10.1016/j.cub.2013.09.020},
  author = {Hanslmayr, Simon and Volberg, Gregor and Wimber, Maria and Dalal, Sarang S. and Greenlee, Mark W.},
  month = nov,
  year = {2013},
  pages = {2273-2278},
  file = {/Users/lancelotdacosta/Zotero/storage/8Z4DKQCI/Hanslmayr et al. - 2013 - Prestimulus Oscillatory Phase at 7 Hz Gates Cortic.pdf;/Users/lancelotdacosta/Zotero/storage/K95GSU85/S0960-9822(13)01136-6.html},
  pmid = {24184106}
}

@article{landauAttentionSamplesStimuli2012,
  title = {Attention {{Samples Stimuli Rhythmically}}},
  volume = {22},
  issn = {0960-9822},
  abstract = {Summary
Overt exploration or sampling behaviors, such as whisking, sniffing, and saccadic eye movements [1, 2], are often characterized by a rhythm. In addition, the electrophysiologically recorded theta or alpha phase predicts global detection performance [3, 4]. These two observations raise the intriguing possibility that covert selective attention samples from multiple stimuli rhythmically. To investigate this possibility, we measured change detection performance on two simultaneously presented stimuli, after resetting attention to one of them. After a reset flash at one stimulus location, detection performance fluctuated rhythmically. When the flash was presented in the right visual field, a 4~Hz rhythm was directly visible in the time courses of behavioral performance at both stimulus locations, and the two rhythms were in antiphase. A left visual field flash exerted only partial reset on performance and induced rhythmic fluctuation at higher frequencies (6\textendash{}10~Hz). These findings show that selective attention samples multiple stimuli rhythmically, and they position spatial attention within the family of exploration behaviors [1].},
  number = {11},
  journal = {Current Biology},
  doi = {10.1016/j.cub.2012.03.054},
  author = {Landau, Ayelet Nina and Fries, Pascal},
  month = jun,
  year = {2012},
  pages = {1000-1004},
  file = {/Users/lancelotdacosta/Zotero/storage/H9D84X57/Landau and Fries - 2012 - Attention Samples Stimuli Rhythmically.pdf;/Users/lancelotdacosta/Zotero/storage/ACGM7Z5S/S0960982212003417.html}
}

@article{buesingNeuralDynamicsSampling2011,
  title = {Neural {{Dynamics}} as {{Sampling}}: {{A Model}} for {{Stochastic Computation}} in {{Recurrent Networks}} of {{Spiking Neurons}}},
  volume = {7},
  issn = {1553-7358},
  shorttitle = {Neural {{Dynamics}} as {{Sampling}}},
  abstract = {The organization of computations in networks of spiking neurons in the brain is still largely unknown, in particular in view of the inherently stochastic features of their firing activity and the experimentally observed trial-to-trial variability of neural systems in the brain. In principle there exists a powerful computational framework for stochastic computations, probabilistic inference by sampling, which can explain a large number of macroscopic experimental data in neuroscience and cognitive science. But it has turned out to be surprisingly difficult to create a link between these abstract models for stochastic computations and more detailed models of the dynamics of networks of spiking neurons. Here we create such a link and show that under some conditions the stochastic firing activity of networks of spiking neurons can be interpreted as probabilistic inference via Markov chain Monte Carlo (MCMC) sampling. Since common methods for MCMC sampling in distributed systems, such as Gibbs sampling, are inconsistent with the dynamics of spiking neurons, we introduce a different approach based on non-reversible Markov chains that is able to reflect inherent temporal processes of spiking neuronal activity through a suitable choice of random variables. We propose a neural network model and show by a rigorous theoretical analysis that its neural activity implements MCMC sampling of a given distribution, both for the case of discrete and continuous time. This provides a step towards closing the gap between abstract functional models of cortical computation and more detailed models of networks of spiking neurons.},
  language = {en},
  number = {11},
  journal = {PLOS Computational Biology},
  doi = {10.1371/journal.pcbi.1002211},
  author = {Buesing, Lars and Bill, Johannes and Nessler, Bernhard and Maass, Wolfgang},
  year = {03-Nov-2011},
  keywords = {Action potentials,Markov models,Membrane potential,Network analysis,Neural networks,Neuronal tuning,Neurons,Probability distribution},
  pages = {e1002211},
  file = {/Users/lancelotdacosta/Zotero/storage/D4RDY24A/Buesing et al. - 2011 - Neural Dynamics as Sampling A Model for Stochasti.pdf}
}

@article{jarzynskiNonequilibriumEqualityFree1997,
  title = {Nonequilibrium {{Equality}} for {{Free Energy Differences}}},
  volume = {78},
  abstract = {An expression is derived for the equilibrium free energy difference between two configurations of a system, in terms of an ensemble of finite-time measurements of the work performed in parametrically switching from one configuration to the other. Two well-known identities emerge as limiting cases of this result., This article appears in the following collection:},
  number = {14},
  journal = {Physical Review Letters},
  doi = {10.1103/PhysRevLett.78.2690},
  author = {Jarzynski, C.},
  month = apr,
  year = {1997},
  pages = {2690-2693},
  file = {/Users/lancelotdacosta/Zotero/storage/DGEVYPI2/Jarzynski - 1997 - Nonequilibrium Equality for Free Energy Difference.pdf;/Users/lancelotdacosta/Zotero/storage/T7RJ7QTH/PhysRevLett.78.html}
}

@inproceedings{bennettNotesLandauerPrinciple2003,
  title = {Notes on {{Landauer}}'s Principle, Reversible Computation, and {{Maxwell}}'s {{Demon}}},
  shorttitle = {{{CH}}},
  abstract = {Landauer's principle, often regarded as the basic principle of the thermodynamics of information processing, holds that any logically irreversible manipulation of information, such as the erasure of a bit or the merging of two computation paths, must be accompanied by a corresponding entropy increase in non-information-bearing degrees of freedom of the information-processing apparatus or its environment. Conversely, it is generally accepted that any logically reversible transformation of information can in principle be accomplished by an appropriate physical mechanism operating in a thermodynamically reversible fashion. These notions have sometimes been criticized either as being false, or as being trivial and obvious, and therefore unhelpful for purposes such as explaining why Maxwell's Demon cannot violate the second law of thermodynamics. Here I attempt to refute some of the arguments against Landauer's principle, while arguing that although in a sense it is indeed a straightforward consequence or restatement of the Second Law, it still has considerable pedagogic and explanatory power, especially in the context of other influential ideas in nineteenth and twentieth century physics. Similar arguments have been given by Jeffrey Bub (2002).},
  booktitle = {Studies in {{History}} and {{Philosophy}} of {{Modern Physics}} 2003},
  author = {Bennett, Charles H.},
  year = {2003},
  file = {/Users/lancelotdacosta/Zotero/storage/JBMN6UBK/Bennett - CH Notes on Landauer's principle, reversible comp.pdf;/Users/lancelotdacosta/Zotero/storage/8BAKCW39/summary.html}
}

@article{landauerIrreversibilityHeatGeneration1961,
  title = {Irreversibility and {{Heat Generation}} in the {{Computing Process}}},
  volume = {5},
  issn = {0018-8646},
  abstract = {It is argued that computing machines inevitably involve devices which perform logical functions that do not have a single-valued inverse. This logical irreversibility is associated with physical irreversibility and requires a minimal heat generation, per machine cycle, typically of the order of kT for each irreversible function. This dissipation serves the purpose of standardizing signals and making them independent of their exact logical history. Two simple, but representative, models of bistable devices are subjected to a more detailed analysis of switching kinetics to yield the relationship between speed and energy dissipation, and to estimate the effects of errors induced by thermal fluctuations.},
  number = {3},
  journal = {IBM Journal of Research and Development},
  doi = {10.1147/rd.53.0183},
  author = {Landauer, R.},
  month = jul,
  year = {1961},
  pages = {183-191},
  file = {/Users/lancelotdacosta/Zotero/storage/NML3IX8B/Landauer - 1961 - Irreversibility and Heat Generation in the Computi.pdf;/Users/lancelotdacosta/Zotero/storage/AG6SZCX4/5392446.html}
}

@phdthesis{dacostaActiveInferenceDiscrete2019,
  title = {Active Inference on Discrete State-Spaces: A Synthesis},
  school = {University College London},
  author = {Da Costa, Lancelot},
  year = {2019}
}


@article{mirzaHumanVisualExploration2018,
  title = {Human Visual Exploration Reduces Uncertainty about the Sensed World},
  author = {Mirza, M. Berk and Adams, Rick A. and Mathys, Christoph and Friston, Karl J.},
  year = {2018},
  volume = {13},
  pages = {e0190429},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0190429},
  abstract = {In previous papers, we introduced a normative scheme for scene construction and epistemic (visual) searches based upon active inference. This scheme provides a principled account of how people decide where to look, when categorising a visual scene based on its contents. In this paper, we use active inference to explain the visual searches of normal human subjects; enabling us to answer some key questions about visual foraging and salience attribution. First, we asked whether there is any evidence for `epistemic foraging'; i.e. exploration that resolves uncertainty about a scene. In brief, we used Bayesian model comparison to compare Markov decision process (MDP) models of scan-paths that did\textendash and did not\textendash contain the epistemic, uncertainty-resolving imperatives for action selection. In the course of this model comparison, we discovered that it was necessary to include non-epistemic (heuristic) policies to explain observed behaviour (e.g., a reading-like strategy that involved scanning from left to right). Despite this use of heuristic policies, model comparison showed that there is substantial evidence for epistemic foraging in the visual exploration of even simple scenes. Second, we compared MDP models that did\textendash and did not\textendash allow for changes in prior expectations over successive blocks of the visual search paradigm. We found that implicit prior beliefs about the speed and accuracy of visual searches changed systematically with experience. Finally, we characterised intersubject variability in terms of subject-specific prior beliefs. Specifically, we used canonical correlation analysis to see if there were any mixtures of prior expectations that could predict between-subject differences in performance; thereby establishing a quantitative link between different behavioural phenotypes and Bayesian belief updating. We demonstrated that better scene categorisation performance is consistently associated with lower reliance on heuristics; i.e., a greater use of a generative model of the scene to direct its exploration.},
  file = {/Users/lancelotdacosta/Zotero/storage/XFQDV98E/Mirza et al. - 2018 - Human visual exploration reduces uncertainty about.pdf;/Users/lancelotdacosta/Zotero/storage/5P8LFFJH/article.html},
  journal = {PLOS ONE},
  keywords = {Behavior,Birds,Cats,Eye movements,Foraging,Free energy,Schizophrenia,Vision},
  language = {en},
  number = {1}
}


@article{parrMarkovBlanketsInformation2020,
  ids = {parrMarkovBlanketsInformation2019},
  title = {Markov Blankets, Information Geometry and Stochastic Thermodynamics},
  author = {Parr, Thomas and Da Costa, Lancelot and Friston, Karl},
  year = {2020},
  month = feb,
  volume = {378},
  pages = {20190159},
  doi = {10.1098/rsta.2019.0159},
  abstract = {This paper considers the relationship between thermodynamics, information and inference. In particular, it explores the thermodynamic concomitants of belief updating, under a variational (free energy) principle for self-organization. In brief, any (weakly mixing) random dynamical system that possesses a Markov blanket\textemdash i.e. a separation of internal and external states\textemdash is equipped with an information geometry. This means that internal states parametrize a probability density over external states. Furthermore, at non-equilibrium steady-state, the flow of internal states can be construed as a gradient flow on a quantity known in statistics as Bayesian model evidence. In short, there is a natural Bayesian mechanics for any system that possesses a Markov blanket. Crucially, this means that there is an explicit link between the inference performed by internal states and their energetics\textemdash as characterized by their stochastic thermodynamics.This article is part of the theme issue `Harmonizing energy-autonomous computing and intelligence'.},
  file = {/Users/lancelotdacosta/Zotero/storage/VRCRVF7F/Parr et al. - 2020 - Markov blankets, information geometry and stochast.pdf;/Users/lancelotdacosta/Zotero/storage/RT7MNHLK/rsta.2019.html},
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  number = {2164}
}




@article{fristonSophisticatedInference2020,
  title = {Sophisticated {{Inference}}},
  author = {Friston, Karl and Da Costa, Lancelot and Hafner, Danijar and Hesp, Casper and Parr, Thomas},
  year = {2020},
  month = jun,
  abstract = {Active inference offers a first principle account of sentient behaviour, from which special and important cases can be derived, e.g., reinforcement learning, active learning, Bayes optimal inference, Bayes optimal design, etc. Active inference resolves the exploitation-exploration dilemma in relation to prior preferences, by placing information gain on the same footing as reward or value. In brief, active inference replaces value functions with functionals of (Bayesian) beliefs, in the form of an expected (variational) free energy. In this paper, we consider a sophisticated kind of active inference, using a recursive form of expected free energy. Sophistication describes the degree to which an agent has beliefs about beliefs. We consider agents with beliefs about the counterfactual consequences of action for states of affairs and beliefs about those latent states. In other words, we move from simply considering beliefs about 'what would happen if I did that' to 'what would I believe about what would happen if I did that'. The recursive form of the free energy functional effectively implements a deep tree search over actions and outcomes in the future. Crucially, this search is over sequences of belief states, as opposed to states per se. We illustrate the competence of this scheme, using numerical simulations of deep decision problems.},
  archivePrefix = {arXiv},
  eprint = {2006.04120},
  eprinttype = {arxiv},
  file = {/Users/lancelotdacosta/Zotero/storage/6J5M4CCK/Friston et al. - 2020 - Sophisticated Inference.pdf;/Users/lancelotdacosta/Zotero/storage/R93RGXNQ/2006.html},
  journal = {arXiv:2006.04120 [cs, q-bio]},
  keywords = {Computer Science - Artificial Intelligence,Quantitative Biology - Neurons and Cognition},
  primaryClass = {cs, q-bio}
}




@article{kullbackInformationSufficiency1951,
  title = {On {{Information}} and {{Sufficiency}}},
  volume = {22},
  issn = {0003-4851, 2168-8990},
  abstract = {Project Euclid - mathematics and statistics online},
  language = {EN},
  number = {1},
  journal = {The Annals of Mathematical Statistics},
  doi = {10.1214/aoms/1177729694},
  author = {Kullback, S. and Leibler, R. A.},
  month = mar,
  year = {1951},
  pages = {79-86},
  file = {/Users/lancelotdacosta/Zotero/storage/D8FYBB5V/Kullback and Leibler - 1951 - On Information and Sufficiency.pdf;/Users/lancelotdacosta/Zotero/storage/GIABSU28/1177729694.html},
  mrnumber = {MR39968},
  zmnumber = {0042.38403}
}

@article{lewickiEfficientCodingNatural2002,
  title = {Efficient Coding of Natural Sounds},
  volume = {5},
  copyright = {2002 Nature Publishing Group},
  issn = {1546-1726},
  abstract = {The auditory system encodes sound by decomposing the amplitude signal arriving at the ear into multiple frequency bands whose center frequencies and bandwidths are approximately exponential functions of the distance from the stapes. This organization is thought to result from the adaptation of cochlear mechanisms to the animal's auditory environment. Here we report that several basic auditory nerve fiber tuning properties can be accounted for by adapting a population of filter shapes to encode natural sounds efficiently. The form of the code depends on sound class, resembling a Fourier transformation when optimized for animal vocalizations and a wavelet transformation when optimized for non-biological environmental sounds. Only for the combined set does the optimal code follow scaling characteristics of physiological data. These results suggest that auditory nerve fibers encode a broad set of natural sounds in a manner consistent with information theoretic principles.},
  language = {en},
  number = {4},
  journal = {Nature Neuroscience},
  doi = {10.1038/nn831},
  author = {Lewicki, Michael S.},
  month = apr,
  year = {2002},
  pages = {356-363},
  file = {/Users/lancelotdacosta/Zotero/storage/DYCTV82B/Lewicki - 2002 - Efficient coding of natural sounds.pdf;/Users/lancelotdacosta/Zotero/storage/9FZKUN3V/nn831.html}
}

@article{olshausenSparseCodingSensory2004,
  title = {Sparse Coding of Sensory Inputs},
  volume = {14},
  issn = {0959-4388},
  abstract = {Several theoretical, computational, and experimental studies suggest that neurons encode sensory information using a small number of active neurons at any given point in time. This strategy, referred to as `sparse coding', could possibly confer several advantages. First, it allows for increased storage capacity in associative memories; second, it makes the structure in natural signals explicit; third, it represents complex data in a way that is easier to read out at subsequent levels of processing; and fourth, it saves energy. Recent physiological recordings from sensory neurons have indicated that sparse coding could be a ubiquitous strategy employed in several different modalities across different organisms.},
  number = {4},
  journal = {Current Opinion in Neurobiology},
  doi = {10.1016/j.conb.2004.07.007},
  author = {Olshausen, Bruno A and Field, David J},
  month = aug,
  year = {2004},
  pages = {481-487},
  file = {/Users/lancelotdacosta/Zotero/storage/B8GY4HDX/S0959438804001035.html}
}

@article{olshausenNewWindowSound2002,
  title = {A New Window on Sound},
  volume = {5},
  copyright = {2002 Nature Publishing Group},
  issn = {1546-1726},
  abstract = {Auditory filters must trade off frequency tuning against temporal precision. The compromise achieved by the mammalian cochlea seems well matched to the sounds of the natural environment.},
  language = {en},
  number = {4},
  journal = {Nature Neuroscience},
  doi = {10.1038/nn0402-292},
  author = {Olshausen, Bruno A. and O'Connor, Kevin N.},
  month = apr,
  year = {2002},
  pages = {292-294},
  file = {/Users/lancelotdacosta/Zotero/storage/SR9936TT/Olshausen and O'Connor - 2002 - A new window on sound.pdf;/Users/lancelotdacosta/Zotero/storage/CVC73JAL/nn0402-292.html}
}

@article{danEfficientCodingNatural1996,
  title = {Efficient {{Coding}} of {{Natural Scenes}} in the {{Lateral Geniculate Nucleus}}: {{Experimental Test}} of a {{Computational Theory}}},
  volume = {16},
  copyright = {Copyright \textcopyright{} 1996 Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  shorttitle = {Efficient {{Coding}} of {{Natural Scenes}} in the {{Lateral Geniculate Nucleus}}},
  abstract = {A recent computational theory suggests that visual processing in the retina and the lateral geniculate nucleus (LGN) serves to recode information into an efficient form (Atick and Redlich, 1990). Information theoretic analysis showed that the representation of visual information at the level of the photoreceptors is inefficient, primarily attributable to a high degree of spatial and temporal correlation in natural scenes. It was predicted, therefore, that the retina and the LGN should recode this signal into a decorrelated form or, equivalently, into a signal with a ``white'' spatial and temporal power spectrum. In the present study, we tested directly the prediction that visual processing at the level of the LGN temporally whitens the natural visual input. We recorded the responses of individual neurons in the LGN of the cat to natural, time-varying images (movies) and, as a control, to white-noise stimuli. Although there is substantial temporal correlation in natural inputs (Dong and Atick, 1995b), we found that the power spectra of LGN responses were essentially white. Between 3 and 15 Hz, the power of the responses had an average variation of only {$\pm$}10.3\%. Thus, the signals that the LGN relays to visual cortex are temporarily decorrelated. Furthermore, the responses of X-cells to natural inputs can be well predicted from their responses to white-noise inputs. We therefore conclude that whitening of natural inputs can be explained largely by the linear filtering properties (Enroth-Cugell and Robson, 1966). Our results suggest that the early visual pathway is well adapted for efficient coding of information in the natural visual environment, in agreement with the prediction of the computational theory.},
  language = {en},
  number = {10},
  journal = {Journal of Neuroscience},
  doi = {10.1523/JNEUROSCI.16-10-03351.1996},
  author = {Dan, Yang and Atick, Joseph J. and Reid, R. Clay},
  month = may,
  year = {1996},
  keywords = {coding,efficiency,information theory,lateral geniculate nucleus,natural scenes,power spectrum,visual cortex},
  pages = {3351-3362},
  file = {/Users/lancelotdacosta/Zotero/storage/UXIRQGQT/Dan et al. - 1996 - Efficient Coding of Natural Scenes in the Lateral .pdf;/Users/lancelotdacosta/Zotero/storage/NWLDQBWS/3351.html},
  pmid = {8627371}
}

@article{stephanBayesianModelSelection2009,
  title = {Bayesian Model Selection for Group Studies},
  volume = {46},
  issn = {1053-8119},
  abstract = {Bayesian model selection (BMS) is a powerful method for determining the most likely among a set of competing hypotheses about the mechanisms that generated observed data. BMS has recently found widespread application in neuroimaging, particularly in the context of dynamic causal modelling (DCM). However, so far, combining BMS results from several subjects has relied on simple (fixed effects) metrics, e.g. the group Bayes factor (GBF), that do not account for group heterogeneity or outliers. In this paper, we compare the GBF with two random effects methods for BMS at the between-subject or group level. These methods provide inference on model-space using a classical and Bayesian perspective respectively. First, a classical (frequentist) approach uses the log model evidence as a subject-specific summary statistic. This enables one to use analysis of variance to test for differences in log-evidences over models, relative to inter-subject differences. We then consider the same problem in Bayesian terms and describe a novel hierarchical model, which is optimised to furnish a probability density on the models themselves. This new variational Bayes method rests on treating the model as a random variable and estimating the parameters of a Dirichlet distribution which describes the probabilities for all models considered. These probabilities then define a multinomial distribution over model space, allowing one to compute how likely it is that a specific model generated the data of a randomly chosen subject as well as the exceedance probability of one model being more likely than any other model. Using empirical and synthetic data, we show that optimising a conditional density of the model probabilities, given the log-evidences for each model over subjects, is more informative and appropriate than both the GBF and frequentist tests of the log-evidences. In particular, we found that the hierarchical Bayesian approach is considerably more robust than either of the other approaches in the presence of outliers. We expect that this new random effects method will prove useful for a wide range of group studies, not only in the context of DCM, but also for other modelling endeavours, e.g. comparing different source reconstruction methods for EEG/MEG or selecting among competing computational models of learning and decision-making.},
  number = {4},
  journal = {NeuroImage},
  doi = {10.1016/j.neuroimage.2009.03.025},
  author = {Stephan, Klaas Enno and Penny, Will D. and Daunizeau, Jean and Moran, Rosalyn J. and Friston, Karl J.},
  month = jul,
  year = {2009},
  keywords = {Bayes factor,DCM,Dynamic causal modelling,EEG,fMRI,Hierarchical models,MEG,Model comparison,Model evidence,Random effects,Source reconstruction,Variational Bayes},
  pages = {1004-1017},
  file = {/Users/lancelotdacosta/Zotero/storage/MRL7KW5H/Stephan et al. - 2009 - Bayesian model selection for group studies.pdf;/Users/lancelotdacosta/Zotero/storage/5CNF4G3M/S1053811909002638.html}
}

@book{claeskensModelSelectionModel2006,
  title = {Model {{Selection}} and {{Model Averaging}}},
  abstract = {Given a data set, you can fit thousands of models at the push of a button, but how do you choose the best? With so many candidate models, overfitting is a real danger. Is the monkey who typed Hamlet actually a good writer? Choosing a model is central to all statistical work with data. We have seen rapid advances in model fitting and in the theoretical understanding of model selection, yet this book is the first to synthesize research and practice from this active field. Model choice criteria are explained, discussed and compared, including the AIC, BIC, DIC and FIC. The uncertainties involved with model selection are tackled, with discussions of frequentist and Bayesian methods; model averaging schemes are presented. Real-data examples are complemented by derivations providing deeper insight into the methodology, and instructive exercises build familiarity with the methods. The companion website features Data sets and R code.},
  publisher = {{Cambridge University Press}},
  author = {Claeskens, Gerda and Hjort, Nils Lid},
  year = {2006},
  file = {/Users/lancelotdacosta/Zotero/storage/KL94G9N9/9780521852258.html}
}

@article{smithActiveInferenceModel2019,
  title = {An Active Inference Model of Concept Learning},
  copyright = {\textcopyright{} 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}The algorithmic and neural basis of concept learning remains poorly understood. In this paper, we articulate a novel, biologically plausible approach to concept learning based on active inference, and on the idea that a generative model can be equipped with extra (hidden state or cause) `slots' that can be engaged when an agent learns about novel concepts. This can be combined with a Bayesian model reduction process, in which any concept learning \textendash{} associated with these slots \textendash{} can be reset in favor of a simpler model with higher model evidence. We use simulations to illustrate this model's ability to add new concepts to its state space, increase the granularity of the concepts it currently possesses, and accomplish a simple form of `one-shot' generalization to new stimuli. Although deliberately simple, these results suggest that active inference may offer useful resources in developing neurocomputational models of concept learning.{$<$}/p{$>$}},
  language = {en},
  journal = {bioRxiv},
  doi = {10.1101/633677},
  author = {Smith, Ryan and Schwartenbeck, Philipp and Parr, Thomas and Friston, Karl J.},
  month = may,
  year = {2019},
  pages = {633677},
  file = {/Users/lancelotdacosta/Zotero/storage/YYHX3YWN/Smith et al. - 2019 - An active inference model of concept learning.pdf;/Users/lancelotdacosta/Zotero/storage/H7C9LAQA/633677v1.html}
}

@article{hobsonWakingDreamingConsciousness2012,
  title = {Waking and Dreaming Consciousness: {{Neurobiological}} and Functional Considerations},
  volume = {98},
  issn = {0301-0082},
  shorttitle = {Waking and Dreaming Consciousness},
  abstract = {{$\blackpointerright$} We present a theoretical review of sleep with a special focus on pontine-geniculate-occipital waves and what they tell us about sleep and consciousness. {$\blackpointerright$} We review the nature and purpose of sleep in terms of protoconsciousness and predictive coding, using the free energy principle. {$\blackpointerright$} By combining these theoretical perspectives, we discover answers to some fundamental questions: such as why is homeothermy suspended during sleep? Why is sleep necessary? Why are we not surprised by our dreams? What is the role of synaptic regression in sleep? {$\blackpointerright$} In brief, we show that the brain can optimize itself during sleep by minimizing the statistical complexity of its model of the waking world. {$\blackpointerright$} The implicit optimization processes are remarkably consistent with the known neurobiology of sleep and provide testable predictions about its functional anatomy., This paper presents a theoretical review of rapid eye movement sleep with a special focus on pontine-geniculate-occipital waves and what they might tell us about the functional anatomy of sleep and consciousness. In particular, we review established ideas about the nature and purpose of sleep in terms of protoconsciousness and free energy minimization. By combining these theoretical perspectives, we discover answers to some fundamental questions about sleep: for example, why is homeothermy suspended during sleep? Why is sleep necessary? Why are we not surprised by our dreams? What is the role of synaptic regression in sleep? The imperatives for sleep that emerge also allow us to speculate about the functional role of PGO waves and make some empirical predictions that can, in principle, be tested using recent advances in the modeling of electrophysiological data.},
  number = {1},
  journal = {Progress in Neurobiology},
  doi = {10.1016/j.pneurobio.2012.05.003},
  author = {Hobson, J.A. and Friston, K.J.},
  month = jul,
  year = {2012},
  pages = {82-98},
  file = {/Users/lancelotdacosta/Zotero/storage/ZZ5DWPCJ/Hobson and Friston - 2012 - Waking and dreaming consciousness Neurobiological.pdf},
  pmid = {22609044},
  pmcid = {PMC3389346}
}

@article{hobsonVirtualRealityConsciousness2014,
  title = {Virtual Reality and Consciousness Inference in Dreaming},
  volume = {5},
  issn = {1664-1078},
  abstract = {This article explores the notion that the brain is genetically endowed with an innate virtual reality generator that \textendash{} through experience-dependent plasticity \textendash{}becomes a generative or predictive model of the world. This model, which is most clearly revealed in rapid eye movement (REM) sleep dreaming, may provide the theatre for conscious experience. Functional neuroimaging evidence for brain activations that are time-locked to rapid eye movements endorses the view that waking consciousness emerges from REM sleep \textendash{} and dreaming lays the foundations for waking perception. In this view, the brain is equipped with a virtual model of the world that generates predictions of its sensations. This model is continually updated and entrained by sensory prediction errors in wakefulness to ensure veridical perception, but not in dreaming. In contrast, dreaming plays an essential role in maintaining and enhancing the capacity to model the world by minimizing model complexity and thereby maximizing both statistical and thermodynamic efficiency. This perspective suggests that consciousness corresponds to the embodied process of inference, realized through the generation of virtual realities (in both sleep and wakefulness). In short, our premise or hypothesis is that the waking brain engages with the sensorium to predict the causes of sensations, while in sleep the brain's generative model is actively refined so that it generates more efficient predictions during waking. We review the evidence in support of this hypothesis \textendash{} evidence that grounds consciousness in biophysical computations whose neuronal and neurochemical infrastructure has been disclosed by sleep research.},
  language = {English},
  journal = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2014.01133},
  author = {Hobson, J. Allan and Hong, Charles C.-H. and Friston, Karl J.},
  year = {2014},
  keywords = {Neuromodulation,Consciousness,free energy,pontine-geniculate-occipital waves,predictive coding,rapid eye movements,Sleep,virtual reality},
  file = {/Users/lancelotdacosta/Zotero/storage/7W2AMNHJ/Hobson et al. - 2014 - Virtual reality and consciousness inference in dre.pdf}
}

@misc{BayesianModelSelection,
  title = {Bayesian Model Selection},
  howpublished = {http://alumni.media.mit.edu/\textasciitilde{}tpminka/statlearn/demo/},
  file = {/Users/lancelotdacosta/Zotero/storage/V5IHU5D5/demo.html}
}

@inproceedings{dauwelsMeasuringNeuralSynchrony2007,
  title = {Measuring {{Neural Synchrony}} by {{Message Passing}}},
  abstract = {A novel approach to measure the interdependence of two time series is proposed, referred to as ``stochastic event synchrony '' (SES); it quantifies the alignment of two point processes by means of the following parameters: time delay, variance of the timing jitter, fraction of ``spurious '' events, and average similarity of events. SES may be applied to generic one-dimensional and multi-dimensional point processes, however, the paper mainly focusses on point processes in time-frequency domain. The average event similarity is in that case described by two parameters: the average frequency offset between events in the time-frequency plane, and the variance of the frequency offset (``frequency jitter''); SES then consists of five parameters in total. Those parameters quantify the synchrony of oscillatory events, and hence, they provide an alternative to existing synchrony measures that quantify amplitude or phase synchrony. The pairwise alignment of point processes is cast as a statistical inference problem, which is solved by applying the max-product algorithm on a graphical model. The SES parameters are determined from the resulting pairwise alignment by maximum a posteriori (MAP) estimation. The proposed interdependence measure is applied to the problem of detecting anomalies in EEG synchrony of Mild Cognitive Impairment (MCI) patients; the results indicate that SES significantly improves the sensitivity of EEG in detecting MCI. 1},
  booktitle = {{{NIPS}}},
  author = {Dauwels, Justin and Vialatte, Fran{\c c}ois and Rutkowski, Tomasz and Cichocki, Andrzej},
  year = {2007},
  file = {/Users/lancelotdacosta/Zotero/storage/L9VKBUD7/Dauwels et al. - Measuring Neural Synchrony by Message Passing.pdf;/Users/lancelotdacosta/Zotero/storage/9D3TYS9P/summary.html}
}

@article{fristonBayesianModelReduction2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.07092},
  primaryClass = {stat},
  title = {Bayesian Model Reduction},
  abstract = {This paper reviews recent developments in statistical structure learning; namely, Bayesian model reduction. Bayesian model reduction is a special but ubiquitous case of Bayesian model comparison that, in the setting of variational Bayes, furnishes an analytic solution for (a lower bound on) model evidence induced by a change in priors. This analytic solution finesses the problem of scoring large model spaces in model comparison or structure learning. This is because each new model can be cast in terms of an alternative set of priors over model parameters. Furthermore, the reduced free energy (i.e. evidence bound on the reduced model) finds an expedient application in hierarchical models, where it plays the role of a summary statistic. In other words, it contains all the necessary information contained in the posterior distributions over parameters of lower levels. In this technical note, we review Bayesian model reduction - in terms of common forms of reduced free energy - and illustrate recent applications in structure learning, hierarchical or empirical Bayes and as a metaphor for neurobiological processes like abductive reasoning and sleep.},
  journal = {arXiv:1805.07092 [stat]},
  author = {Friston, Karl and Parr, Thomas and Zeidman, Peter},
  month = may,
  year = {2018},
  keywords = {Statistics - Methodology},
  file = {/Users/lancelotdacosta/Zotero/storage/MSIRXKDM/Friston et al. - 2018 - Bayesian model reduction.pdf;/Users/lancelotdacosta/Zotero/storage/6Y68H4K5/1805.html}
}

@article{gershmanLearningLatentStructure2010,
  title = {Learning Latent Structure: Carving Nature at Its Joints},
  volume = {20},
  issn = {1873-6882},
  shorttitle = {Learning Latent Structure},
  abstract = {Reinforcement learning (RL) algorithms provide powerful explanations for simple learning and decision-making behaviors and the functions of their underlying neural substrates. Unfortunately, in real-world situations that involve many stimuli and actions, these algorithms learn pitifully slowly, exposing their inferiority in comparison to animal and human learning. Here we suggest that one reason for this discrepancy is that humans and animals take advantage of structure that is inherent in real-world tasks to simplify the learning problem. We survey an emerging literature on 'structure learning'--using experience to infer the structure of a task--and how this can be of service to RL, with an emphasis on structure in perception and action.},
  language = {eng},
  number = {2},
  journal = {Current Opinion in Neurobiology},
  doi = {10.1016/j.conb.2010.02.008},
  author = {Gershman, Samuel J. and Niv, Yael},
  month = apr,
  year = {2010},
  keywords = {Humans,Algorithms,Animals,Artificial Intelligence,Brain,Learning,Models; Animal,Models; Neurological,Neuropsychology,Perception,Psychomotor Performance,Reinforcement (Psychology)},
  pages = {251-256},
  file = {/Users/lancelotdacosta/Zotero/storage/M52GUCWL/Gershman and Niv - 2010 - Learning latent structure carving nature at its j.pdf},
  pmid = {20227271},
  pmcid = {PMC2862793}
}

@article{tervoNeuralImplementationStructure2016,
  title = {Toward the Neural Implementation of Structure Learning},
  volume = {37},
  issn = {1873-6882},
  abstract = {Despite significant advances in neuroscience, the neural bases of intelligence remain poorly understood. Arguably the most elusive aspect of intelligence is the ability to make robust inferences that go far beyond one's experience. Animals categorize objects, learn to vocalize and may even estimate causal relationships - all in the face of data that is often ambiguous and sparse. Such inductive leaps are thought to result from the brain's ability to infer latent structure that governs the environment. However, we know little about the neural computations that underlie this ability. Recent advances in developing computational frameworks that can support efficient structure learning and inductive inference may provide insight into the underlying component processes and help pave the path for uncovering their neural implementation.},
  language = {eng},
  journal = {Current Opinion in Neurobiology},
  doi = {10.1016/j.conb.2016.01.014},
  author = {Tervo, D. Gowanlock R. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
  month = apr,
  year = {2016},
  keywords = {Animals,Brain,Learning,Models; Neurological},
  pages = {99-105},
  pmid = {26874471}
}

@article{baezRelativeEntropyBiological2016,
  title = {Relative {{Entropy}} in {{Biological Systems}}},
  volume = {18},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  abstract = {In this paper we review various information-theoretic characterizations of the approach to equilibrium in biological systems. The replicator equation, evolutionary game theory, Markov processes and chemical reaction networks all describe the dynamics of a population or probability distribution. Under suitable assumptions, the distribution will approach an equilibrium with the passage of time. Relative entropy\textemdash{}that is, the Kullback\textendash{}Leibler divergence, or various generalizations of this\textemdash{}provides a quantitative measure of how far from equilibrium the system is. We explain various theorems that give conditions under which relative entropy is nonincreasing. In biochemical applications these results can be seen as versions of the Second Law of Thermodynamics, stating that free energy can never increase with the passage of time. In ecological applications, they make precise the notion that a population gains information from its environment as it approaches equilibrium.},
  language = {en},
  number = {2},
  journal = {Entropy},
  doi = {10.3390/e18020046},
  author = {Baez, John C. and Pollard, Blake S.},
  month = feb,
  year = {2016},
  keywords = {free energy,game theory,Kullback–Leibler divergence,Markov process,reaction network,relative entropy,relative information,Second Law},
  pages = {46},
  file = {/Users/lancelotdacosta/Zotero/storage/XZAGDNYA/Baez and Pollard - 2016 - Relative Entropy in Biological Systems.pdf;/Users/lancelotdacosta/Zotero/storage/PE96NUUD/46.html}
}

@book{vonbekesySensoryInhibition1967,
  title = {Sensory {{Inhibition}}},
  publisher = {{Princeton University Press}},
  author = {Von B{\'e}k{\'e}sy, Georg},
  year = {1967}
}

@article{fusterPrefrontalCortexBridging1990,
  title = {Prefrontal Cortex and the Bridging of Temporal Gaps in the Perception-Action Cycle},
  volume = {608},
  issn = {0077-8923},
  abstract = {Normal behavior is characterized by a constant circular flow of influences from sensory receptors to motor effectors, to the physical environment, back to sensory receptors, and so on. This cybernetic cycle of influences (the perception-action cycle) governs all sequences of behavior to make them adaptive and goal directed. In the primate (including the human primate), considerable evidence indicates that dorsolateral prefrontal cortex is essential for the bridging of temporal gaps in the perception-action cycle, in other words, for mediating cross-temporal contingencies of behavior. This chapter summarizes some neuropsychological and neurophysiological evidence in support of this conclusion. The evidence has been obtained from monkeys performing delay tasks, which epitomize the principle of cross-temporal contingency.},
  language = {eng},
  journal = {Annals of the New York Academy of Sciences},
  doi = {10.1111/j.1749-6632.1990.tb48901.x},
  author = {Fuster, J. M.},
  year = {1990},
  keywords = {Humans,Animals,Models; Neurological,Psychomotor Performance,Frontal Lobe,Haplorhini,Memory},
  pages = {318-329; discussion 330-336},
  pmid = {2127512}
}

@article{coullNeuroanatomicalNeurochemicalSubstrates2011,
  title = {Neuroanatomical and Neurochemical Substrates of Timing},
  volume = {36},
  issn = {1740-634X},
  abstract = {We all have a sense of time. Yet, there are no sensory receptors specifically dedicated for perceiving time. It is an almost uniquely intangible sensation: we cannot see time in the way that we see color, shape, or even location. So how is time represented in the brain? We explore the neural substrates of metrical representations of time such as duration estimation (explicit timing) or temporal expectation (implicit timing). Basal ganglia (BG), supplementary motor area, cerebellum, and prefrontal cortex have all been linked to the explicit estimation of duration. However, each region may have a functionally discrete role and will be differentially implicated depending upon task context. Among these, the dorsal striatum of the BG and, more specifically, its ascending nigrostriatal dopaminergic pathway seems to be the most crucial of these regions, as shown by converging functional neuroimaging, neuropsychological, and psychopharmacological investigations in humans, as well as lesion and pharmacological studies in animals. Moreover, neuronal firing rates in both striatal and interconnected frontal areas vary as a function of duration, suggesting a neurophysiological mechanism for the representation of time in the brain, with the excitatory-inhibitory balance of interactions among distinct subtypes of striatal neuron serving to fine-tune temporal accuracy and precision.},
  language = {eng},
  number = {1},
  journal = {Neuropsychopharmacology: Official Publication of the American College of Neuropsychopharmacology},
  doi = {10.1038/npp.2010.113},
  author = {Coull, Jennifer T. and Cheng, Ruey-Kuang and Meck, Warren H.},
  month = jan,
  year = {2011},
  keywords = {Humans,Neural Pathways,Neurons,Animals,Brain,Models; Neurological,Action Potentials,Brain Chemistry,Dopamine,Neostriatum,Time Perception},
  pages = {3-25},
  file = {/Users/lancelotdacosta/Zotero/storage/3I7X86BH/Coull et al. - 2011 - Neuroanatomical and neurochemical substrates of ti.pdf},
  pmid = {20668434},
  pmcid = {PMC3055517}
}

@article{riesenhuberHierarchicalModelsObject1999,
  title = {Hierarchical Models of Object Recognition in Cortex},
  volume = {2},
  copyright = {1999 Nature America Inc.},
  issn = {1546-1726},
  abstract = {Visual processing in cortex is classically modeled as a hierarchy of increasingly sophisticated representations, naturally extending the model of simple to complex cells of Hubel and Wiesel. Surprisingly, little quantitative modeling has been done to explore the biological feasibility of this class of models to explain aspects of higher-level visual processing such as object recognition. We describe a new hierarchical model consistent with physiological data from inferotemporal cortex that accounts for this complex visual task and makes testable predictions. The model is based on a MAX-like operation applied to inputs to certain cortical neurons that may have a general role in cortical function.},
  language = {en},
  number = {11},
  journal = {Nature Neuroscience},
  doi = {10.1038/14819},
  author = {Riesenhuber, Maximilian and Poggio, Tomaso},
  month = nov,
  year = {1999},
  pages = {1019-1025},
  file = {/Users/lancelotdacosta/Zotero/storage/9EUV6ZT9/Riesenhuber and Poggio - 1999 - Hierarchical models of object recognition in corte.pdf;/Users/lancelotdacosta/Zotero/storage/368U6KX5/nn1199_1019.html}
}

@article{carpenterMassivelyParallelArchitecture1987,
  title = {A Massively Parallel Architecture for a Self-Organizing Neural Pattern Recognition Machine},
  volume = {37},
  issn = {0734-189X},
  abstract = {A neural network architecture for the learning of recognition categories is derived. Real-time network dynamics are completely characterized through mathematical analysis and computer simulations. The architecture self-organizes and self-stabilizes its recognition codes in response to arbitrary orderings of arbitrarily many and arbitrarily complex binary input patterns. Top-down attentional and matching mechanisms are critical in self-stabilizing the code learning process. The architecture embodies a parallel search scheme which updates itself adaptively as the learning process unfolds. After learning self-stabilizes, the search process is automatically disengaged. Thereafter input patterns directly access their recognition codes without any search. Thus recognition time does not grow as a function of code complexity. A novel input pattern can directly access a category if it shares invariant properties with the set of familiar exemplars of that category. These invariant properties emerge in the form of learned critical feature patterns, or prototypes. The architecture possesses a context-sensitive self-scaling property which enables its emergent critical feature patterns to form. They detect and remember statistically predictive configurations of featural elements which are derived from the set of all input patterns that are ever experienced. Four types of attentional process\textemdash{}priming, gain control, vigilance, and intermodal competition\textemdash{}are mechanistically characterized. Top\textemdash{}down priming and gain control are needed for code matching and self-stabilization. Attentional vigilance determines how fine the learned categories will be. If vigilance increases due to an environmental disconfirmation, then the system automatically searches for and learns finer recognition categories. A new nonlinear matching law (the {$\textfrac{2}{3}$} Rule) and new nonlinear associative laws (the Weber Law Rule, the Associative Decay Rule, and the Template Learning Rule) are needed to achieve these properties. All the rules describe emergent properties of parallel network interactions. The architecture circumvents the noise, saturation, capacity, orthogonality, and linear predictability constraints that limit the codes which can be stably learned by alternative recognition models.},
  number = {1},
  journal = {Computer Vision, Graphics, and Image Processing},
  doi = {10.1016/S0734-189X(87)80014-2},
  author = {Carpenter, Gail A. and Grossberg, Stephen},
  month = jan,
  year = {1987},
  pages = {54-115},
  file = {/Users/lancelotdacosta/Zotero/storage/F4DLL8MA/S0734189X87800142.html}
}

@article{ittiModelSaliencybasedVisual1998,
  title = {A Model of Saliency-Based Visual Attention for Rapid Scene Analysis},
  volume = {20},
  issn = {0162-8828},
  abstract = {A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail.},
  number = {11},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  doi = {10.1109/34.730558},
  author = {Itti, L. and Koch, C. and Niebur, E.},
  month = nov,
  year = {1998},
  keywords = {Neural networks,Biological system modeling,Brain modeling,Computer architecture,computer vision,dynamical neural network,feature extraction,Feature extraction,Hardware,Image analysis,image recognition,Layout,neural nets,Object detection,rapid scene analysis,saliency,scene understanding,target detection,target tracking,topographical saliency map,visual attention,visual search,Visual system},
  pages = {1254-1259},
  file = {/Users/lancelotdacosta/Zotero/storage/R257AXWW/730558.html}
}

@article{reverdyParameterEstimationSoftmax2016,
  title = {Parameter {{Estimation}} in {{Softmax Decision}}-{{Making Models With Linear Objective Functions}}},
  volume = {13},
  issn = {1545-5955},
  abstract = {We contribute to the development of a systematic means to infer features of human decision-making from behavioral data. Motivated by the common use of softmax selection in models of human decision-making, we study the maximum-likelihood (ML) parameter estimation problem for softmax decision-making models with linear objective functions. We present conditions under which the likelihood function is convex. These allow us to provide sufficient conditions for convergence of the resulting ML estimator and to construct its asymptotic distribution. In the case of models with nonlinear objective functions, we show how the estimator can be applied by linearizing about a nominal parameter value. We apply the estimator to fit the stochastic Upper Credible Limit (UCL) model of human decision-making to human subject data. The fits show statistically significant differences in behavior across related, but distinct, tasks.},
  number = {1},
  journal = {IEEE Transactions on Automation Science and Engineering},
  doi = {10.1109/TASE.2015.2499244},
  author = {Reverdy, P. and Leonard, N. E.},
  month = jan,
  year = {2016},
  keywords = {Stochastic processes,Biological system modeling,asymptotic distribution,Automation,behavioral data,behavioural sciences,convergence,Data models,decision making,Decision making,decision-making,estimation,human decision-making,linear objective functions,Linear programming,maximum likelihood estimation,maximum-likelihood parameter estimation problem,ML estimator,ML parameter estimation problem,nonlinear objective functions,Parameter estimation,softmax decision-making models,softmax selection,stochastic processes,stochastic upper credible limit model,sufficient conditions,UCL model},
  pages = {54-67},
  file = {/Users/lancelotdacosta/Zotero/storage/MNZ7GKUF/Reverdy and Leonard - 2016 - Parameter Estimation in Softmax Decision-Making Mo.pdf;/Users/lancelotdacosta/Zotero/storage/66RW8G7N/7336571.html}
}

@article{reverdyModelingHumanDecisionmaking2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1307.6134},
  primaryClass = {cs, math, stat},
  title = {Modeling {{Human Decision}}-Making in {{Generalized Gaussian Multi}}-Armed {{Bandits}}},
  abstract = {We present a formal model of human decision-making in explore-exploit tasks using the context of multi-armed bandit problems, where the decision-maker must choose among multiple options with uncertain rewards. We address the standard multi-armed bandit problem, the multi-armed bandit problem with transition costs, and the multi-armed bandit problem on graphs. We focus on the case of Gaussian rewards in a setting where the decision-maker uses Bayesian inference to estimate the reward values. We model the decision-maker's prior knowledge with the Bayesian prior on the mean reward. We develop the upper credible limit (UCL) algorithm for the standard multi-armed bandit problem and show that this deterministic algorithm achieves logarithmic cumulative expected regret, which is optimal performance for uninformative priors. We show how good priors and good assumptions on the correlation structure among arms can greatly enhance decision-making performance, even over short time horizons. We extend to the stochastic UCL algorithm and draw several connections to human decision-making behavior. We present empirical data from human experiments and show that human performance is efficiently captured by the stochastic UCL algorithm with appropriate parameters. For the multi-armed bandit problem with transition costs and the multi-armed bandit problem on graphs, we generalize the UCL algorithm to the block UCL algorithm and the graphical block UCL algorithm, respectively. We show that these algorithms also achieve logarithmic cumulative expected regret and require a sub-logarithmic expected number of transitions among arms. We further illustrate the performance of these algorithms with numerical examples. NB: Appendix G included in this version details minor modifications that correct for an oversight in the previously-published proofs. The remainder of the text reflects the published work.},
  journal = {arXiv:1307.6134 [cs, math, stat]},
  author = {Reverdy, Paul and Srivastava, Vaibhav and Leonard, Naomi E.},
  month = jul,
  year = {2013},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Mathematics - Optimization and Control},
  file = {/Users/lancelotdacosta/Zotero/storage/Y6KUARW5/Reverdy et al. - 2013 - Modeling Human Decision-making in Generalized Gaus.pdf;/Users/lancelotdacosta/Zotero/storage/IWXF7JF7/1307.html}
}

@article{dawCorticalSubstratesExploratory2006,
  title = {Cortical Substrates for Exploratory Decisions in Humans},
  volume = {441},
  issn = {1476-4687},
  abstract = {Decision making in an uncertain environment poses a conflict between the opposing demands of gathering and exploiting information. In a classic illustration of this 'exploration-exploitation' dilemma, a gambler choosing between multiple slot machines balances the desire to select what seems, on the basis of accumulated experience, the richest option, against the desire to choose a less familiar option that might turn out more advantageous (and thereby provide information for improving future decisions). Far from representing idle curiosity, such exploration is often critical for organisms to discover how best to harvest resources such as food and water. In appetitive choice, substantial experimental evidence, underpinned by computational reinforcement learning (RL) theory, indicates that a dopaminergic, striatal and medial prefrontal network mediates learning to exploit. In contrast, although exploration has been well studied from both theoretical and ethological perspectives, its neural substrates are much less clear. Here we show, in a gambling task, that human subjects' choices can be characterized by a computationally well-regarded strategy for addressing the explore/exploit dilemma. Furthermore, using this characterization to classify decisions as exploratory or exploitative, we employ functional magnetic resonance imaging to show that the frontopolar cortex and intraparietal sulcus are preferentially active during exploratory decisions. In contrast, regions of striatum and ventromedial prefrontal cortex exhibit activity characteristic of an involvement in value-based exploitative decision making. The results suggest a model of action selection under uncertainty that involves switching between exploratory and exploitative behavioural modes, and provide a computationally precise characterization of the contribution of key decision-related brain systems to each of these functions.},
  language = {eng},
  number = {7095},
  journal = {Nature},
  doi = {10.1038/nature04766},
  author = {Daw, Nathaniel D. and O'Doherty, John P. and Dayan, Peter and Seymour, Ben and Dolan, Raymond J.},
  month = jun,
  year = {2006},
  keywords = {Humans,Models; Neurological,Choice Behavior,Decision Making,Exploratory Behavior,Gambling,Magnetic Resonance Imaging,Models; Psychological,Prefrontal Cortex,Reward,Uncertainty},
  pages = {876-879},
  file = {/Users/lancelotdacosta/Zotero/storage/D3JSDGGU/Daw et al. - 2006 - Cortical substrates for exploratory decisions in h.pdf;/Users/lancelotdacosta/Zotero/storage/S44SCQGX/Daw et al. - 2006 - Cortical substrates for exploratory decisions in h.pdf},
  pmid = {16778890},
  pmcid = {PMC2635947}
}

@misc{kurtKullbackLeiblerDivergenceTwo2013,
  title = {Kullback-{{Leibler Divergence Between Two Dirichlet}} (and {{Beta}}) {{Distributions}}},
  language = {en-US},
  author = {Kurt, Baris},
  year = {2013},
  file = {/Users/lancelotdacosta/Zotero/storage/2XGXK435/kullback-leibler-divergence-between-two-dirichlet-and-beta-distributions.html}
}

@phdthesis{parrComputationalNeurologyActive2019,
  address = {{London}},
  type = {Ph.{{D}}.},
  title = {The Computational Neurology of Active Vision},
  school = {University College London},
  author = {Parr, Thomas},
  year = {2019},
  file = {/Users/lancelotdacosta/Zotero/storage/MK9YXIWI/Parr - 2019 - The computational neurology of active vision.pdf}
}

@article{senguptaNeuronalGaugeTheory2016,
  title = {Towards a {{Neuronal Gauge Theory}}},
  volume = {14},
  issn = {1545-7885},
  abstract = {Given the amount of knowledge and data accruing in the neurosciences, is it time to formulate a general principle for neuronal dynamics that holds at evolutionary, developmental, and perceptual timescales? In this paper, we propose that the brain (and other self-organised biological systems) can be characterised via the mathematical apparatus of a gauge theory. The picture that emerges from this approach suggests that any biological system (from a neuron to an organism) can be cast as resolving uncertainty about its external milieu, either by changing its internal states or its relationship to the environment. Using formal arguments, we show that a gauge theory for neuronal dynamics\textemdash{}based on approximate Bayesian inference\textemdash{}has the potential to shed new light on phenomena that have thus far eluded a formal description, such as attention and the link between action and perception.},
  language = {en},
  number = {3},
  journal = {PLOS Biology},
  doi = {10.1371/journal.pbio.1002400},
  author = {Sengupta, Biswa and Tozzi, Arturo and Cooray, Gerald K. and Douglas, Pamela K. and Friston, Karl J.},
  year = {08-Mar-2016},
  keywords = {Free energy,Attention,Probability distribution,Differential geometry,Manifolds,Nervous system,Non-Euclidean geometry,Symmetry},
  pages = {e1002400},
  file = {/Users/lancelotdacosta/Zotero/storage/A724G4EY/Sengupta et al. - 2016 - Towards a Neuronal Gauge Theory.pdf;/Users/lancelotdacosta/Zotero/storage/U72YVI96/article.html}
}

@article{rajWiringEconomyPrinciple2011,
  title = {The {{Wiring Economy Principle}}: {{Connectivity Determines Anatomy}} in the {{Human Brain}}},
  volume = {6},
  issn = {1932-6203},
  shorttitle = {The {{Wiring Economy Principle}}},
  abstract = {Minimization of the wiring cost of white matter fibers in the human brain appears to be an organizational principle. We investigate this aspect in the human brain using whole brain connectivity networks extracted from high resolution diffusion MRI data of 14 normal volunteers. We specifically address the question of whether brain anatomy determines its connectivity or vice versa. Unlike previous studies we use weighted networks, where connections between cortical nodes are real-valued rather than binary off-on connections. In one set of analyses we found that the connectivity structure of the brain has near optimal wiring cost compared to random networks with the same number of edges, degree distribution and edge weight distribution. A specifically designed minimization routine could not find cheaper wiring without significantly degrading network performance. In another set of analyses we kept the observed brain network topology and connectivity but allowed nodes to freely move on a 3D manifold topologically identical to the brain. An efficient minimization routine was written to find the lowest wiring cost configuration. We found that beginning from any random configuration, the nodes invariably arrange themselves in a configuration with a striking resemblance to the brain. This confirms the widely held but poorly tested claim that wiring economy is a driving principle of the brain. Intriguingly, our results also suggest that the brain mainly optimizes for the most desirable network connectivity, and the observed brain anatomy is merely a result of this optimization.},
  number = {9},
  journal = {PLoS ONE},
  doi = {10.1371/journal.pone.0014832},
  author = {Raj, Ashish and Chen, Yu-hsien},
  month = sep,
  year = {2011},
  file = {/Users/lancelotdacosta/Zotero/storage/7BPQYY3D/Raj and Chen - 2011 - The Wiring Economy Principle Connectivity Determi.pdf},
  pmid = {21915250},
  pmcid = {PMC3168442}
}

@inproceedings{georgeBeliefPropagationWiring2005,
  title = {Belief {{Propagation}} and {{Wiring Length Optimization}} as {{Organizing Principles}} for {{Cortical Microcircuits}}},
  abstract = {In this paper we explore how functional and anatomical constraints and resource optimization could be combined to obtain a canonical cortical micro-circuit and an explanation for its laminar organization. We start with the assumption that cortical regions are involved in Bayesian Belief Propagation. This imposes a set of constraints on the type of neurons and the connection patterns between neurons in that region. In addition there are anatomical constraints that a region has to adhere to. There are several different configurations of neurons consistent with both these constraints. Among all such configurations, it is reasonable to expect that Nature has chosen the configuration with the minimum wiring length. We cast the problem of finding the optimum configuration as a combinatorial optimization problem. A near-optimal solution to this problem matched anatomical and physiological data. As the result of this investigation, we propose a canonical cortical micro-circuit that will support Bayesian Belief Propagation computation and whose laminar organization is near optimal in its wiring length. We describe how the details of this circuit match many of the anatomical and physiological findings and discuss the implications of these results to experimenters and theorists.},
  author = {George, Dileep},
  year = {2005},
  keywords = {Neurons,Bayesian network,Belief propagation,Combinatorial optimization,Computation,Mathematical optimization,Optimization problem,Software propagation,Wiring},
  file = {/Users/lancelotdacosta/Zotero/storage/FLHNF953/George - 2005 - Belief Propagation and Wiring Length Optimization .pdf}
}

@article{chenWiringOptimizationCan2006,
  title = {Wiring Optimization Can Relate Neuronal Structure and Function},
  volume = {103},
  copyright = {\textcopyright{} 2006 by The National Academy of Sciences of the USA},
  issn = {0027-8424, 1091-6490},
  abstract = {We pursue the hypothesis that neuronal placement in animals minimizes wiring costs for given functional constraints, as specified by synaptic connectivity. Using a newly compiled version of the Caenorhabditis elegans wiring diagram, we solve for the optimal layout of 279 nonpharyngeal neurons. In the optimal layout, most neurons are located close to their actual positions, suggesting that wiring minimization is an important factor. Yet some neurons exhibit strong deviations from ``optimal'' position. We propose that biological factors relating to axonal guidance and command neuron functions contribute to these deviations. We capture these factors by proposing a modified wiring cost function.},
  language = {en},
  number = {12},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.0506806103},
  author = {Chen, Beth L. and Hall, David H. and Chklovskii, Dmitri B.},
  month = mar,
  year = {2006},
  keywords = {Caenorhabditis elegans,optimal placement},
  pages = {4723-4728},
  file = {/Users/lancelotdacosta/Zotero/storage/54JPB8CX/Chen et al. - 2006 - Wiring optimization can relate neuronal structure .pdf;/Users/lancelotdacosta/Zotero/storage/PT8FXERW/4723.html},
  pmid = {16537428}
}

@article{parrAnatomyInferenceGenerative2018,
  title = {The {{Anatomy}} of {{Inference}}: {{Generative Models}} and {{Brain Structure}}},
  volume = {12},
  issn = {1662-5188},
  shorttitle = {The {{Anatomy}} of {{Inference}}},
  abstract = {To infer the causes of its sensations, the brain must call on a generative (predictive) model. This necessitates passing local messages between populations of neurons to update beliefs about hidden variables in the world beyond its sensory samples. It also entails inferences about how we will act. Active inference is a principled framework that frames perception and action as approximate Bayesian inference. This has been successful in accounting for a wide range of physiological and behavioural phenomena. Recently, a process theory has emerged that attempts to relate inferences to their neurobiological substrates. In this paper, we review and develop the anatomical aspects of this process theory. We argue that the form of the generative models required for inference constrains the way in which brain regions connect to one another. Specifically, neuronal populations representing beliefs about a variable must receive input from populations representing the Markov blanket of that variable. We illustrate this idea in four different domains: perception, planning, attention, and movement. In doing so, we attempt to show how appealing to generative models enables us to account for anatomical brain architectures. Ultimately, committing to an anatomical theory of inference ensures we can form empirical hypotheses that can be tested using neuroimaging, neuropsychological, and electrophysiological experiments.},
  language = {English},
  journal = {Frontiers in Computational Neuroscience},
  doi = {10.3389/fncom.2018.00090},
  author = {Parr, Thomas and Friston, Karl J.},
  year = {2018},
  keywords = {Active inference.,Bayesian,Generative Model,message passing,Neuroanatomy,predictive processing},
  file = {/Users/lancelotdacosta/Zotero/storage/NTV6M2E5/Parr and Friston - 2018 - The Anatomy of Inference Generative Models and Br.pdf}
}

@incollection{bernsHowBasalGanglia1996,
  address = {{Berlin, Heidelberg}},
  series = {Research and {{Perspectives}} in {{Neurosciences}}},
  title = {How the {{Basal Ganglia Make Decisions}}},
  isbn = {978-3-642-79928-0},
  abstract = {The primate basal ganglia are a collection of subcortical structures that have long been considered part of the extrapyramidal motor system, the part of the motor system concerned with automatic aspects of movement. Despite a large amount of data regarding their anatomy and physiology, the role of the basal ganglia in both action planning and decision making remains enigmatic. Anatomical labeling studies have suggested that the striatum receives projections from the cerebral cortex that coarsely preserves topography, and that the basal ganglia maintain a segregation of information streams (Goldman-Rakic and Selemon 1986; Hoover and Strick 1993; Parent 1990). We suggest that the connectivity of the basal ganglia is ideally suited to selecting optimal actions for given cognitive and sensory states. We demonstrate how a computational network of pools of neurons connected in the arrangement found in the basal ganglia can perform what we term a ``winner-lose-all'' function. The winner-lose-all mechanism refers to the fact that the neurons of the output stage of the basal ganglia, the internal segment of the globus pallidus (GPi), are tonically active and are inhibited when corresponding striatal afferents fire. Thus the GPi neuron that is selected is actually inhibited because it loses rather than wins the competition. Diffuse excitatory projections from the subthalamic nucleus prevent all but the winning pallidal neuron pool from being inhibited. Because the thalamic targets of the GPi projection in turn feed back to the approximate cortical area of the originating afferent, this cortical-subcortical loop is ideally suited not only for the aforementioned action-selection, but also for the generation of sequences appropriate for given cortical states. We demonstrate how the circuitry of the basal ganglia can learn to select the best action for different cortical states and how the feedback representation of the action-selection leads to the generation of sequences of actions.},
  language = {en},
  booktitle = {Neurobiology of {{Decision}}-{{Making}}},
  publisher = {{Springer Berlin Heidelberg}},
  doi = {10.1007/978-3-642-79928-0_6},
  author = {Berns, G. S. and Sejnowski, T. J.},
  editor = {Damasio, A. R. and Damasio, H. and Christen, Y.},
  year = {1996},
  keywords = {Basal Ganglion,Firing Rate,Globus Pallidus,Globus Pallidus Internus,Ventral Tegmental Area},
  pages = {101-113},
  file = {/Users/lancelotdacosta/Zotero/storage/YAJES2EJ/Berns and Sejnowski - 1996 - How the Basal Ganglia Make Decisions.pdf}
}

@article{dingBasalGangliaContributions2013,
  title = {The Basal Ganglia's Contributions to Perceptual Decision-Making},
  volume = {79},
  issn = {0896-6273},
  abstract = {Perceptual decision-making is a computationally demanding process that requires the brain to interpret incoming sensory information in the context of goals, expectations, preferences, and other factors. These integrative processes engage much of cortex but also require contributions from subcortical structures to affect behavior. Here we summarize recent evidence supporting specific computational roles of the basal ganglia in perceptual decision-making. These roles likely share common mechanisms with the basal ganglia's other, more well-established functions in motor control, learning, and other aspects of cognition and thus can provide insights into the general roles of this important subcortical network in higher brain function.},
  number = {4},
  journal = {Neuron},
  doi = {10.1016/j.neuron.2013.07.042},
  author = {Ding, Long and Gold, Joshua I.},
  month = aug,
  year = {2013},
  pages = {640-649},
  file = {/Users/lancelotdacosta/Zotero/storage/DCKIXF43/Ding and Gold - 2013 - The basal ganglia’s contributions to perceptual de.pdf},
  pmid = {23972593},
  pmcid = {PMC3771079}
}

@article{thibautBasalGangliaPlay2016,
  title = {Basal Ganglia Play a Crucial Role in Decision Making},
  volume = {18},
  issn = {1294-8322},
  abstract = {Many studies have suggested that the striatum, located at the interface of the cortico-basal ganglia-thalamic circuit, consists of separate circuits that serve distinct functions It plays an important role in motor planning, value processing, and decision making.},
  number = {1},
  journal = {Dialogues in Clinical Neuroscience},
  author = {Thibaut, Florence},
  month = mar,
  year = {2016},
  pages = {3},
  file = {/Users/lancelotdacosta/Zotero/storage/ADG9AQAI/Thibaut - 2016 - Basal ganglia play a crucial role in decision maki.pdf},
  pmid = {27069375},
  pmcid = {PMC4826768}
}

@article{haberPrimateBasalGanglia2003,
  title = {The Primate Basal Ganglia: Parallel and Integrative Networks},
  volume = {26},
  issn = {0891-0618},
  shorttitle = {The Primate Basal Ganglia},
  abstract = {The basal ganglia and frontal cortex operate together to execute goal directed behaviors. This requires not only the execution of motor plans, but also the behaviors that lead to this execution, including emotions and motivation that drive behaviors, cognition that organizes and plans the general strategy, motor planning, and finally, the execution of that plan. The components of the frontal cortex that mediate these behaviors, are reflected in the organization, physiology, and connections between areas of frontal cortex and in their projections through basal ganglia circuits. This comprises a series of parallel pathways. However, this model does not address how information flows between circuits thereby developing new learned behaviors (or actions) from a combination of inputs from emotional, cognitive, and motor cortical areas. Recent anatomical evidence from primates demonstrates that the neuro-networks within basal ganglia pathways are in a position to move information across functional circuits. Two networks are: the striato-nigral-striatal network and the thalamo-cortical-thalamic network. Within each of these sets of connected structures, there are both reciprocal connections linking up regions associated with similar functions and non-reciprocal connections linking up regions that are associated with different cortical basal ganglia circuits. Each component of information (from limbic to motor outcome) sends both feedback connection, and also a feedforward connection, allowing the transfer of information. Information is channeled from limbic, to cognitive, to motor circuits. Action decision-making processes are thus influenced by motivation and cognitive inputs, allowing the animal to respond appropriate to environmental cues.},
  language = {eng},
  number = {4},
  journal = {Journal of Chemical Neuroanatomy},
  author = {Haber, Suzanne N.},
  month = dec,
  year = {2003},
  keywords = {Humans,Neural Pathways,Animals,Basal Ganglia,Mental Processes,Primates},
  pages = {317-330},
  pmid = {14729134}
}

@article{jahanshahiFrontostriatosubthalamicpallidalNetworkGoaldirected2015,
  title = {A Fronto-Striato-Subthalamic-Pallidal Network for Goal-Directed and Habitual Inhibition},
  volume = {16},
  issn = {1471-0048},
  abstract = {Classically, the basal ganglia have been considered to have a role in producing habitual and goal-directed behaviours. In this article, we review recent evidence that expands this role, indicating that the basal ganglia are also involved in neural and behavioural inhibition in the motor and non-motor domains. We then distinguish between goal-directed and habitual (also known as automatic) inhibition mediated by fronto-striato-subthalamic-pallido-thalamo-cortical networks. We also suggest that imbalance between goal-directed and habitual action and inhibition contributes to some manifestations of Parkinson's disease, Tourette syndrome and obsessive-compulsive disorder. Finally, we propose that basal ganglia surgery improves these disorders by restoring a functional balance between facilitation and inhibition.},
  language = {eng},
  number = {12},
  journal = {Nature Reviews. Neuroscience},
  doi = {10.1038/nrn4038},
  author = {Jahanshahi, Marjan and Obeso, Ignacio and Rothwell, John C. and Obeso, Jos{\'e} A.},
  month = dec,
  year = {2015},
  keywords = {Humans,Neural Pathways,Animals,Brain,Goals,Habituation; Psychophysiologic,Inhibition (Psychology),Nerve Net},
  pages = {719-732},
  pmid = {26530468}
}

@article{tschantzScalingActiveInference2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1911.10601},
  primaryClass = {cs, eess, math, stat},
  title = {Scaling Active Inference},
  abstract = {In reinforcement learning (RL), agents often operate in partially observed and uncertain environments. Model-based RL suggests that this is best achieved by learning and exploiting a probabilistic model of the world. 'Active inference' is an emerging normative framework in cognitive and computational neuroscience that offers a unifying account of how biological agents achieve this. On this framework, inference, learning and action emerge from a single imperative to maximize the Bayesian evidence for a niched model of the world. However, implementations of this process have thus far been restricted to low-dimensional and idealized situations. Here, we present a working implementation of active inference that applies to high-dimensional tasks, with proof-of-principle results demonstrating efficient exploration and an order of magnitude increase in sample efficiency over strong model-free baselines. Our results demonstrate the feasibility of applying active inference at scale and highlight the operational homologies between active inference and current model-based approaches to RL.},
  journal = {arXiv:1911.10601 [cs, eess, math, stat]},
  author = {Tschantz, Alexander and Baltieri, Manuel and Seth, Anil K. and Buckley, Christopher L.},
  month = nov,
  year = {2019},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Theory,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Statistics - Machine Learning},
  file = {/Users/lancelotdacosta/Zotero/storage/N5V7CVDA/Tschantz et al. - 2019 - Scaling active inference.pdf;/Users/lancelotdacosta/Zotero/storage/947W6F8I/1911.html}
}



@article{fristonActiveInferenceAgency2012,
  title = {Active Inference and Agency: Optimal Control without Cost Functions},
  volume = {106},
  issn = {1432-0770},
  shorttitle = {Active Inference and Agency},
  abstract = {This paper describes a variational free-energy formulation of (partially observable) Markov decision problems in decision making under uncertainty. We show that optimal control can be cast as active inference. In active inference, both action and posterior beliefs about hidden states minimise a free energy bound on the negative log-likelihood of observed states, under a generative model. In this setting, reward or cost functions are absorbed into prior beliefs about state transitions and terminal states. Effectively, this converts optimal control into a pure inference problem, enabling the application of standard Bayesian filtering techniques. We then consider optimal trajectories that rest on posterior beliefs about hidden states in the future. Crucially, this entails modelling control as a hidden state that endows the generative model with a representation of agency. This leads to a distinction between models with and without inference on hidden control states; namely, agency-free and agency-based models, respectively.},
  language = {en},
  number = {8},
  journal = {Biological Cybernetics},
  doi = {10.1007/s00422-012-0512-8},
  author = {Friston, Karl and Samothrakis, Spyridon and Montague, Read},
  month = oct,
  year = {2012},
  keywords = {Bayesian,Free energy,Action,Agency,Inference,Optimal control,Partially observable Markov decision processes},
  pages = {523-541},
  file = {/Users/lancelotdacosta/Zotero/storage/IN8LS5QS/Friston et al. - 2012 - Active inference and agency optimal control witho.pdf;/Users/lancelotdacosta/Zotero/storage/JZYEWI56/Friston et al. - 2012 - Active inference and agency optimal control witho.pdf}
}

@article{parrPrecisionFalsePerceptual2018,
  title = {Precision and {{False Perceptual Inference}}},
  volume = {12},
  issn = {1662-5145},
  abstract = {Accurate perceptual inference fundamentally depends upon accurate beliefs about the reliability of sensory data. In this paper, we describe a Bayes optimal and biologically plausible scheme that refines these beliefs through a gradient descent on variational free energy. To illustrate this, we simulate belief updating during visual foraging and show that changes in estimated sensory precision (i.e. confidence in visual data) are highly sensitive to prior beliefs about the contents of a visual scene. In brief, confident prior beliefs induce an increase in estimated precision when consistent with sensory evidence, but a decrease when they conflict. Prior beliefs held with low confidence are rapidly updated to posterior beliefs, determined by sensory data. These induce much smaller changes in beliefs about sensory precision. We argue that pathologies of scene construction may be due to abnormal priors, and show that these can induce a reduction in estimated sensory precision. Having previously associated this precision with cholinergic signalling, we note that several neurodegenerative conditions are associated with visual disturbances and cholinergic deficits; notably, the synucleinopathies. On relating the message passing in our model to the functional anatomy of the ventral visual stream, we find that simulated neuronal loss in temporal lobe regions induces confident, inaccurate, empirical prior beliefs at lower levels in the visual hierarchy. This provides a plausible, if speculative, computational mechanism for the loss of cholinergic signalling and the visual disturbances associated with temporal lobe Lewy body pathology. This may be seen as an illustration of the sorts of hypotheses that may be expressed within this computational framework.},
  language = {English},
  journal = {Frontiers in Integrative Neuroscience},
  doi = {10.3389/fnint.2018.00039},
  author = {Parr, Thomas and Benrimoh, David A. and Vincent, Peter and Friston, Karl J.},
  year = {2018},
  keywords = {active inference,Precision,Saccades,Synucleinopathy,Visual System},
  file = {/Users/lancelotdacosta/Zotero/storage/ATLHF5DP/Parr et al. - 2018 - Precision and False Perceptual Inference.pdf}
}



@article{fitzgeraldActiveInferenceEvidence2015,
  title = {Active Inference, Evidence Accumulation, and the Urn Task},
  volume = {27},
  issn = {1530-888X},
  abstract = {Deciding how much evidence to accumulate before making a decision is a problem we and other animals often face, but one that is not completely understood. This issue is particularly important because a tendency to sample less information (often known as reflection impulsivity) is a feature in several psychopathologies, such as psychosis. A formal understanding of information sampling may therefore clarify the computational anatomy of psychopathology. In this theoretical letter, we consider evidence accumulation in terms of active (Bayesian) inference using a generic model of Markov decision processes. Here, agents are equipped with beliefs about their own behavior--in this case, that they will make informed decisions. Normative decision making is then modeled using variational Bayes to minimize surprise about choice outcomes. Under this scheme, different facets of belief updating map naturally onto the functional anatomy of the brain (at least at a heuristic level). Of particular interest is the key role played by the expected precision of beliefs about control, which we have previously suggested may be encoded by dopaminergic neurons in the midbrain. We show that manipulating expected precision strongly affects how much information an agent characteristically samples, and thus provides a possible link between impulsivity and dopaminergic dysfunction. Our study therefore represents a step toward understanding evidence accumulation in terms of neurobiologically plausible Bayesian inference and may cast light on why this process is disordered in psychopathology.},
  language = {eng},
  number = {2},
  journal = {Neural Computation},
  doi = {10.1162/NECO_a_00699},
  author = {FitzGerald, Thomas H. B. and Schwartenbeck, Philipp and Moutoussis, Michael and Dolan, Raymond J. and Friston, Karl},
  month = feb,
  year = {2015},
  keywords = {Computer Simulation,Humans,Cognition,Animals,Brain,Dopamine,Choice Behavior,Decision Making,Bayes Theorem,Entropy,Game Theory,Markov Chains,Models; Theoretical,Reaction Time},
  pages = {306-328},
  file = {/Users/lancelotdacosta/Zotero/storage/YQBFUTEG/FitzGerald et al. - 2015 - Active inference, evidence accumulation, and the u.pdf},
  pmid = {25514108},
  pmcid = {PMC4426890}
}

@article{fitzgeraldPrecisionNeuronalDynamics2015,
  title = {Precision and Neuronal Dynamics in the Human Posterior Parietal Cortex during Evidence Accumulation},
  volume = {107},
  issn = {1053-8119},
  abstract = {Primate studies show slow ramping activity in posterior parietal cortex (PPC) neurons during perceptual decision-making. These findings have inspired a rich theoretical literature to account for this activity. These accounts are largely unrelated to Bayesian theories of perception and predictive coding, a related formulation of perceptual inference in the cortical hierarchy. Here, we tested a key prediction of such hierarchical inference, namely that the estimated precision (reliability) of information ascending the cortical hierarchy plays a key role in determining both the speed of decision-making and the rate of increase of PPC activity. Using dynamic causal modelling of magnetoencephalographic (MEG) evoked responses, recorded during a simple perceptual decision-making task, we recover ramping-activity from an anatomically and functionally plausible network of regions, including early visual cortex, the middle temporal area (MT) and PPC. Precision, as reflected by the gain on pyramidal cell activity, was strongly correlated with both the speed of decision making and the slope of PPC ramping activity. Our findings indicate that the dynamics of neuronal activity in the human PPC during perceptual decision-making recapitulate those observed in the macaque, and in so doing we link observations from primate electrophysiology and human choice behaviour. Moreover, the synaptic gain control modulating these dynamics is consistent with predictive coding formulations of evidence accumulation.},
  journal = {NeuroImage},
  doi = {10.1016/j.neuroimage.2014.12.015},
  author = {FitzGerald, Thomas H. B. and Moran, Rosalyn J. and Friston, Karl J. and Dolan, Raymond J.},
  month = feb,
  year = {2015},
  pages = {219-228},
  file = {/Users/lancelotdacosta/Zotero/storage/LKU86WKT/FitzGerald et al. - 2015 - Precision and neuronal dynamics in the human poste.pdf;/Users/lancelotdacosta/Zotero/storage/F93MMPEC/S1053811914010076.html}
}

@article{schwartenbeckOptimalInferenceSuboptimal2015,
  title = {Optimal Inference with Suboptimal Models: Addiction and Active {{Bayesian}} Inference},
  volume = {84},
  issn = {1532-2777},
  shorttitle = {Optimal Inference with Suboptimal Models},
  abstract = {When casting behaviour as active (Bayesian) inference, optimal inference is defined with respect to an agent's beliefs - based on its generative model of the world. This contrasts with normative accounts of choice behaviour, in which optimal actions are considered in relation to the true structure of the environment - as opposed to the agent's beliefs about worldly states (or the task). This distinction shifts an understanding of suboptimal or pathological behaviour away from aberrant inference as such, to understanding the prior beliefs of a subject that cause them to behave less 'optimally' than our prior beliefs suggest they should behave. Put simply, suboptimal or pathological behaviour does not speak against understanding behaviour in terms of (Bayes optimal) inference, but rather calls for a more refined understanding of the subject's generative model upon which their (optimal) Bayesian inference is based. Here, we discuss this fundamental distinction and its implications for understanding optimality, bounded rationality and pathological (choice) behaviour. We illustrate our argument using addictive choice behaviour in a recently described 'limited offer' task. Our simulations of pathological choices and addictive behaviour also generate some clear hypotheses, which we hope to pursue in ongoing empirical work.},
  language = {eng},
  number = {2},
  journal = {Medical Hypotheses},
  doi = {10.1016/j.mehy.2014.12.007},
  author = {Schwartenbeck, Philipp and FitzGerald, Thomas H. B. and Mathys, Christoph and Dolan, Ray and Wurst, Friedrich and Kronbichler, Martin and Friston, Karl},
  month = feb,
  year = {2015},
  keywords = {Computer Simulation,Humans,Cognition,Choice Behavior,Decision Making,Models; Psychological,Bayes Theorem,Behavior; Addictive},
  pages = {109-117},
  file = {/Users/lancelotdacosta/Zotero/storage/AEER9N2A/Schwartenbeck et al. - 2015 - Optimal inference with suboptimal models addictio.pdf},
  pmid = {25561321},
  pmcid = {PMC4312353}
}

@article{fristonAnatomyChoiceDopamine2014,
  title = {The Anatomy of Choice: Dopamine and Decision-Making},
  volume = {369},
  issn = {0962-8436},
  shorttitle = {The Anatomy of Choice},
  abstract = {This paper considers goal-directed decision-making in terms of embodied or active inference. We associate bounded rationality with approximate Bayesian inference that optimizes a free energy bound on model evidence. Several constructs such as expected utility, exploration or novelty bonuses, softmax choice rules and optimism bias emerge as natural consequences of free energy minimization. Previous accounts of active inference have focused on predictive coding. In this paper, we consider variational Bayes as a scheme that the brain might use for approximate Bayesian inference. This scheme provides formal constraints on the computational anatomy of inference and action, which appear to be remarkably consistent with neuroanatomy. Active inference contextualizes optimal decision theory within embodied inference, where goals become prior beliefs. For example, expected utility theory emerges as a special case of free energy minimization, where the sensitivity or inverse temperature (associated with softmax functions and quantal response equilibria) has a unique and Bayes-optimal solution. Crucially, this sensitivity corresponds to the precision of beliefs about behaviour. The changes in precision during variational updates are remarkably reminiscent of empirical dopaminergic responses\textemdash{}and they may provide a new perspective on the role of dopamine in assimilating reward prediction errors to optimize decision-making.},
  number = {1655},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  doi = {10.1098/rstb.2013.0481},
  author = {Friston, Karl and Schwartenbeck, Philipp and FitzGerald, Thomas and Moutoussis, Michael and Behrens, Timothy and Dolan, Raymond J.},
  month = nov,
  year = {2014},
  file = {/Users/lancelotdacosta/Zotero/storage/6NYH4W7V/Friston et al. - 2014 - The anatomy of choice dopamine and decision-makin.pdf;/Users/lancelotdacosta/Zotero/storage/C4AIEKSY/Friston et al. - 2014 - The anatomy of choice dopamine and decision-makin.pdf},
  pmid = {25267823},
  pmcid = {PMC4186234}
}

@article{fitzgeraldDopamineRewardLearning2015,
  title = {Dopamine, Reward Learning, and Active Inference},
  volume = {9},
  issn = {1662-5188},
  abstract = {Temporal difference learning models propose phasic dopamine signaling encodes reward prediction errors that drive learning. This is supported by studies where optogenetic stimulation of dopamine neurons can stand in lieu of actual reward. Nevertheless, a large body of data also shows that dopamine is not necessary for learning, and that dopamine depletion primarily affects task performance. We offer a resolution to this paradox based on an hypothesis that dopamine encodes the precision of beliefs about alternative actions, and thus controls the outcome-sensitivity of behavior. We extend an active inference scheme for solving Markov decision processes to include learning, and show that simulated dopamine dynamics strongly resemble those actually observed during instrumental conditioning. Furthermore, simulated dopamine depletion impairs performance but spares learning, while simulated excitation of dopamine neurons drives reward learning, through aberrant inference about outcome states. Our formal approach provides a novel and parsimonious reconciliation of apparently divergent experimental findings.},
  journal = {Frontiers in Computational Neuroscience},
  doi = {10.3389/fncom.2015.00136},
  author = {FitzGerald, Thomas H. B. and Dolan, Raymond J. and Friston, Karl},
  month = nov,
  year = {2015},
  file = {/Users/lancelotdacosta/Zotero/storage/Z67S3H8D/FitzGerald et al. - 2015 - Dopamine, reward learning, and active inference.pdf},
  pmid = {26581305},
  pmcid = {PMC4631836}
}

@article{schwartenbeckDopaminergicMidbrainEncodes2015,
  title = {The {{Dopaminergic Midbrain Encodes}} the {{Expected Certainty}} about {{Desired Outcomes}}},
  volume = {25},
  issn = {1460-2199},
  abstract = {Dopamine plays a key role in learning; however, its exact function in decision making and choice remains unclear. Recently, we proposed a generic model based on active (Bayesian) inference wherein dopamine encodes the precision of beliefs about optimal policies. Put simply, dopamine discharges reflect the confidence that a chosen policy will lead to desired outcomes. We designed a novel task to test this hypothesis, where subjects played a "limited offer" game in a functional magnetic resonance imaging experiment. Subjects had to decide how long to wait for a high offer before accepting a low offer, with the risk of losing everything if they waited too long. Bayesian model comparison showed that behavior strongly supported active inference, based on surprise minimization, over classical utility maximization schemes. Furthermore, midbrain activity, encompassing dopamine projection neurons, was accurately predicted by trial-by-trial variations in model-based estimates of precision. Our findings demonstrate that human subjects infer both optimal policies and the precision of those inferences, and thus support the notion that humans perform hierarchical probabilistic Bayesian inference. In other words, subjects have to infer both what they should do as well as how confident they are in their choices, where confidence may be encoded by dopaminergic firing.},
  language = {eng},
  number = {10},
  journal = {Cerebral Cortex (New York, N.Y.: 1991)},
  doi = {10.1093/cercor/bhu159},
  author = {Schwartenbeck, Philipp and FitzGerald, Thomas H. B. and Mathys, Christoph and Dolan, Ray and Friston, Karl},
  month = oct,
  year = {2015},
  keywords = {active inference,Humans,Adult,Dopamine,Choice Behavior,Magnetic Resonance Imaging,Ventral Tegmental Area,Bayes Theorem,Brain Mapping,Cerebral Cortex,confidence,Conflict (Psychology),dopamine,Female,Male,Middle Aged,neuroeconomics,precision,Risk,Substantia Nigra,Young Adult},
  pages = {3434-3445},
  file = {/Users/lancelotdacosta/Zotero/storage/K9S42MAM/Schwartenbeck et al. - 2015 - The Dopaminergic Midbrain Encodes the Expected Cer.pdf},
  pmid = {25056572},
  pmcid = {PMC4585497}
}

@article{schwartenbeckEvidenceSurpriseMinimization2015,
  title = {Evidence for Surprise Minimization over Value Maximization in Choice Behavior},
  volume = {5},
  copyright = {2015 Nature Publishing Group},
  issn = {2045-2322},
  abstract = {Classical economic models are predicated on the idea that the ultimate aim of choice is to maximize utility or reward. In contrast, an alternative perspective highlights the fact that adaptive behavior requires agents' to model their environment and minimize surprise about the states they frequent. We propose that choice behavior can be more accurately accounted for by surprise minimization compared to reward or utility maximization alone. Minimizing surprise makes a prediction at variance with expected utility models; namely, that in addition to attaining valuable states, agents attempt to maximize the entropy over outcomes and thus `keep their options open'. We tested this prediction using a simple binary choice paradigm and show that human decision-making is better explained by surprise minimization compared to utility maximization. Furthermore, we replicated this entropy-seeking behavior in a control task with no explicit utilities. These findings highlight a limitation of purely economic motivations in explaining choice behavior and instead emphasize the importance of belief-based motivations.},
  language = {en},
  journal = {Scientific Reports},
  doi = {10.1038/srep16575},
  author = {Schwartenbeck, Philipp and FitzGerald, Thomas H. B. and Mathys, Christoph and Dolan, Ray and Kronbichler, Martin and Friston, Karl},
  month = nov,
  year = {2015},
  pages = {16575},
  file = {/Users/lancelotdacosta/Zotero/storage/YIXF9I7J/Schwartenbeck et al. - 2015 - Evidence for surprise minimization over value maxi.pdf;/Users/lancelotdacosta/Zotero/storage/A7RT55X2/srep16575.html}
}

@article{moutoussisFormalModelInterpersonal2014,
  title = {A Formal Model of Interpersonal Inference},
  volume = {8},
  issn = {1662-5161},
  abstract = {Introduction: We propose that active Bayesian inference\textemdash{}a general framework for decision-making\textemdash{}can equally be applied to interpersonal exchanges. Social cognition, however, entails special challenges. We address these challenges through a novel formulation of a formal model and demonstrate its psychological significance., Method: We review relevant literature, especially with regards to interpersonal representations, formulate a mathematical model and present a simulation study. The model accommodates normative models from utility theory and places them within the broader setting of Bayesian inference. Crucially, we endow people's prior beliefs, into which utilities are absorbed, with preferences of self and others. The simulation illustrates the model's dynamics and furnishes elementary predictions of the theory., Results: (1) Because beliefs about self and others inform both the desirability and plausibility of outcomes, in this framework interpersonal representations become beliefs that have to be actively inferred. This inference, akin to ``mentalizing'' in the psychological literature, is based upon the outcomes of interpersonal exchanges. (2) We show how some well-known social-psychological phenomena (e.g., self-serving biases) can be explained in terms of active interpersonal inference. (3) Mentalizing naturally entails Bayesian updating of how people value social outcomes. Crucially this includes inference about one's own qualities and preferences., Conclusion: We inaugurate a Bayes optimal framework for modeling intersubject variability in mentalizing during interpersonal exchanges. Here, interpersonal representations are endowed with explicit functional and affective properties. We suggest the active inference framework lends itself to the study of psychiatric conditions where mentalizing is distorted.},
  journal = {Frontiers in Human Neuroscience},
  doi = {10.3389/fnhum.2014.00160},
  author = {Moutoussis, Michael and {Trujillo-Barreto}, Nelson J. and {El-Deredy}, Wael and Dolan, Raymond J. and Friston, Karl J.},
  month = mar,
  year = {2014},
  file = {/Users/lancelotdacosta/Zotero/storage/XLV5W7UP/Moutoussis et al. - 2014 - A formal model of interpersonal inference.pdf},
  pmid = {24723872},
  pmcid = {PMC3971175}
}

@article{fitzgeraldModelAveragingOptimal2014,
  title = {Model Averaging, Optimal Inference, and Habit Formation},
  volume = {8},
  issn = {1662-5161},
  abstract = {Postulating that the brain performs approximate Bayesian inference generates principled and empirically testable models of neuronal function \textendash{} the subject of much current interest in neuroscience and related disciplines. Current formulations address inference and learning under some assumed and particular model. In reality, organisms are often faced with an additional challenge \textendash{} that of determining which model or models of their environment are the best for guiding behaviour. Bayesian model averaging \textendash{} which says that an agent should weight the predictions of different models according to their evidence \textendash{} provides a principled way to solve this problem. Importantly, because model evidence is determined by both the accuracy and complexity of the model, optimal inference requires that these be traded off against one another. This means an agent's behaviour should show an equivalent balance. We hypothesise that Bayesian model averaging plays an important role in cognition, given that it is both optimal and realisable within a plausible neuronal architecture. We outline model averaging and how it might be implemented, and then explore a number of implications for brain and behaviour. In particular, we propose that model averaging can explain a number of apparently suboptimal phenomena within the framework of approximate (bounded) Bayesian inference, focussing particularly upon the relationship between goal-directed and habitual behaviour.},
  language = {English},
  journal = {Frontiers in Human Neuroscience},
  doi = {10.3389/fnhum.2014.00457},
  author = {FitzGerald, Thomas H. B. and Dolan, Raymond J. and Friston, Karl J.},
  year = {2014},
  keywords = {active inference,predictive coding,Bayesian inference,bistable perception,bounded rationality,habit,Interference effects},
  file = {/Users/lancelotdacosta/Zotero/storage/WZZM3FI9/FitzGerald et al. - 2014 - Model averaging, optimal inference, and habit form.pdf}
}

@article{fristonFunctionalAnatomyTime2016,
  title = {The {{Functional Anatomy}} of {{Time}}: {{What}} and {{When}} in the {{Brain}}},
  volume = {20},
  issn = {1879-307X},
  shorttitle = {The {{Functional Anatomy}} of {{Time}}},
  abstract = {This Opinion article considers the implications for functional anatomy of how we represent temporal structure in our exchanges with the world. It offers a theoretical treatment that tries to make sense of the architectural principles seen in mammalian brains. Specifically, it considers a factorisation between representations of temporal succession and representations of content or, heuristically, a segregation into when and what. This segregation may explain the central role of the hippocampus in neuronal hierarchies while providing a tentative explanation for recent observations of how ordinal sequences are encoded. The implications for neuroanatomy and physiology may have something important to say about how self-organised cell assembly sequences enable the brain to exhibit purposeful behaviour that transcends the here and now.},
  language = {eng},
  number = {7},
  journal = {Trends in Cognitive Sciences},
  doi = {10.1016/j.tics.2016.05.001},
  author = {Friston, Karl and Buzs{\'a}ki, Gyorgy},
  month = jul,
  year = {2016},
  keywords = {Bayesian,Humans,Animals,Brain,Neuroanatomy,Brain Mapping,hippocampus,inference,ordinal,sequences,spatiotemporal},
  pages = {500-511},
  pmid = {27261057}
}

@article{parrComputationalNeuropsychologyBayesian2018,
  title = {Computational {{Neuropsychology}} and {{Bayesian Inference}}},
  volume = {12},
  issn = {1662-5161},
  abstract = {Computational theories of brain function have become very influential in neuroscience. They have facilitated the growth of formal approaches to disease, particularly in psychiatric research. In this paper, we provide a narrative review of the body of computational research addressing neuropsychological syndromes, and focus on those that employ Bayesian frameworks. Bayesian approaches to understanding brain function formulate perception and action as inferential processes. These inferences combine `prior' beliefs with a generative (predictive) model to explain the causes of sensations. Under this view, neuropsychological deficits can be thought of as false inferences that arise due to aberrant prior beliefs (that are poor fits to the real world). This draws upon the notion of a Bayes optimal pathology \textendash{} optimal inference with suboptimal priors \textendash{} and provides a means for computational phenotyping. In principle, any given neuropsychological disorder could be characterised by the set of prior beliefs that would make a patient's behaviour appear Bayes optimal. We start with an overview of some key theoretical constructs and use these to motivate a form of computational neuropsychology that relates anatomical structures in the brain to the computations they perform. Throughout, we draw upon computational accounts of neuropsychological syndromes. These are selected to emphasise the key features of a Bayesian approach, and the possible types of pathological prior that may be present. They range from visual neglect through hallucinations to autism. Through these illustrative examples, we review the use of Bayesian approaches to understand the link between biology and computation that is at the heart of neuropsychology.},
  language = {English},
  journal = {Frontiers in Human Neuroscience},
  doi = {10.3389/fnhum.2018.00061},
  author = {Parr, Thomas and Rees, Geraint and Friston, Karl J.},
  year = {2018},
  keywords = {active inference,predictive coding,Neuropsychology,Computational phenotyping,Precision},
  file = {/Users/lancelotdacosta/Zotero/storage/EV4SUGYT/Parr et al. - 2018 - Computational Neuropsychology and Bayesian Inferen.pdf}
}

@article{benrimohActiveInferenceAuditory2018,
  title = {Active {{Inference}} and {{Auditory Hallucinations}}},
  volume = {2},
  abstract = {Auditory verbal hallucinations (AVH) are often distressing symptoms of several neuropsychiatric conditions, including schizophrenia. Using a Markov decision process formulation of active inference, we develop a novel model of AVH as false (positive) inference. Active inference treats perception as a process of hypothesis testing, in which sensory data are used to disambiguate between alternative hypotheses about the world. Crucially, this depends upon a delicate balance between prior beliefs about unobserved (hidden) variables and the sensations they cause. A false inference that a voice is present, even in the absence of auditory sensations, suggests that prior beliefs dominate perceptual inference. Here we consider the computational mechanisms that could cause this imbalance in perception. Through simulation, we show that the content of (and confidence in) prior beliefs depends on beliefs about policies (here sequences of listening and talking) and on beliefs about the reliability of sensory data. We demonstrate several ways in which hallucinatory percepts could occur when an agent expects to hear a voice in the presence of imprecise sensory data. This model expresses, in formal terms, alternative computational mechanisms that underwrite AVH and, speculatively, can be mapped onto neurobiological changes associated with schizophrenia. The interaction of action and perception is important in modeling AVH, given that speech is a fundamentally enactive and interactive process\textemdash{}and that hallucinators often actively engage with their voices.},
  journal = {Computational Psychiatry},
  doi = {10.1162/cpsy_a_00022},
  author = {Benrimoh, David and Parr, Thomas and Vincent, Peter and Adams, Rick A. and Friston, Karl},
  month = nov,
  year = {2018},
  pages = {183-204},
  file = {/Users/lancelotdacosta/Zotero/storage/7EVPEY9H/Benrimoh et al. - 2018 - Active Inference and Auditory Hallucinations.pdf;/Users/lancelotdacosta/Zotero/storage/DNF27XIJ/cpsy_a_00022.html}
}

@article{salesLocusCoeruleusTracking2018,
  title = {Locus {{Coeruleus}} Tracking of Prediction Errors Optimises Cognitive Flexibility: An {{Active Inference}} Model},
  copyright = {\textcopyright{} 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  shorttitle = {Locus {{Coeruleus}} Tracking of Prediction Errors Optimises Cognitive Flexibility},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}The locus coeruleus (LC) in the pons is the major source of noradrenaline (NA) in the brain. Two modes of LC firing have been associated with distinct cognitive states: changes in tonic rates of firing are correlated with global levels of arousal and behavioural flexibility, whilst phasic LC responses are evoked by salient stimuli. Here, we unify these two modes of firing by modelling the response of the LC as a correlate of a prediction error when inferring states for action planning under Active Inference (AI).{$<$}/p{$><$}p{$>$}We simulate a classic Go/No-go reward learning task and a three-arm foraging task and show that, if LC activity is considered to reflect the magnitude of high level `state-action' prediction errors, then both tonic and phasic modes of firing are emergent features of belief updating. We also demonstrate that when contingencies change, AI agents can update their internal models more quickly by feeding back this state-action prediction error \textendash{} reflected in LC firing and noradrenaline release \textendash{} to optimise learning rate, enabling large adjustments over short timescales. We propose that such prediction errors are mediated by cortico-LC connections, whilst ascending input from LC to cortex modulates belief updating in anterior cingulate cortex (ACC).{$<$}/p{$><$}p{$>$}In short, we characterise the LC/ NA system within a general theory of brain function. In doing so, we show that contrasting, behaviour-dependent firing patterns are an emergent property of the LC's crucial role in translating prediction errors into an optimal mediation between plasticity and stability.{$<$}/p{$><$}h3{$>$}Author Summary{$<$}/h3{$>$} {$<$}p{$>$}The brain uses sensory information to build internal models and make predictions about the world. When errors of prediction occur, models must be updated to ensure desired outcomes are still achieved. Neuromodulator chemicals provide a possible pathway for triggering such changes in brain state. One such neuromodulator, noradrenaline, originates predominantly from a cluster of neurons in the brainstem - the locus coeruleus (LC) - and plays a key role in behaviour, for instance, in determining the balance between exploiting or exploring the environment.{$<$}/p{$><$}p{$>$}Here we use Active Inference (AI), a mathematical model of perception and action, to formally describe LC function. We propose that LC activity is triggered by errors in prediction and that the subsequent release of noradrenaline alters the rate of learning about the environment. Biologically, this describes an LC-cortex feedback loop promoting behavioural flexibility in times of uncertainty. We model LC output as a simulated animal performs two tasks known to elicit archetypal responses. We find that experimentally observed `phasic' and `tonic' patterns of LC activity emerge naturally, and that modulation of learning rates improves task performance. This provides a simple, unified computational account of noradrenergic computational function within a general model of behaviour.{$<$}/p{$>$}},
  language = {en},
  journal = {bioRxiv},
  doi = {10.1101/340620},
  author = {Sales, Anna C. and Friston, Karl J. and Jones, Matthew W. and Pickering, Anthony E. and Moran, Rosalyn J.},
  month = jun,
  year = {2018},
  pages = {340620},
  file = {/Users/lancelotdacosta/Zotero/storage/YIK46B2J/Sales et al. - 2018 - Locus Coeruleus tracking of prediction errors opti.pdf;/Users/lancelotdacosta/Zotero/storage/THTBZP9I/340620v1.html}
}

@article{vincentEyeUncertaintyModelling2019,
  title = {With an Eye on Uncertainty: {{Modelling}} Pupillary Responses to Environmental Volatility},
  volume = {15},
  issn = {1553-7358},
  shorttitle = {With an Eye on Uncertainty},
  abstract = {Living creatures must accurately infer the nature of their environments. They do this despite being confronted by stochastic and context sensitive contingencies\textemdash{}and so must constantly update their beliefs regarding their uncertainty about what might come next. In this work, we examine how we deal with uncertainty that evolves over time. This prospective uncertainty (or imprecision) is referred to as volatility and has previously been linked to noradrenergic signals that originate in the locus coeruleus. Using pupillary dilatation as a measure of central noradrenergic signalling, we tested the hypothesis that changes in pupil diameter reflect inferences humans make about environmental volatility. To do so, we collected pupillometry data from participants presented with a stream of numbers. We generated these numbers from a process with varying degrees of volatility. By measuring pupillary dilatation in response to these stimuli\textemdash{}and simulating the inferences made by an ideal Bayesian observer of the same stimuli\textemdash{}we demonstrate that humans update their beliefs about environmental contingencies in a Bayes optimal way. We show this by comparing general linear (convolution) models that formalised competing hypotheses about the causes of pupillary changes. We found greater evidence for models that included Bayes optimal estimates of volatility than those without. We additionally explore the interaction between different causes of pupil dilation and suggest a quantitative approach to characterising a person's prior beliefs about volatility.},
  language = {en},
  number = {7},
  journal = {PLOS Computational Biology},
  doi = {10.1371/journal.pcbi.1007126},
  author = {Vincent, Peter and Parr, Thomas and Benrimoh, David and Friston, Karl J.},
  year = {05-Jul-2019},
  keywords = {Free energy,Autism,Dynamical systems,Electroencephalography,Markov processes,Mathematical models,Sequence analysis,Simulation and modeling},
  pages = {e1007126},
  file = {/Users/lancelotdacosta/Zotero/storage/Z2NKS6ED/Vincent et al. - 2019 - With an eye on uncertainty Modelling pupillary re.pdf;/Users/lancelotdacosta/Zotero/storage/9SC8ZXQ9/article.html}
}

@article{sharotOptimismBias2011,
  title = {The Optimism Bias},
  volume = {21},
  issn = {0960-9822},
  abstract = {Summary
The ability to anticipate is a hallmark of cognition. Inferences about what will occur in the future are critical to decision making, enabling us to prepare our actions so as to avoid harm and gain reward. Given the importance of these future projections, one might expect the brain to possess accurate, unbiased foresight. Humans, however, exhibit a pervasive and surprising bias: when it comes to predicting what will happen to us tomorrow, next week, or fifty years from now, we overestimate the likelihood of positive events, and underestimate the likelihood of negative events. For example, we underrate our chances of getting divorced, being in a car accident, or suffering from cancer. We also expect to live longer than objective measures would warrant, overestimate our success in the job market, and believe that our children will be especially talented. This phenomenon is known as the optimism bias, and it is one of the most consistent, prevalent, and robust biases documented in psychology and behavioral economics.},
  number = {23},
  journal = {Current Biology},
  doi = {10.1016/j.cub.2011.10.030},
  author = {Sharot, Tali},
  month = dec,
  year = {2011},
  pages = {R941-R945},
  file = {/Users/lancelotdacosta/Zotero/storage/CH7KC2GA/Sharot - 2011 - The optimism bias.pdf;/Users/lancelotdacosta/Zotero/storage/IXNZTZ9G/S0960982211011912.html}
}

@article{mckayEvolutionMisbelief2009,
  title = {The Evolution of Misbelief},
  volume = {32},
  issn = {1469-1825},
  abstract = {From an evolutionary standpoint, a default presumption is that true beliefs are adaptive and misbeliefs maladaptive. But if humans are biologically engineered to appraise the world accurately and to form true beliefs, how are we to explain the routine exceptions to this rule? How can we account for mistaken beliefs, bizarre delusions, and instances of self-deception? We explore this question in some detail. We begin by articulating a distinction between two general types of misbelief: those resulting from a breakdown in the normal functioning of the belief formation system (e.g., delusions) and those arising in the normal course of that system's operations (e.g., beliefs based on incomplete or inaccurate information). The former are instances of biological dysfunction or pathology, reflecting "culpable" limitations of evolutionary design. Although the latter category includes undesirable (but tolerable) by-products of "forgivably" limited design, our quarry is a contentious subclass of this category: misbeliefs best conceived as design features. Such misbeliefs, unlike occasional lucky falsehoods, would have been systematically adaptive in the evolutionary past. Such misbeliefs, furthermore, would not be reducible to judicious - but doxastically noncommittal - action policies. Finally, such misbeliefs would have been adaptive in themselves, constituting more than mere by-products of adaptively biased misbelief-producing systems. We explore a range of potential candidates for evolved misbelief, and conclude that, of those surveyed, only positive illusions meet our criteria.},
  language = {eng},
  number = {6},
  journal = {The Behavioral and Brain Sciences},
  doi = {10.1017/S0140525X09990975},
  author = {McKay, Ryan T. and Dennett, Daniel C.},
  month = dec,
  year = {2009},
  keywords = {Humans,Adaptation; Psychological,Biological Evolution,Culture,Deception,Delusions,Psychological Theory,Religion and Psychology},
  pages = {493-510; discussion 510-561},
  pmid = {20105353}
}

@article{gregoryPerceptionsHypotheses1980,
  title = {Perceptions as Hypotheses},
  volume = {290},
  issn = {0962-8436},
  abstract = {Perceptions may be compared with hypotheses in science. The methods of acquiring scientific knowledge provide a working paradigm for investigating processes of perception. Much as the information channels of instruments, such as radio telescopes, transmit signals which are processed according to various assumptions to give useful data, so neural signals are processed to give data for perception. To understand perception, the signal codes and the stored knowledge or assumptions used for deriving perceptual hypotheses must be discovered. Systematic  perceptual errors are important clues for appreciating signal channel limitations, and for discovering hypothesis-generating procedures. Although this distinction between 'physiological' and 'cognitive' aspects of perception may be logically clear, it is in practice surprisingly difficult to establish which are responsible even for clearly established phenomena such as the classical distortion illusions. Experimental results are presented, aimed at distinguishing between and disconvering what happens when there is mismatch with the neural signal channel, and when neural signals are processed inappropriately for the current situation. This leads us to make some distinctions between perceptual and scientific hypotheses, which raise in a new form the problem: What are 'objects'?},
  language = {eng},
  number = {1038},
  journal = {Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences},
  doi = {10.1098/rstb.1980.0090},
  author = {Gregory, R. L.},
  month = jul,
  year = {1980},
  keywords = {Science,Humans,Perception,Research Design},
  pages = {181-197},
  pmid = {6106237}
}

@article{ittiBayesianSurpriseAttracts2009,
  title = {Bayesian Surprise Attracts Human Attention},
  volume = {49},
  issn = {0042-6989},
  abstract = {We propose a formal Bayesian definition of surprise to capture subjective aspects of sensory information. Surprise measures how data affects an observer, in terms of differences between posterior and prior beliefs about the world. Only data observations which substantially affect the observer's beliefs yield surprise, irrespectively of how rare or informative in Shannon's sense these observations are. We test the framework by quantifying the extent to which humans may orient attention and gaze towards surprising events or items while watching television. To this end, we implement a simple computational model where a low-level, sensory form of surprise is computed by simple simulated early visual neurons. Bayesian surprise is a strong attractor of human attention, with 72\% of all gaze shifts directed towards locations more surprising than the average, a figure rising to 84\% when focusing the analysis onto regions simultaneously selected by all observers. The proposed theory of surprise is applicable across different spatio-temporal scales, modalities, and levels of abstraction.},
  number = {10},
  journal = {Vision research},
  doi = {10.1016/j.visres.2008.09.007},
  author = {Itti, Laurent and Baldi, Pierre},
  month = may,
  year = {2009},
  keywords = {Attention,Eye Movements,Humans,Photic Stimulation,Adult,Psychomotor Performance,Exploratory Behavior,Models; Psychological,Bayes Theorem,Female,Male,Young Adult,Psychophysics,Television},
  pages = {1295-1306},
  file = {/Users/lancelotdacosta/Zotero/storage/9YLEGMUU/Itti and Baldi - 2009 - Bayesian surprise attracts human attention.pdf;/Users/lancelotdacosta/Zotero/storage/VN93CH68/Itti and Baldi - 2009 - Bayesian surprise attracts human attention.pdf},
  pmid = {18834898},
  pmcid = {PMC2782645}
}

@article{brownActiveInferenceSensory2013,
  title = {Active Inference, Sensory Attenuation and Illusions},
  volume = {14},
  issn = {1612-4790},
  abstract = {Active inference provides a simple and neurobiologically plausible account of how action and perception are coupled in producing (Bayes) optimal behaviour. This can be seen most easily as minimising prediction error: we can either change our predictions to explain sensory input through perception. Alternatively, we can actively change sensory input to fulfil our predictions. In active inference, this action is mediated by classical reflex arcs that minimise proprioceptive prediction error created by descending proprioceptive predictions. However, this creates a conflict between action and perception; in that, self-generated movements require predictions to override the sensory evidence that one is not actually moving. However, ignoring sensory evidence means that externally generated sensations will not be perceived. Conversely, attending to (proprioceptive and somatosensory) sensations enables the detection of externally generated events but precludes generation of actions. This conflict can be resolved by attenuating the precision of sensory evidence during movement or, equivalently, attending away from the consequences of self-made acts. We propose that this Bayes optimal withdrawal of precise sensory evidence during movement is the cause of psychophysical sensory attenuation. Furthermore, it explains the force-matching illusion and reproduces empirical results almost exactly. Finally, if attenuation is removed, the force-matching illusion disappears and false (delusional) inferences about agency emerge. This is important, given the negative correlation between sensory attenuation and delusional beliefs in normal subjects--and the reduction in the magnitude of the illusion in schizophrenia. Active inference therefore links the neuromodulatory optimisation of precision to sensory attenuation and illusory phenomena during the attribution of agency in normal subjects. It also provides a functional account of deficits in syndromes characterised by false inference and impaired movement--like schizophrenia and Parkinsonism--syndromes that implicate abnormal modulatory neurotransmission.},
  language = {eng},
  number = {4},
  journal = {Cognitive Processing},
  doi = {10.1007/s10339-013-0571-3},
  author = {Brown, Harriet and Adams, Rick A. and Parees, Isabel and Edwards, Mark and Friston, Karl},
  month = nov,
  year = {2013},
  keywords = {Attention,Humans,Behavior,Algorithms,Bayes Theorem,Illusions,Psychophysics,Sensation},
  pages = {411-427},
  file = {/Users/lancelotdacosta/Zotero/storage/UP2E9Q2K/Brown et al. - 2013 - Active inference, sensory attenuation and illusion.pdf},
  pmid = {23744445},
  pmcid = {PMC3824582}
}

@article{adamsPredictionsNotCommands2013,
  title = {Predictions Not Commands: Active Inference in the Motor System},
  volume = {218},
  issn = {1863-2653},
  shorttitle = {Predictions Not Commands},
  abstract = {The descending projections from motor cortex share many features with top-down or backward connections in visual cortex; for example, corticospinal projections originate in infragranular layers, are highly divergent and (along with descending cortico-cortical projections) target cells expressing NMDA receptors. This is somewhat paradoxical because backward modulatory characteristics would not be expected of driving motor command signals. We resolve this apparent paradox using a functional characterisation of the motor system based on Helmholtz's ideas about perception; namely, that perception is inference on the causes of visual sensations. We explain behaviour in terms of inference on the causes of proprioceptive sensations. This explanation appeals to active inference, in which higher cortical levels send descending proprioceptive predictions, rather than motor commands. This process mirrors perceptual inference in sensory cortex, where descending connections convey predictions, while ascending connections convey prediction errors. The anatomical substrate of this recurrent message passing is a hierarchical system consisting of functionally asymmetric driving (ascending) and modulatory (descending) connections: an arrangement that we show is almost exactly recapitulated in the motor system, in terms of its laminar, topographic and physiological characteristics. This perspective casts classical motor reflexes as minimising prediction errors and may provide a principled explanation for why motor cortex is agranular.},
  number = {3},
  journal = {Brain Structure \& Function},
  doi = {10.1007/s00429-012-0475-5},
  author = {Adams, Rick A. and Shipp, Stewart and Friston, Karl J.},
  month = may,
  year = {2013},
  pages = {611-643},
  file = {/Users/lancelotdacosta/Zotero/storage/VYVIP25L/Adams et al. - 2013 - Predictions not commands active inference in the .pdf},
  pmid = {23129312},
  pmcid = {PMC3637647}
}

@article{fristonPerceptionsHypothesesSaccades2012,
  title = {Perceptions as {{Hypotheses}}: {{Saccades}} as {{Experiments}}},
  volume = {3},
  issn = {1664-1078},
  shorttitle = {Perceptions as {{Hypotheses}}},
  abstract = {If perception corresponds to hypothesis testing (Gregory, 1980); then visual searches might be construed as experiments that generate sensory data. In this work, we explore the idea that saccadic eye movements are optimal experiments, in which data are gathered to test hypotheses or beliefs about how those data are caused. This provides a plausible model of visual search that can be motivated from the basic principles of self-organised behaviour: namely, the imperative to minimise the entropy of hidden states of the world and their sensory consequences. This imperative is met if agents sample hidden states of the world efficiently. This efficient sampling of salient information can be derived in a fairly straightforward way, using approximate Bayesian inference and variational free energy minimisation. Simulations of the resulting active inference scheme reproduce sequential eye movements that are reminiscent of empirically observed saccades and provide some counterintuitive insights into the way that sensory evidence is accumulated or simulated into beliefs about the world.},
  language = {English},
  journal = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2012.00151},
  author = {Friston, Karl and Adams, Rick and Perrinet, Laurent and Breakspear, Michael},
  year = {2012},
  keywords = {Bayesian,active inference,free energy,visual search,exploration,prediction,salience,surprise},
  file = {/Users/lancelotdacosta/Zotero/storage/LSJV3NF9/Friston et al. - 2012 - Perceptions as Hypotheses Saccades as Experiments.pdf}
}

@article{brownFreeEnergyIllusionsCornsweet2012,
  title = {Free-{{Energy}} and {{Illusions}}: {{The Cornsweet Effect}}},
  volume = {3},
  issn = {1664-1078},
  shorttitle = {Free-{{Energy}} and {{Illusions}}},
  abstract = {In this paper, we review the nature of illusions using the free-energy formulation of Bayesian perception. We reiterate the notion that illusory percepts are, in fact, Bayes-optimal and represent the most likely explanation for ambiguous sensory input. This point is illustrated using perhaps the simplest of visual illusions; namely, the Cornsweet effect. By using plausible prior beliefs about the spatial gradients of luminance and reflectance in the visual world, we show that the Cornsweet effect emerges as a natural consequence of Bayes-optimal perception. Furthermore, we were able to simulate the appearance of secondary illusory percepts (Mach bands) as a function of stimulus contrast. The contrast-dependent emergence of the Cornsweet effect and subsequent appearance of Mach bands were simulated using a simple but plausible generative model. We verified the qualitative and quantitative predictions of this model psychophysically, using briefly presented stimuli at different contrast levels and a fixed alternative forced choice paradigm. Because our generative model was inverted using a neurobiologically plausible scheme based on generalised predictive coding, we could use the inversion as a simulation of neuronal inference and processing. This allowed us to simulate event related potentials. We hope to use this inversion scheme as a model of empirical electromagnetic responses in future work; to associate the functional architecture of the generative model with neuronal responses in visual cortex.},
  language = {English},
  journal = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2012.00043},
  author = {Brown, Harriet and Friston, Karl J.},
  year = {2012},
  keywords = {predictive coding,Perception,Bayesian inference,Illusions,Cornsweet effect,Craik-O'Brien-Cornsweet illusion,free-energy,Perceptual priors},
  file = {/Users/lancelotdacosta/Zotero/storage/5EG5L5T2/Brown and Friston - 2012 - Free-Energy and Illusions The Cornsweet Effect.pdf}
}

@article{ackleyLearningAlgorithmBoltzmann1985,
  title = {A Learning Algorithm for Boltzmann Machines},
  volume = {9},
  issn = {0364-0213},
  abstract = {The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.},
  number = {1},
  journal = {Cognitive Science},
  doi = {10.1016/S0364-0213(85)80012-4},
  author = {Ackley, David H. and Hinton, Geoffrey E. and Sejnowski, Terrence J.},
  month = jan,
  year = {1985},
  pages = {147-169},
  file = {/Users/lancelotdacosta/Zotero/storage/NBZDIX6H/Ackley et al. - 1985 - A learning algorithm for boltzmann machines.pdf;/Users/lancelotdacosta/Zotero/storage/GAFC2ASZ/S0364021385800124.html}
}

@article{catalBayesianPolicySelection2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1904.08149},
  primaryClass = {cs},
  title = {Bayesian Policy Selection Using Active Inference},
  abstract = {Learning to take actions based on observations is a core requirement for artificial agents to be able to be successful and robust at their task. Reinforcement Learning (RL) is a well-known technique for learning such policies. However, current RL algorithms often have to deal with reward shaping, have difficulties generalizing to other environments and are most often sample inefficient. In this paper, we explore active inference and the free energy principle, a normative theory from neuroscience that explains how self-organizing biological systems operate by maintaining a model of the world and casting action selection as an inference problem. We apply this concept to a typical problem known to the RL community, the mountain car problem, and show how active inference encompasses both RL and learning from demonstrations.},
  journal = {arXiv:1904.08149 [cs]},
  author = {{\c C}atal, Ozan and Nauta, Johannes and Verbelen, Tim and Simoens, Pieter and Dhoedt, Bart},
  month = apr,
  year = {2019},
  keywords = {Computer Science - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/lancelotdacosta/Zotero/storage/9M2BUQA4/Çatal et al. - 2019 - Bayesian policy selection using active inference.pdf;/Users/lancelotdacosta/Zotero/storage/7VEUA7ZN/1904.html}
}

@misc{KlDivergenceTwo,
  title = {Kl Divergence between Two Dirichlet Distributions - {{Google Search}}},
  howpublished = {https://www.google.com/search?q=kl+divergence+between+two+dirichlet+distributions\&oq=kl+\&aqs=chrome.1.69i57j69i59l4j69i60.2165j0j7\&sourceid=chrome\&ie=UTF-8},
  file = {/Users/lancelotdacosta/Zotero/storage/YY2ERKKC/search.html}
}

@article{salakhutdinovEfficientLearningProcedure2012,
  title = {An Efficient Learning Procedure for Deep {{Boltzmann}} Machines},
  volume = {24},
  issn = {1530-888X},
  abstract = {We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent statistics are estimated using a variational approximation that tends to focus on a single mode, and data-independent statistics are estimated using persistent Markov chains. The use of two quite different techniques for estimating the two types of statistic that enter into the gradient of the log likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer pretraining phase that initializes the weights sensibly. The pretraining also allows the variational inference to be initialized sensibly with a single bottom-up pass. We present results on the MNIST and NORB data sets showing that deep Boltzmann machines learn very good generative models of handwritten digits and 3D objects. We also show that the features discovered by deep Boltzmann machines are a very effective way to initialize the hidden layers of feedforward neural nets, which are then discriminatively fine-tuned.},
  language = {eng},
  number = {8},
  journal = {Neural Computation},
  doi = {10.1162/NECO_a_00311},
  author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
  month = aug,
  year = {2012},
  pages = {1967-2006},
  pmid = {22509963}
}

@article{bauckhageComputingKullbackLeiblerDivergence2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1401.6853},
  primaryClass = {cs, math},
  title = {Computing the {{Kullback}}-{{Leibler Divergence}} between Two {{Generalized Gamma Distributions}}},
  abstract = {We derive a closed form solution for the Kullback-Leibler divergence between two generalized gamma distributions. These notes are meant as a reference and provide a guided tour towards a result of practical interest that is rarely explicated in the literature.},
  journal = {arXiv:1401.6853 [cs, math]},
  author = {Bauckhage, Christian},
  month = jan,
  year = {2014},
  keywords = {Computer Science - Information Theory},
  file = {/Users/lancelotdacosta/Zotero/storage/WEEXTDL7/Bauckhage - 2014 - Computing the Kullback-Leibler Divergence between .pdf;/Users/lancelotdacosta/Zotero/storage/FNMVB68I/1401.html}
}

@unpublished{pennyKLdivergenceNormalGamma2001,
  title = {{{KL}}-Divergence of {{Normal}}, {{Gamma}}, {{Dirichlet}} and {{Wishart}} Densitites},
  author = {Penny, W.D.},
  year = {2001}
}

@book{jordanHiddenMarkovDecision1997,
  title = {Hidden {{Markov}} Decision Trees},
  abstract = {We study a time series model that can be viewed as a decision tree with Markov temporal structure. The model is intractable for exact calculations, thus we utilize variational approximations. We consider three different distributions for the approximation: one in which the Markov calculations are performed exactly and the layers of the decision tree are decoupled, one in which the decision tree calculations are performed exactly and the time steps of the Markov chain are decoupled, and one in which a Viterbi-like assumption is made to pick out a single most likely state sequence. We present simulation results for artificial data and the Bach chorales. Accepted for oral presentation at NIPS*96. 1 Introduction  Decision trees are regression or classification models that are based on a nested decomposition of the input space. An input vector x is classified recursively by a set of "decisions" at the nonterminal nodes of a tree, resulting in the choice of a terminal node at which an output...},
  author = {Jordan, Michael I. and Ghahramani, Zoubin and Saul, Lawrence K.},
  year = {1997},
  file = {/Users/lancelotdacosta/Zotero/storage/YQ8RJ9WZ/Jordan et al. - 1997 - Hidden Markov decision trees.pdf;/Users/lancelotdacosta/Zotero/storage/C3GAKBYC/summary.html}
}

@article{eichenbaumHippocampusMemoryPlace1999,
  title = {The {{Hippocampus}}, {{Memory}}, and {{Place Cells}}: {{Is It Spatial Memory}} or a {{Memory Space}}?},
  volume = {23},
  issn = {0896-6273},
  shorttitle = {The {{Hippocampus}}, {{Memory}}, and {{Place Cells}}},
  language = {English},
  number = {2},
  journal = {Neuron},
  doi = {10.1016/S0896-6273(00)80773-4},
  author = {Eichenbaum, Howard and Dudchenko, Paul and Wood, Emma and Shapiro, Matthew and Tanila, Heikki},
  month = jun,
  year = {1999},
  pages = {209-226},
  file = {/Users/lancelotdacosta/Zotero/storage/U6CY7JY7/Eichenbaum et al. - 1999 - The Hippocampus, Memory, and Place Cells Is It Sp.pdf;/Users/lancelotdacosta/Zotero/storage/MT5SGDFH/S0896-6273(00)80773-4.html},
  pmid = {10399928}
}

@article{okeefePlaceUnitsHippocampus1976,
  title = {Place Units in the Hippocampus of the Freely Moving Rat},
  volume = {51},
  issn = {0014-4886},
  abstract = {Single units were recorded from the CA1 field of the hippocampus in the freely-moving rat. They were classified as place units, displace units or others. Place units were defined as those for which the rat's position on the maze was a necessary condition for maximal unit firing. Some of these place units (misplace units) fired maximally when the animal sniffed in a place, either because it found something new there or failed to find something which was usually there. Displace units increased their rates during behaviors associated with theta activity in the hippocampal slow waves. In general these were behaviors which changed the rat's position relative to the environment. The influence of various environmental manipulations (e.g., turning off the room lights) on the firing pattern of the place units was tested and the results suggest that they were not responding to a simple sensory stimulus nor to a specific motor behavior. Nor could the unit firing be due purely to motivational or incentive factors. The results are interpreted as strong support for the cognitive map theory of hippocampal function.},
  number = {1},
  journal = {Experimental Neurology},
  doi = {10.1016/0014-4886(76)90055-8},
  author = {O'Keefe, John},
  month = jan,
  year = {1976},
  pages = {78-109},
  file = {/Users/lancelotdacosta/Zotero/storage/IKSZNFVH/0014488676900558.html}
}

@article{iglesiasHierarchicalPredictionErrors2013,
  title = {Hierarchical {{Prediction Errors}} in {{Midbrain}} and {{Basal Forebrain}} during {{Sensory Learning}}},
  volume = {80},
  issn = {0896-6273},
  abstract = {Summary
In Bayesian brain theories, hierarchically related prediction errors (PEs) play a central role for predicting sensory inputs and inferring their underlying causes, e.g., the probabilistic structure of the environment and its volatility. Notably, PEs at different hierarchical levels may be encoded by different neuromodulatory transmitters. Here, we tested this possibility in computational fMRI studies of audio-visual learning. Using a hierarchical Bayesian model, we found that low-level PEs about visual stimulus outcome were reflected by widespread activity in visual and supramodal areas but also in the midbrain. In contrast, high-level PEs about stimulus probabilities were encoded by the basal forebrain. These findings were replicated in two groups of healthy volunteers. While our fMRI measures do not reveal the exact neuron types activated in midbrain and basal forebrain, they suggest a dichotomy between neuromodulatory systems, linking dopamine to low-level PEs about stimulus outcome and acetylcholine to more abstract PEs about stimulus probabilities.
Video Abstract},
  number = {2},
  journal = {Neuron},
  doi = {10.1016/j.neuron.2013.09.009},
  author = {Iglesias, Sandra and Mathys, Christoph and Brodersen, Kay H. and Kasper, Lars and Piccirelli, Marco and {den Ouden}, Hanneke E. M. and Stephan, Klaas E.},
  month = oct,
  year = {2013},
  pages = {519-530},
  file = {/Users/lancelotdacosta/Zotero/storage/MSLPR42Y/Iglesias et al. - 2013 - Hierarchical Prediction Errors in Midbrain and Bas.pdf;/Users/lancelotdacosta/Zotero/storage/XM47N444/S0896627313008076.html}
}

@article{chaoLargeScaleCorticalNetworks2018,
  title = {Large-{{Scale Cortical Networks}} for {{Hierarchical Prediction}} and {{Prediction Error}} in the {{Primate Brain}}},
  volume = {100},
  issn = {1097-4199},
  abstract = {According to predictive-coding theory, cortical areas continuously generate and update predictions of sensory inputs at different hierarchical levels and emit prediction errors when the predicted and actual inputs differ. However, predictions and prediction errors are simultaneous and interdependent processes, making it difficult to disentangle their constituent neural network organization. Here, we test the theory by using high-density electrocorticography (ECoG) in monkeys during an auditory "local-global" paradigm in which the temporal regularities of the stimuli were controlled at two hierarchical levels. We decomposed the broadband data and identified lower- and higher-level prediction-error signals in early auditory cortex and anterior temporal cortex, respectively, and a prediction-update signal sent from prefrontal cortex back to temporal cortex. The prediction-error and prediction-update signals were transmitted via {$\gamma$} ({$>$}40~Hz) and {$\alpha$}/{$\beta$} ({$<$}30~Hz) oscillations, respectively. Our findings provide strong support for hierarchical predictive coding and outline how it is dynamically implemented using distinct cortical areas and frequencies.},
  language = {eng},
  number = {5},
  journal = {Neuron},
  doi = {10.1016/j.neuron.2018.10.004},
  author = {Chao, Zenas C. and Takaura, Kana and Wang, Liping and Fujii, Naotaka and Dehaene, Stanislas},
  month = may,
  year = {2018},
  keywords = {Neural Pathways,Time Factors,Animals,Models; Neurological,Prefrontal Cortex,Male,Acoustic Stimulation,Auditory Cortex,Auditory Perception,Brain Waves,Electrocorticography,Evoked Potentials; Auditory,Macaca,Temporal Lobe},
  pages = {1252-1266.e3},
  pmid = {30482692}
}

@article{lundqvistGammaBetaBursts2016,
  title = {Gamma and {{Beta Bursts Underlie Working Memory}}},
  volume = {90},
  issn = {1097-4199},
  abstract = {Working memory is thought to result from sustained neuron spiking. However, computational models suggest complex dynamics with discrete oscillatory bursts. We analyzed local field potential (LFP) and spiking from the prefrontal cortex (PFC) of monkeys performing a working memory task. There were brief~bursts of narrow-band gamma oscillations (45-100~Hz), varied in time and frequency, accompanying encoding and re-activation of sensory information. They appeared at a minority of recording sites associated with spiking reflecting the to-be-remembered items. Beta oscillations (20-35~Hz) also occurred in brief, variable bursts but reflected a default state interrupted by encoding and decoding. Only activity of neurons reflecting encoding/decoding correlated with changes in gamma burst rate. Thus, gamma bursts could gate access to, and prevent sensory interference with, working memory. This supports the hypothesis that working memory is manifested by discrete oscillatory dynamics and spiking, not sustained activity.},
  language = {eng},
  number = {1},
  journal = {Neuron},
  doi = {10.1016/j.neuron.2016.02.028},
  author = {Lundqvist, Mikael and Rose, Jonas and Herman, Pawel and Brincat, Scott L. and Buschman, Timothy J. and Miller, Earl K.},
  month = apr,
  year = {2016},
  keywords = {Memory; Short-Term,Task Performance and Analysis,Neurons,Animals,Brain,Prefrontal Cortex,Electroencephalography,Beta Rhythm,Gamma Rhythm,Macaca fascicularis,Macaca mulatta},
  pages = {152-164},
  file = {/Users/lancelotdacosta/Zotero/storage/FXK3MZRE/Lundqvist et al. - 2016 - Gamma and Beta Bursts Underlie Working Memory.pdf},
  pmid = {26996084},
  pmcid = {PMC5220584}
}

@article{hillenbrandAcousticCharacteristicsAmerican1995,
  title = {Acoustic Characteristics of {{American English}} Vowels},
  volume = {97},
  issn = {0001-4966},
  abstract = {The purpose of this study was to replicate and extend the classic study of vowel acoustics by Peterson and Barney (PB) [J. Acoust. Soc. Am. 24, 175-184 (1952)]. Recordings were made of 45 men, 48 women, and 46 children producing the vowels /i,I,e, epsilon,ae,a, [symbol: see text],O,U,u, lambda,3 iota/ in h-V-d syllables. Formant contours for F1-F4 were measured from LPC spectra using a custom interactive editing tool. For comparison with the PB data, formant patterns were sampled at a time that was judged by visual inspection to be maximally steady. Analysis of the formant data shows numerous differences between the present data and those of PB, both in terms of average frequencies of F1 and F2, and the degree of overlap among adjacent vowels. As with the original study, listening tests showed that the signals were nearly always identified as the vowel intended by the talker. Discriminant analysis showed that the vowels were more poorly separated than the PB data based on a static sample of the formant pattern. However, the vowels can be separated with a high degree of accuracy if duration and spectral change information is included.},
  language = {eng},
  number = {5 Pt 1},
  journal = {The Journal of the Acoustical Society of America},
  doi = {10.1121/1.411872},
  author = {Hillenbrand, J. and Getty, L. A. and Clark, M. J. and Wheeler, K.},
  month = may,
  year = {1995},
  keywords = {Humans,Adult,Female,Male,Child,Phonetics,Speech Acoustics},
  pages = {3099-3111},
  pmid = {7759650}
}

@incollection{remezSpokenExpressionIndividual2010,
  title = {Spoken {{Expression}} of {{Individual Identity}} and the Listener},
  booktitle = {Expressing Oneself/Expressing One's Self: {{Communication}}, Cognition, Language, and Identity},
  author = {Remez, Robert E.},
  year = {2010},
  pages = {167-181}
}

@article{banzigerRoleIntonationEmotional2005,
  title = {The Role of Intonation in Emotional Expressions},
  volume = {46},
  issn = {1872-7182(Electronic),0167-6393(Print)},
  abstract = {The influence of emotions on intonation patterns (more specifically F0/pitch contours) is addressed in this article. A number of authors have claimed that specific intonation patterns reflect specific emotions, whereas others have found little evidence supporting this claim and argued that F0 pitch and other vocal aspects are continuously, rather than categorically, affected by emotions and/or emotional arousal. In this contribution, a new coding system for the assessment of F0 contours in emotion portrayals is presented. Results obtained for actor portrayed emotional expressions show that mean level and range of F0 in the contours vary strongly as a function of the degree of activation of the portrayed emotions. In contrast, there was comparatively little evidence for qualitatively different contour shapes for different emotions. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  number = {3-4},
  journal = {Speech Communication},
  doi = {10.1016/j.specom.2005.02.016},
  author = {B{\"a}nziger, Tanja and Scherer, Klaus R.},
  year = {2005},
  keywords = {Emotional States,Inflection,Speech Pitch},
  pages = {252-267},
  file = {/Users/lancelotdacosta/Zotero/storage/6MMMWH37/Bänziger and Scherer - 2005 - The role of intonation in emotional expressions.pdf;/Users/lancelotdacosta/Zotero/storage/NNSMS6FQ/2005-08508-003.html}
}

@article{mannInfluencePrecedingLiquid1980,
  title = {Influence of Preceding Liquid on Stop-Consonant Perception},
  volume = {28},
  issn = {1532-5962(Electronic),0031-5117(Print)},
  abstract = {The CV portions of natural tokens of [al-da], [al-ga], [ar-da], [ar-ga] were excised and replaced with closely matched synthetic stimuli drawn from a [da]-[ga] continuum. The resulting hybrid disyllables were then presented to 9 listeners who labeled both liquids and stops. The hypothesis that each of the 2 perceptual effects finds a parallel in speech production is supported by spectrograms of the original utterances. Thus, speech perception appears to reflect compensation for coarticulation during speech production. (8 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  number = {5},
  journal = {Perception \& Psychophysics},
  doi = {10.3758/BF03204884},
  author = {Mann, Virginia A.},
  year = {1980},
  keywords = {Consonants,Speech Perception,Syllables},
  pages = {407-412},
  file = {/Users/lancelotdacosta/Zotero/storage/GW5Z96WE/Mann - 1980 - Influence of preceding liquid on stop-consonant pe.pdf;/Users/lancelotdacosta/Zotero/storage/X8QCY66F/1981-31869-001.html}
}

@article{holtNeighboringSpectralContent2000,
  title = {Neighboring Spectral Content Influences Vowel Identification},
  volume = {108},
  issn = {0001-4966(Print)},
  abstract = {Investigated the relative contributions of phonetic labeling and spectral content to context effects in vowel perception in 4 experiments with a total of 103 native English-speaking undergraduates with normal hearing. Exp 1 introduced vowel stimuli spanning a perceptual range via manipulation of F2 frequency in a test of effects of [bVb] and [dVd] contexts on vowel identification. There was a significant effect of consonant context on vowel identification. Ss differentially labeled vowels dependent on consonant context. Results of Exps 2 and 3 indicate that non speech acoustic contexts with spectral characteristics modeled after consonant contexts of Exp 1 are sufficient to produce shifts in vowel identification. Exp 4 linked results of Exps 2 and 3 back to more ordinary speech perception in consonant-vowel-consonant stimuli. Voiced, but not voiceless, consonant context modulated listeners' vowel identity responses. The present data, in combination with other recent results (e.g., L. L. Holt, 1999) suggest that general perceptual mechanisms sensitive to similarities in spectral content play an important role in context effects in speech perception. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  number = {2},
  journal = {Journal of the Acoustical Society of America},
  doi = {10.1121/1.429604},
  author = {Holt, Lori L. and Lotto, Andrew J. and Kluender, Keith R.},
  year = {2000},
  keywords = {Auditory Perception,Phonetics,Speech Perception,Acoustics,Vowels},
  pages = {710-722},
  file = {/Users/lancelotdacosta/Zotero/storage/K5ZNCMJ5/2000-00504-006.html}
}

@book{rosenthalComputationalAuditoryScene1998,
  address = {{Mahwah, NJ, US}},
  series = {Computational Auditory Scene Analysis},
  title = {Computational Auditory Scene Analysis},
  isbn = {978-0-8058-2283-0},
  abstract = {The papers selected for inclusion in this collection are representative of a growing body of work in computational auditory scene analysis (CASA). Until recently, most of the work in computer understanding of sound has been heavily concentrated on the problem of automatic speech recognition (ASR), and today ASR is still a primary goal. Increasingly, however, researchers have been struck by the gap between computer ASR systems (as they are usually implemented) and the capabilities of the human auditory system. This interest is motivated not only by the growing awareness that unless they embody such an understanding, ASR systems will not be effective in unconstrained environments. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  publisher = {{Lawrence Erlbaum Associates Publishers}},
  editor = {Rosenthal, David F. and Okuno, Hiroshi G.},
  year = {1998},
  keywords = {Speech Perception,Auditory Scene Analysis,Automated Speech Recognition,Computer Applications},
  file = {/Users/lancelotdacosta/Zotero/storage/HHKGTR7F/1998-07853-000.html}
}

@article{chorowskiEndtoendContinuousSpeech2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.1602},
  primaryClass = {cs, stat},
  title = {End-to-End {{Continuous Speech Recognition}} Using {{Attention}}-Based {{Recurrent NN}}: {{First Results}}},
  shorttitle = {End-to-End {{Continuous Speech Recognition}} Using {{Attention}}-Based {{Recurrent NN}}},
  abstract = {We replace the Hidden Markov Model (HMM) which is traditionally used in in continuous speech recognition with a bi-directional recurrent neural network encoder coupled to a recurrent neural network decoder that directly emits a stream of phonemes. The alignment between the input and output sequences is established using an attention mechanism: the decoder emits each symbol based on a context created with a subset of input symbols elected by the attention mechanism. We report initial results demonstrating that this new approach achieves phoneme error rates that are comparable to the state-of-the-art HMM-based decoders, on the TIMIT dataset.},
  journal = {arXiv:1412.1602 [cs, stat]},
  author = {Chorowski, Jan and Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  month = dec,
  year = {2014},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/lancelotdacosta/Zotero/storage/GB6XJRLW/Chorowski et al. - 2014 - End-to-end Continuous Speech Recognition using Att.pdf;/Users/lancelotdacosta/Zotero/storage/8MKDVHE3/1412.html}
}

@inproceedings{chanListenAttendSpell2016,
  title = {Listen, {{Attend}} and {{Spell}}: {{A Neural Network}} for {{Large Vocabulary Conversational Speech Recognition}}},
  shorttitle = {Listen, {{Attend}} and {{Spell}}},
  booktitle = {{{ICASSP}}},
  author = {Chan, William and Jaitly, Navdeep and Le, Quoc V. and Vinyals, Oriol},
  year = {2016},
  file = {/Users/lancelotdacosta/Zotero/storage/DBC396AB/Chan et al. - 2016 - Listen, Attend and Spell A Neural Network for Lar.pdf}
}

@inproceedings{battenbergExploringNeuralTransducers2017,
  title = {Exploring Neural Transducers for End-to-End Speech Recognition},
  abstract = {In this work, we perform an empirical comparison among the CTC, RNN-Transducer, and attention-based Seq2Seq models for end-to-end speech recognition. We show that, without any language model, Seq2Seq and RNN-Transducer models both outperform the best reported CTC models with a language model, on the popular Hub5'00 benchmark. On our internal diverse dataset, these trends continue - RNN-Transducer models rescored with a language model after beam search outperform our best CTC models. These results simplify the speech recognition pipeline so that decoding can now be expressed purely as neural network operations. We also study how the choice of encoder architecture affects the performance of the three models - when all encoder layers are forward only, and when encoders downsample the input representation aggressively.},
  booktitle = {2017 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  doi = {10.1109/ASRU.2017.8268937},
  author = {Battenberg, Eric and Chen, Jitong and Child, Rewon and Coates, Adam and Li, Yashesh Gaur Yi and Liu, Hairong and Satheesh, Sanjeev and Sriram, Anuroop and Zhu, Zhenyao},
  month = dec,
  year = {2017},
  keywords = {Neural networks,CTC models,Decoding,end-to-end speech recognition,Hidden Markov models,Hub500 benchmark,language model,Mathematical model,neural transducers,recurrent neural nets,RNN-Transducer models,Seq2Seq models,Speech,speech recognition,speech recognition pipeline,Task analysis,Transducers},
  pages = {206-213},
  file = {/Users/lancelotdacosta/Zotero/storage/5JL56KGI/8268937.html}
}

@inproceedings{horiAdvancesJointCTCattention2017,
  title = {Advances in Joint {{CTC}}-Attention Based End-to-End Speech Recognition with a Deep {{CNN}} Encoder and {{RNN}}-{{LM}}},
  volume = {2017-August},
  language = {English},
  booktitle = {Proceedings of the {{Annual Conference}} of the {{International Speech Communication Association}}, {{INTERSPEECH}}},
  doi = {10.21437/Interspeech.2017-1296},
  author = {Hori, Takaaki and Watanabe, Shinji and Zhang, Yu and Chan, William},
  month = jan,
  year = {2017},
  pages = {949-953},
  file = {/Users/lancelotdacosta/Zotero/storage/MD296UIU/Hori et al. - 2017 - Advances in joint CTC-attention based end-to-end s.pdf;/Users/lancelotdacosta/Zotero/storage/M9WYZTVL/advances-in-joint-ctc-attention-based-end-to-end-speech-recogniti.html}
}

@inproceedings{prabhavalkarComparisonSequencetoSequenceModels2017,
  title = {A {{Comparison}} of {{Sequence}}-to-{{Sequence Models}} for {{Speech Recognition}}},
  abstract = {In this work, we conduct a detailed evaluation of various allneural, end-to-end trained, sequence-to-sequence models applied to the task of speech recognition. Notably, each of these systems directly predicts graphemes in the written domain, without using an external pronunciation lexicon, or a separate language model. We examine several sequence-to-sequence models including connectionist temporal classification (CTC), the recurrent neural network (RNN) transducer, an attentionbased model, and a model which augments the RNN transducer with an attention mechanism. We find that the sequence-to-sequence models are competitive with traditional state-of-the-art approaches on dictation test sets, although the baseline, which uses a separate pronunciation and language model, outperforms these models on voice-search test sets.},
  booktitle = {{{INTERSPEECH}}},
  doi = {10.21437/interspeech.2017-233},
  author = {Prabhavalkar, Rohit and Rao, Kanishka and Sainath, Tara N. and Li, Bo and Johnson, Leif and Jaitly, Navdeep},
  year = {2017},
  keywords = {Artificial neural network,Baseline (configuration management),Connectionism,End-to-end principle,Language model,Lexicon,Random neural network,Recurrent neural network,Speech recognition,Transducer},
  file = {/Users/lancelotdacosta/Zotero/storage/5DVGIZ6G/Prabhavalkar et al. - 2017 - A Comparison of Sequence-to-Sequence Models for Sp.pdf}
}

@article{chiuStateoftheartSpeechRecognition2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.01769},
  primaryClass = {cs, eess, stat},
  title = {State-of-the-Art {{Speech Recognition With Sequence}}-to-{{Sequence Models}}},
  abstract = {Attention-based encoder-decoder architectures such as Listen, Attend, and Spell (LAS), subsume the acoustic, pronunciation and language model components of a traditional automatic speech recognition (ASR) system into a single neural network. In previous work, we have shown that such architectures are comparable to state-of-theart ASR systems on dictation tasks, but it was not clear if such architectures would be practical for more challenging tasks such as voice search. In this work, we explore a variety of structural and optimization improvements to our LAS model which significantly improve performance. On the structural side, we show that word piece models can be used instead of graphemes. We also introduce a multi-head attention architecture, which offers improvements over the commonly-used single-head attention. On the optimization side, we explore synchronous training, scheduled sampling, label smoothing, and minimum word error rate optimization, which are all shown to improve accuracy. We present results with a unidirectional LSTM encoder for streaming recognition. On a 12, 500 hour voice search task, we find that the proposed changes improve the WER from 9.2\% to 5.6\%, while the best conventional system achieves 6.7\%; on a dictation task our model achieves a WER of 4.1\% compared to 5\% for the conventional system.},
  journal = {arXiv:1712.01769 [cs, eess, stat]},
  author = {Chiu, Chung-Cheng and Sainath, Tara N. and Wu, Yonghui and Prabhavalkar, Rohit and Nguyen, Patrick and Chen, Zhifeng and Kannan, Anjuli and Weiss, Ron J. and Rao, Kanishka and Gonina, Ekaterina and Jaitly, Navdeep and Li, Bo and Chorowski, Jan and Bacchiani, Michiel},
  month = dec,
  year = {2017},
  keywords = {Statistics - Machine Learning,Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/lancelotdacosta/Zotero/storage/NYH9YN98/Chiu et al. - 2017 - State-of-the-art Speech Recognition With Sequence-.pdf;/Users/lancelotdacosta/Zotero/storage/S2TBSIEW/1712.html}
}

@book{bourlardConnectionistSpeechRecognition1994,
  title = {Connectionist Speech Recognition: A Hybrid Approach},
  publisher = {{Kluwer Academic Publishers}},
  author = {Bourlard, Herve \' and Morgan, Nelson},
  year = {1994},
  file = {/Users/lancelotdacosta/Zotero/storage/JLVL6TCI/Bourlard et al. - List of Tables ix.pdf;/Users/lancelotdacosta/Zotero/storage/BXFL8USQ/summary.html}
}

@inproceedings{youngTreebasedStateTying1994,
  address = {{Stroudsburg, PA, USA}},
  series = {{{HLT}} '94},
  title = {Tree-Based {{State Tying}} for {{High Accuracy Acoustic Modelling}}},
  isbn = {978-1-55860-357-8},
  abstract = {The key problem to be faced when building a HMM-based continuous speech recogniser is maintaining the balance between model complexity and available training data. For large vocabulary systems requiring cross-word context dependent modelling, this is particularly acute since many such contexts will never occur in the training data. This paper describes a method of creating a tied-state continuous speech recognition system using a phonetic decision tree. This tree-based clustering is shown to lead to similar recognition performance to that obtained using an earlier data-driven approach but to have the additional advantage of providing a mapping for unseen triphones. State-tying is also compared with traditional model-based tying and shown to be clearly superior. Experimental results are presented for both the Resource Management and Wall Street Journal tasks.},
  booktitle = {Proceedings of the {{Workshop}} on {{Human Language Technology}}},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.3115/1075812.1075885},
  author = {Young, S. J. and Odell, J. J. and Woodland, P. C.},
  year = {1994},
  pages = {307--312},
  file = {/Users/lancelotdacosta/Zotero/storage/L3Y4MMBB/Young et al. - 1994 - Tree-based State Tying for High Accuracy Acoustic .pdf}
}

@inproceedings{seniorGMMFreeDNNTraining2014,
  title = {{{GMM}}-{{Free DNN Training}}},
  abstract = {While deep neural networks (DNNs) have become the dominant acoustic model (AM) for speech recognition systems, they are still dependent on Gaussian mixture models (GMMs) for alignments both for supervised training and for context dependent (CD) tree building. Here we explore bootstrapping DNN AM training without GMM AMs and show that CD trees can be built with DNN alignments which are better matched to the DNN model and its features. We show that these trees and alignments result in better models than from the GMM alignments and trees. By removing the GMM acoustic model altogether we simplify the system required to train a DNN from scratch.},
  author = {Senior, Andrew W. and Heigold, Georg and Bacchiani, Michiel and Liao, Hank},
  year = {2014},
  keywords = {Artificial neural network,Speech recognition,Acoustic cryptanalysis,Acoustic model,Deep learning,Gm(m),Google Map Maker,Mixture model,Neural Network Simulation,Trees (plant)},
  file = {/Users/lancelotdacosta/Zotero/storage/B29SLURH/Senior et al. - 2014 - GMM-Free DNN Training.pdf}
}

@article{kimEffectAgeBinaural2006,
  title = {Effect of Age on Binaural Speech Intelligibility in Normal Hearing Adults},
  volume = {48},
  abstract = {Abstract Sentence perception performance, in quiet and in background noise, was measured in three groups of adult subjects categorized as young, middle-aged, and elderly. Pure tone audiometric thresholds, measures of inner ear function, obtained in all subjects were within the clinically normal hearing range. The primary purpose of this study was to determine the effect of age on speech perception: a secondary purpose was to determine if the speech recognition problem commonly reported in elderly subjects might be due to alterations at sites central to the peripheral nervous system inner ear. Standardized sentence lists were presented in free field conditions in order to invoke binaural hearing that occurs at the brainstem level, and to simulate everyday speech-in-noise listening conditions. The results indicated: (1) an age effect on speech perception performance in quiet and in noise backgrounds, (2) absolute pure tone thresholds conventionally obtained monaurally do not accurately predict suprathreshold speech perception performance in elderly subjects, and (3) by implication the listening problems of the elderly may be influenced by auditory processing changes upstream of the inner ear.},
  journal = {Speech Communication},
  doi = {10.1016/j.specom.2005.09.004},
  author = {Kim, SungHee and Frisina, Robert D. and Mapes, Frances M. and Hickman, Elizabeth D.},
  year = {2006},
  keywords = {Speech recognition,Auditory processing disorder,Binaural beats,Categorization,Intelligibility (philosophy),JUNG,Mihajlo D. Mesarovic,Peripheral,Simulation},
  pages = {591-597}
}

@inproceedings{hintonAutoencodersMinimumDescription1993,
  address = {{San Francisco, CA, USA}},
  series = {{{NIPS}}'93},
  title = {Autoencoders, {{Minimum Description Length}} and {{Helmholtz Free Energy}}},
  abstract = {An autoencoder network uses a set of recognition weights to convert an input vector into a code vector. It then uses a set of generative weights to convert the code vector into an approximate reconstruction of the input vector. We derive an objective function for training autoencoders based on the Minimum Description Length (MDL) principle. The aim is to minimize the information required to describe both the code vector and the reconstruction error. We show that this information is minimized by choosing code vectors stochastically according to a Boltzmann distribution, where the generative weights define the energy of each possible code vector given the input vector. Unfortunately, if the code vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approximation gives an upper bound on the description length. Even when this bound is poor, it can be used as a Lyapunov function for learning both the generative and the recognition weights. We demonstrate that this approach can be used to learn factorial codes.},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Neural Information Processing Systems}}},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  author = {Hinton, Geoffrey E. and Zemel, Richard S.},
  year = {1993},
  pages = {3--10}
}

@article{vandenbroekRiskSensitivePath2010,
  title = {Risk Sensitive Path Integral Control},
  journal = {UAI},
  author = {{van den Broek}, Bart and Wiegerinck, Wim and Kappen, Bert},
  year = {2010}
}

@article{flemingRisksensitiveControlOptimal2002,
  title = {Risk-Sensitive Control and an Optimal Investment Model {{II}}},
  volume = {12},
  issn = {1050-5164, 2168-8737},
  abstract = {We consider an optimal investment problem proposed by Bielecki and Pliska. The goal of the investment problem is to optimize the long-term growth of expected utility of wealth. We consider HARA utility functions with exponent -{$\infty{}<\gamma{}<$}1-{$\infty{}<\gamma{}<$}1-\textbackslash{}infty{$<$} \textbackslash{}gamma{$<$} 1. The problem can be reformulated as an infinite time horizon risk-sensitive control problem. Some useful ideas and results from the theory of risk-sensitive control can be used in the analysis. Especially, we analyze the associated dynamical programming equation. Then an optimal (or approximately optimal) Markovian investment policy can be derived.},
  language = {en},
  number = {2},
  journal = {The Annals of Applied Probability},
  doi = {10.1214/aoap/1026915623},
  author = {Fleming, W. H. and Sheu, S. J.},
  month = may,
  year = {2002},
  keywords = {dynamical programming equation,long-term growth rate,optimal investment model,Ricatti equation,Risk-sensitive stochastic control},
  pages = {730-767},
  file = {/Users/lancelotdacosta/Zotero/storage/4Z8HJAH3/Fleming and Sheu - 2002 - Risk-sensitive control and an optimal investment m.pdf;/Users/lancelotdacosta/Zotero/storage/REWZIRBK/1026915623.html},
  mrnumber = {MR1910647},
  zmnumber = {1074.93038}
}

@article{bartoNoveltySurprise2013,
  title = {Novelty or {{Surprise}}?},
  volume = {4},
  issn = {1664-1078},
  abstract = {Novelty and surprise play significant roles in animal behavior and in attempts to understand the neural mechanisms underlying it. They also play important roles in technology, where detecting observations that are novel or surprising is central to many applications, such as medical diagnosis, text processing, surveillance, and security. Theories of motivation, particularly of intrinsic motivation, place novelty and surprise among the primary factors that arouse interest, motivate exploratory or avoidance behavior, and drive learning. In many of these studies, novelty and surprise are not distinguished from one another: the words are used more-or-less interchangeably. However, while undeniably closely related, novelty and surprise are very different. The purpose of this article is first to highlight the differences between novelty and surprise and to discuss how they are related by presenting an extensive review of mathematical and computational proposals related to them, and then to explore the implications of this for understanding behavioral and neuroscience data. We argue that opportunities for improved understanding of behavior and its neural basis are likely being missed by failing to distinguish between novelty and surprise.},
  language = {English},
  journal = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2013.00907},
  author = {Barto, Andrew and Mirolli, Marco and Baldassarre, Gianluca},
  year = {2013},
  keywords = {expectation,intrinsic motivation,novelty,novelty detection,surprise},
  file = {/Users/lancelotdacosta/Zotero/storage/6ALN54NY/Barto et al. - 2013 - Novelty or Surprise.pdf;/Users/lancelotdacosta/Zotero/storage/SIJN7XIF/Barto et al. - 2013 - Novelty or Surprise.pdf},
  ids = {bartoNoveltySurprise2013a}
}

@article{kauderGenesisMarginalUtility1953,
  title = {Genesis of the {{Marginal Utility Theory}}: {{From Aristotle}} to the {{End}} of the {{Eighteenth Century}}},
  volume = {63},
  issn = {0013-0133},
  shorttitle = {Genesis of the {{Marginal Utility Theory}}},
  abstract = {Emil Kauder;  Genesis of the Marginal Utility Theory: From Aristotle to the End of the Eighteenth Century, The Economic Journal, Volume 63, Issue 251, 1 Septemb},
  language = {en},
  number = {251},
  journal = {The Economic Journal},
  doi = {10.2307/2226451},
  author = {Kauder, Emil},
  month = sep,
  year = {1953},
  pages = {638-650},
  file = {/Users/lancelotdacosta/Zotero/storage/F9INDSWP/Kauder - 1953 - Genesis of the Marginal Utility Theory From Arist.pdf;/Users/lancelotdacosta/Zotero/storage/DIVDU64M/5258662.html}
}

@article{schmidhuberFormalTheoryCreativity2010,
  title = {Formal {{Theory}} of {{Creativity}}, {{Fun}}, and {{Intrinsic Motivation}} (1990\textendash{}2010)},
  volume = {2},
  issn = {1943-0604, 1943-0612},
  abstract = {The simple, but general formal theory of fun and intrinsic motivation and creativity (1990-2010) is based on the concept of maximizing intrinsic reward for the active creation or discovery of novel, surprising patterns allowing for improved prediction or data compression. It generalizes the traditional field of active learning, and is related to old, but less formal ideas in aesthetics theory and developmental psychology. It has been argued that the theory explains many essential aspects of intelligence including autonomous development, science, art, music, and humor. This overview first describes theoretically optimal (but not necessarily practical) ways of implementing the basic computational principles on exploratory, intrinsically motivated agents or robots, encouraging them to provoke event sequences exhibiting previously unknown, but learnable algorithmic regularities. Emphasis is put on the importance of limited computational resources for online prediction and compression. Discrete and continuous time formulations are given. Previous practical, but nonoptimal implementations (1991, 1995, and 1997-2002) are reviewed, as well as several recent variants by others (2005-2010). A simplified typology addresses current confusion concerning the precise nature of intrinsic motivation.},
  number = {3},
  journal = {IEEE Transactions on Autonomous Mental Development},
  doi = {10.1109/TAMD.2010.2056368},
  author = {Schmidhuber, J{\"u}rgen},
  month = sep,
  year = {2010},
  keywords = {surprise,intrinsic motivation,novelty,active learning,Active learning,aesthetics,aesthetics theory,art,Art,attention,cognition,Computational intelligence,creativity,data compression,Data compression,developmental psychology,event sequences,Eyes,Feedback,Fingers,formal theory of creativity,fun,humor,Intelligent robots,intrinsic reward,limited computational resources,music,novel patterns,Pediatrics,Predictive models,Psychology,science,typology of intrinsic motivation},
  pages = {230-247},
  file = {/Users/lancelotdacosta/Zotero/storage/SRZQVJ6L/5508364.html}
}

@article{oudeyerWhatIntrinsicMotivation2009,
  title = {What Is Intrinsic Motivation? {{A}} Typology of Computational Approaches},
  volume = {1},
  issn = {1662-5218},
  shorttitle = {What Is Intrinsic Motivation?},
  abstract = {Intrinsic motivation, the causal mechanism for spontaneous exploration and curiosity, is a central concept in developmental psychology. It has been argued to be a crucial mechanism for open-ended cognitive development in humans, and as such has gathered a growing interest from developmental roboticists in the recent years. The goal of this paper is threefold. First, it provides a synthesis of the different approaches of intrinsic motivation in psychology. Second, by interpreting these approaches in a computational reinforcement learning framework, we argue that they are not operational and even sometimes inconsistent. Third, we set the ground for a systematic operational study of intrinsic motivation by presenting a formal typology of possible computational approaches. This typology is partly based on existing computational models, but also presents new ways of conceptualizing intrinsic motivation. We argue that this kind of computational typology might be useful for opening new avenues for research both in psychology and developmental robotics.},
  language = {English},
  journal = {Frontiers in Neurorobotics},
  doi = {10.3389/neuro.12.006.2007},
  author = {Oudeyer, Pierre-Yves and Kaplan, Frederic},
  year = {2009},
  keywords = {artificial intelligence,cognitive development,computational modeling,curiosity,developmental robotics,exploration,intrinsic motivation,reinforcement learning,Reward},
  file = {/Users/lancelotdacosta/Zotero/storage/G7PEIDKP/Oudeyer and Kaplan - 2009 - What is intrinsic motivation A typology of comput.pdf;/Users/lancelotdacosta/Zotero/storage/J3AUIBDT/Oudeyer and Kaplan - 2009 - What is intrinsic motivation A typology of comput.pdf},
  ids = {oudeyerWhatIntrinsicMotivation2009a}
}

@misc{IntrinsicMotivationSelfDetermination,
  title = {Intrinsic {{Motivation}} and {{Self}}-{{Determination}} in {{Human Behavior}} | {{Edward Deci}} | {{Springer}}},
  howpublished = {https://www.springer.com/gp/book/9780306420221},
  file = {/Users/lancelotdacosta/Zotero/storage/YL44KBDY/9780306420221.html}
}

@book{deciIntrinsicMotivationSelfDetermination1985,
  series = {Perspectives in {{Social Psychology}}},
  title = {Intrinsic {{Motivation}} and {{Self}}-{{Determination}} in {{Human Behavior}}},
  isbn = {978-0-306-42022-1},
  abstract = {Early in this century, most empirically oriented psychologists believed that all motivation was based in the physiology of a set of non-nervous\- system tissue needs. The theories of that era reflected this belief and used it in an attempt to explain an increasing number of phenomena. It was not until the 1950s that it became irrefutably clear that much of human motivation is based not in these drives, but rather in a set of innate psychological needs. Their physiological basis is less understood; and as concepts, these needs lend themselves more easily to psycho\- logical than to physiological theorizing. The convergence of evidence from a variety of scholarly efforts suggests that there are three such needs: self-determination, competence, and interpersonal relatedness. This book is primarily about self-determination and competence (with particular emphasis on the former), and about the processes and structures that relate to these needs. The need for interpersonal relat\- edness, while no less important, remains to be explored, and the findings from those explorations will need to be integrated with the present theory to develop a broad, organismic theory of human motivation. Thus far, we have articulated self-determination theory, which is offered as a working theory-a theory in the making. To stimulate the research that will allow it to evolve further, we have stated self-determination theory in the form of minitheories that relate to more circumscribed domains, and we have developed paradigms for testing predictions from the various minitheories.},
  language = {en},
  publisher = {{Springer US}},
  doi = {10.1007/978-1-4899-2271-7},
  author = {Deci, Edward and Ryan, Richard M.},
  year = {1985},
  file = {/Users/lancelotdacosta/Zotero/storage/CV9GQCDB/9780306420221.html;/Users/lancelotdacosta/Zotero/storage/XFKUZTJA/9780306420221.html},
  ids = {deciIntrinsicMotivationSelfDetermination1985a}
}

@article{howardInformationValueTheory1966,
  title = {Information {{Value Theory}}},
  volume = {2},
  issn = {0536-1567, 2168-2887},
  abstract = {The information theory developed by Shannon was designed to place a quantitative measure on the amount of information involved in any communication. The early developers stressed that the information measure was dependent only on the probabilistic structure of the communication process. For example, if losing all your assets in the stock market and having whale steak for supper have the same probability, then the information associated with the occurrence of either event is the same. Attempts to apply Shannon's information theory to problems beyond communications have, in the large, come to grief. The failure of these attempts could have been predicted because no theory that involves just the probabilities of outcomes without considering their consequences could possibly be adequate in describing the importance of uncertainty to a decision maker. It is necessary to be concerned not only with the probabilistic nature of the uncertainties that surround us, but also with the economic impact that these uncertainties will have on us. In this paper the theory of the value of information that arises from considering jointly the probabilistic and economic factors that affect decisions is discussed and illustrated. It is found that numerical values can be assigned to the elimination or reduction of any uncertainty. Furthermore, it is seen that the joint elimination of the uncertainty about a number of even independent factors in a problem can have a value that differs from the sum of the values of eliminating the uncertainty in each factor separately.},
  number = {1},
  journal = {IEEE Transactions on Systems Science and Cybernetics},
  doi = {10.1109/TSSC.1966.300074},
  author = {Howard, Ronald A.},
  month = aug,
  year = {1966},
  keywords = {Uncertainty,Boundary conditions,Economic forecasting,Information theory,Linear systems,Logic,Random variables,State-space methods,Stress measurement,Systems engineering and theory},
  pages = {22-26},
  file = {/Users/lancelotdacosta/Zotero/storage/VZPP5TXK/4082064.html}
}

@article{schwartenbeckExplorationNoveltySurprise2013,
  title = {Exploration, Novelty, Surprise, and Free Energy Minimization},
  volume = {4},
  issn = {1664-1078},
  abstract = {This paper reviews recent developments under the free energy principle that introduce a normative perspective on classical economic (utilitarian) decision-making based on (active) Bayesian inference. It has been suggested that the free energy principle precludes novelty and complexity, because it assumes that biological systems\textemdash{}like ourselves\textemdash{}try to minimize the long-term average of surprise to maintain their homeostasis. However, recent formulations show that minimizing surprise leads naturally to concepts such as exploration and novelty bonuses. In this approach, agents infer a policy that minimizes surprise by minimizing the difference (or relative entropy) between likely and desired outcomes, which involves both pursuing the goal-state that has the highest expected utility (often termed ``exploitation'') and visiting a number of different goal-states (``exploration''). Crucially, the opportunity to visit new states increases the value of the current state. Casting decision-making problems within a variational framework, therefore, predicts that our behavior is governed by both the entropy and expected utility of future states. This dissolves any dialectic between minimizing surprise and exploration or novelty seeking.},
  journal = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2013.00710},
  author = {Schwartenbeck, Philipp and FitzGerald, Thomas and Dolan, Raymond J. and Friston, Karl},
  month = oct,
  year = {2013},
  file = {/Users/lancelotdacosta/Zotero/storage/MB494G39/Schwartenbeck et al. - 2013 - Exploration, novelty, surprise, and free energy mi.pdf},
  pmid = {24109469},
  pmcid = {PMC3791848}
}

@article{sunPlanningBeSurprised2011,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1103.5708},
  primaryClass = {cs, stat},
  title = {Planning to {{Be Surprised}}: {{Optimal Bayesian Exploration}} in {{Dynamic Environments}}},
  shorttitle = {Planning to {{Be Surprised}}},
  abstract = {To maximize its success, an AGI typically needs to explore its initially unknown world. Is there an optimal way of doing so? Here we derive an affirmative answer for a broad class of environments.},
  journal = {arXiv:1103.5708 [cs, stat]},
  author = {Sun, Yi and Gomez, Faustino and Schmidhuber, Juergen},
  month = mar,
  year = {2011},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence},
  file = {/Users/lancelotdacosta/Zotero/storage/GQ3D3LUM/Sun et al. - 2011 - Planning to Be Surprised Optimal Bayesian Explora.pdf;/Users/lancelotdacosta/Zotero/storage/TQINLD6T/Sun et al. - 2011 - Planning to Be Surprised Optimal Bayesian Explora.pdf;/Users/lancelotdacosta/Zotero/storage/992ZY3HB/1103.html;/Users/lancelotdacosta/Zotero/storage/AQCRSF28/1103.html}
}

@article{mhaskarNeuralNetworksOptimal1996,
  title = {Neural {{Networks}} for {{Optimal Approximation}} of {{Smooth}} and {{Analytic Functions}}},
  volume = {8},
  issn = {0899-7667},
  abstract = {We prove that neural networks with a single hidden layer are capable of providing an optimal order of approximation for functions assumed to possess a given number of derivatives, if the activation function evaluated by each principal element satisfies certain technical conditions. Under these conditions, it is also possible to construct networks that provide a geometric order of approximation for analytic target functions. The permissible activation functions include the squashing function (1 - e-x)-1 as well as a variety of radial basis functions. Our proofs are constructive. The weights and thresholds of our networks are chosen independently of the target function; we give explicit formulas for the coefficients as simple, continuous, linear functionals of the target function.},
  number = {1},
  journal = {Neural Computation},
  doi = {10.1162/neco.1996.8.1.164},
  author = {Mhaskar, H. N.},
  month = jan,
  year = {1996},
  pages = {164-177},
  file = {/Users/lancelotdacosta/Zotero/storage/Y8YHKEP2/6796304.html}
}

@article{todorovGeneralDualityOptimal2008,
  title = {General Duality between Optimal Control and Estimation},
  abstract = {Optimal control and estimation are dual in the LQG setting, as Kalman discovered, however this duality has proven difficult to extend beyond LQG. Here we obtain a more natural form of LQG duality by replacing the Kalman-Bucy filter with the information filter. We then generalize this result to non-linear stochastic systems, discrete stochastic systems, and deterministic systems. All forms of duality are established by relating exponentiated costs to probabilities. Unlike the LQG setting where control and estimation are in one-to-one correspondence, in the general case control turns out to be a larger problem class than estimation and only a sub-class of control problems have estimation duals. These are problems where the Bellman equation is intrinsically linear. Apart from their theoretical significance, our results make it possible to apply estimation algorithms to control problems and vice versa.},
  journal = {2008 47th IEEE Conference on Decision and Control},
  doi = {10.1109/cdc.2008.4739438},
  author = {Todorov, Emanuel},
  year = {2008},
  keywords = {Optimal control,Approximation algorithm,Bellman equation,Kalman filter,Nonlinear system,One-to-one (data model),Stochastic process},
  pages = {4286-4292}
}

@article{schmidhuberDevelopmentalRoboticsOptimal2006,
  title = {Developmental Robotics, Optimal Artificial Curiosity, Creativity, Music, and the Fine Arts},
  volume = {18},
  issn = {0954-0091},
  abstract = {Even in the absence of external reward, babies and scientists and others explore their world. Using some sort of adaptive predictive world model, they improve their ability to answer questions such as what happens if I do this or that? They lose interest in both the predictable things and those predicted to remain unpredictable despite some effort. One can design curious robots that do the same. The author's basic idea (1990, 1991) for doing so is a reinforcement learning (RL) controller is rewarded for action sequences that improve the predictor. Here, this idea is revisited in the context of recent results on optimal predictors and optimal RL machines. Several new variants of the basic principle are proposed. Finally, it is pointed out how the fine arts can be formally understood as a consequence of the principle: given some subjective observer, great works of art and music yield observation histories exhibiting more novel, previously unknown compressibility/regularity/predictability (with respect to the observer's particular learning algorithm) than lesser works, thus deepening the observer's understanding of the world and what is possible in it.},
  number = {2},
  journal = {Connection Science},
  doi = {10.1080/09540090600768658},
  author = {Schmidhuber, J{\"u}rgen},
  month = jun,
  year = {2006},
  keywords = {Exploration,Artificial creativity,Artificial curiosity,Developmental robotics,Fine arts,Music,World models},
  pages = {173-187},
  file = {/Users/lancelotdacosta/Zotero/storage/RJS78AZ3/09540090600768658.html}
}


@book{barlowPossiblePrinciplesUnderlying1961,
  title = {Possible {{Principles Underlying}} the {{Transformations}} of {{Sensory Messages}}},
  isbn = {978-0-262-31421-3},
  abstract = {This chapter is an attempt to formulate ideas about the operations performed by physiological mechanisms, and not merely a discussion of the physiological mechanisms of sensory pathways. It presents three hypotheses regarding the purpose of sensory relays. The first one is the ``password'' hypothesis, which posits that, since animals respond specifically to specific stimuli, their sensory pathways must possess mechanisms for detecting such stimuli and discriminating between them. The second hypothesis is the fashionable one that relays act as control points at which the flow of information is modulated according to the requirements of other parts of the nervous system. Finally, the third hypothesis theorizes that reduction of redundancy is an important principle guiding the organization of sensory messages and is carried out at relays in the sensory pathways.},
  language = {en\_US},
  publisher = {{The MIT Press}},
  author = {Barlow, H. B.},
  year = {1961},
  file = {/Users/lancelotdacosta/Zotero/storage/FXC5M2CN/upso-9780262518420-chapter-13.html}
}




@article{barlowInductiveInferenceCoding1974,
  title = {Inductive {{Inference}}, {{Coding}}, {{Perception}}, and {{Language}}},
  volume = {3},
  issn = {0301-0066},
  abstract = {The sensory system of animals, the words and language that connect the thoughts of one individual with those of another, and man-made communication systems, are all greatly influenced by the way information is coded. It is suggested that the role of inductive reasoning is to improve the efficiency of linguistic communication by changing the code, and its effectiveness in performing this function explains its survival value\textemdash{}the reason why this habit of mind is biologically successful and persists. The principles of coding are best understood in a limited, well-defined situation, but inductive reasoning should be viewed in a broad context, which must include the physiological and psychological mechanisms that form perceptions from physical stimuli and attach words to perceptions.},
  language = {en},
  number = {2},
  journal = {Perception},
  doi = {10.1068/p030123},
  author = {Barlow, H B},
  month = jun,
  year = {1974},
  pages = {123-134}
}

@article{linskerPerceptualNeuralOrganization1990,
  title = {Perceptual {{Neural Organization}}: {{Some Approaches Based}} on {{Network Models}} and {{Information Theory}}},
  volume = {13},
  shorttitle = {Perceptual {{Neural Organization}}},
  number = {1},
  journal = {Annual Review of Neuroscience},
  doi = {10.1146/annurev.ne.13.030190.001353},
  author = {Linsker, R},
  year = {1990},
  pages = {257-281},
  pmid = {2183677}
}

@article{lindleyMeasureInformationProvided1956,
  title = {On a {{Measure}} of the {{Information Provided}} by an {{Experiment}}},
  volume = {27},
  issn = {0003-4851},
  abstract = {A measure is introduced of the information provided by an experiment. The measure is derived from the work of Shannon [10] and involves the knowledge prior to performing the experiment, expressed through a prior probability distribution over the parameter space. The measure is used to compare some pairs of experiments without reference to prior distributions; this method of comparison is contrasted with the methods discussed by Blackwell. Finally, the measure is applied to provide a solution to some problems of experimental design, where the object of experimentation is not to reach decisions but rather to gain knowledge about the world.},
  number = {4},
  journal = {The Annals of Mathematical Statistics},
  author = {Lindley, D. V.},
  year = {1956},
  pages = {986-1005}
}

@article{todorovOptimalFeedbackControl2002,
  title = {Optimal Feedback Control as a Theory of Motor Coordination},
  volume = {5},
  issn = {1097-6256},
  abstract = {A central problem in motor control is understanding how the many biomechanical degrees of freedom are coordinated to achieve a common goal. An especially puzzling aspect of coordination is that behavioral goals are achieved reliably and repeatedly with movements rarely reproducible in their detail. Existing theoretical frameworks emphasize either goal achievement or the richness of motor variability, but fail to reconcile the two. Here we propose an alternative theory based on stochastic optimal feedback control. We show that the optimal strategy in the face of uncertainty is to allow variability in redundant (task-irrelevant) dimensions. This strategy does not enforce a desired trajectory, but uses feedback more intelligently, correcting only those deviations that interfere with task goals. From this framework, task-constrained variability, goal-directed corrections, motor synergies, controlled parameters, simplifying rules and discrete coordination modes emerge naturally. We present experimental results from a range of motor tasks to support this theory.},
  language = {eng},
  number = {11},
  journal = {Nature Neuroscience},
  doi = {10.1038/nn963},
  author = {Todorov, Emanuel and Jordan, Michael I.},
  month = nov,
  year = {2002},
  keywords = {Humans,Models; Neurological,Feedback,Fingers,Motor Activity,Motor Neurons,Movement},
  pages = {1226-1235},
  pmid = {12404008}
}

@article{bossaertsBehaviouralEconomicsNeuroeconomics2015,
  series = {Neuroeconomics},
  title = {From Behavioural Economics to Neuroeconomics to Decision Neuroscience: The Ascent of Biology in Research on Human Decision Making},
  volume = {5},
  issn = {2352-1546},
  shorttitle = {From Behavioural Economics to Neuroeconomics to Decision Neuroscience},
  abstract = {Here, we briefly review the evolution of research on human decision-making over the past few decades. We discern a trend whereby biology moves from subserving economics (neuroeconomics), to providing the data that advance our knowledge of the nature of human decision-making (decision neuroscience). Examples illustrate that the integration of behavioural and biological models is fruitful especially for understanding heterogeneity of choice in humans.},
  language = {en},
  journal = {Current Opinion in Behavioral Sciences},
  doi = {10.1016/j.cobeha.2015.07.001},
  author = {Bossaerts, Peter and Murawski, Carsten},
  month = oct,
  year = {2015},
  pages = {37-42},
  file = {/Users/lancelotdacosta/Zotero/storage/VFC8R8Y6/Bossaerts and Murawski - 2015 - From behavioural economics to neuroeconomics to de.pdf;/Users/lancelotdacosta/Zotero/storage/HCDKF2X3/S2352154615000881.html}
}

@article{opticanTemporalEncodingTwodimensional1987,
  title = {Temporal Encoding of Two-Dimensional Patterns by Single Units in Primate Inferior Temporal Cortex. {{III}}. {{Information}} Theoretic Analysis},
  volume = {57},
  issn = {0022-3077},
  abstract = {Ablation and single-unit studies in primates have shown that inferior temporal (IT) cortex is important for pattern discrimination. The first paper in this series suggested that single units in IT cortex of alert monkeys respond to a set of two-dimensional patterns with complex temporal modulation of their spike trains. The second paper quantified the waveform of the modulated responses of IT neurons with principal components and demonstrated that the coefficients of two to four of the principal components were stimulus dependent. Although the coefficients of the principal components are uncorrelated, it is possible that they are not statistically independent. That is, several coefficients could be determined by the same feature of the stimulus, and thus could be conveying the same information. The final part of this study examined this issue by comparing the amount of information about the stimulus that can be conveyed by two codes: a temporal waveform code derived from the coefficients of the first three principal components and a mean rate code derived from the spike count. We considered the neuron to be an information channel conveying messages about stimulus parameters. Previous applications of information theory to neurophysiology have dealt either with the theoretical capacity of neuronal channels or the temporal distribution of information within the spike train. This previous work usually used a general binary code to represent the spike train of a neuron's response. Such a general approach yields no indication of the nature of the neuron's intrinsic coding scheme because it depends only on the timing of spikes in the response. In particular, it is independent of any statistical properties of the responses. Our approach uses the principal components of the response waveform to derive a code for representing information about the stimuli. We regard this code as an indication of the neuron's intrinsic coding scheme, because it is based on the statistical properties of the neuronal responses. We measured how much information about the stimulus was present in the neuron's responses. This transmitted information was calculated for codes based on either the spike count or on the first three principal components of the response waveform. The information transmitted by each of the first three principal components was largely independent of that transmitted by the others. It was found that the average amount of information transmitted by the principal components was about twice as large as that transmitted by the spike count.(ABSTRACT TRUNCATED AT 400 WORDS)},
  language = {eng},
  number = {1},
  journal = {Journal of Neurophysiology},
  doi = {10.1152/jn.1987.57.1.162},
  author = {Optican, L. M. and Richmond, B. J.},
  month = jan,
  year = {1987},
  keywords = {Time Factors,Neurons,Animals,Models; Neurological,Models; Psychological,Temporal Lobe,Macaca mulatta,Electrophysiology,Form Perception,Information Theory,Pattern Recognition; Visual,Probability},
  pages = {162-178},
  pmid = {3559670}
}

@book{hakenSynergeticsIntroductionNonequilibrium1978,
  address = {{Berlin Heidelberg}},
  edition = {2},
  series = {Springer {{Series}} in {{Synergetics}}},
  title = {Synergetics: {{An Introduction Nonequilibrium Phase Transitions}} and {{Self}}-{{Organization}} in {{Physics}}, {{Chemistry}} and {{Biology}}},
  isbn = {978-3-642-96469-5},
  shorttitle = {Synergetics},
  abstract = {The publication of this second edition was motivated by several facts. First of all, the first edition had been sold out in less than one year. It had found excellent critics and enthusiastic responses from professors and students welcoming this new interdisciplinary approach. This appreciation is reflected by the fact that the book is presently translated into Russian and Japanese also. I have used this opportunity to include some of the most interesting recent developments. Therefore I have added a whole new chapter on the fascinating and rapidly growing field of chaos dealing with irregular motion caused by deterministic forces. This kind of phenomenon is presently found in quite diverse fields ranging from physics to biology. Furthermore I have included a section on the analytical treatment of a morphogenetic model using the order parameter concept developed in this book. Among the further additions, there is now a complete description of the onset of ultrashort laser pulses. It goes without{$\cdot$} saying that the few minor mis\- prints or errors of the first edition have been corrected. I wish to thank all who have helped me to incorporate these additions.},
  language = {en},
  publisher = {{Springer-Verlag}},
  doi = {10.1007/978-3-642-96469-5},
  author = {Haken, Hermann},
  year = {1978},
  file = {/Users/lancelotdacosta/Zotero/storage/8VBPIKXH/9783642964695.html}
}

@book{kauffmanOriginsOrderSelforganization1993,
  title = {The {{Origins}} of {{Order}}: {{Self}}-Organization and {{Selection}} in {{Evolution}}},
  isbn = {978-0-19-507951-7},
  shorttitle = {The {{Origins}} of {{Order}}},
  abstract = {Stuart Kauffman here presents a brilliant new paradigm for evolutionary biology, one that extends the basic concepts of Darwinian evolution to accommodate recent findings and perspectives from the fields of biology, physics, chemistry and mathematics. The book drives to the heart of the exciting debate on the origins of life and maintenance of order in complex biological systems. It focuses on the concept of self-organization: the spontaneous emergence of order widely observed throughout nature. Kauffman here argues that self-organization plays an important role in the emergence of life itself and may play as fundamental a role in shaping life's subsequent evolution as does the Darwinian process of natural selection. Yet until now no systematic effort has been made to incorporate the concept of self-organization into evolutionary theory. The construction requirements which permit complex systems to adapt remain poorly understood, as is the extent to which selection itself can yield systems able to adapt more successfully. This book explores these themes. It shows how complex systems, contrary to expectations, can spontaneously exhibit stunning degrees of order, and how this order, in turn, is essential for understanding the emergence and development of life on Earth. Topics include the new biotechnology of applied molecular evolution, with its important implications for developing new drugs and vaccines; the balance between order and chaos observed in many naturally occurring systems; new insights concerning the predictive power of statistical mechanics in biology; and other major issues. Indeed, the approaches investigated here may prove to be the new center around which biologicalscience itself will evolve. The work is written for all those interested in the cutting edge of research in the life sciences.},
  language = {en},
  publisher = {{Oxford University Press}},
  author = {Kauffman, Stuart A.},
  year = {1993},
  keywords = {Science / Life Sciences / Evolution},
  googlebooks = {lZcSpRJz0dgC}
}

@book{nicolisSelforganizationNonequilibriumSystems1977,
  address = {{New York}},
  title = {Self-Organization in {{Nonequilibrium Systems}}: {{From Dissipative Structures}} to {{Order Through Fluctuations}}},
  isbn = {978-0-471-02401-9},
  shorttitle = {Self-Organization in {{Nonequilibrium Systems}}},
  abstract = {Membranes, Dissipative Structures, and Evolution Edited by G. Nicolis \& R. Lefever Focuses on the problem of the emergence/maintenance of biological order at successively higher levels of complexity. Covers the spatiotemporal organization of simple biochemical networks; the formation of pluricellular or macromolecular assemblies; the evolution of these structures; and the functions of specific biological structures. Volume 29 in Advances in Chemical Physics Series, I. Prigogine \& Stuart A. Rice, Editors. 1975 Theory and Applications of Molecular Paramagnetism Edited by E. A. Boudreaux \& L. N. Mulay Comprehensively treats the basic theory of paramagnetic phenomena from both the classical and mechanical vantages. It examines the magnetic behavior of Lanthanide and Actinide elements as well as traditional transition metals. For each class of compounds, appropriate details of descriptive and mathematical theory are given before their applications. 1976 Theory and Aapplications of Molecular Diamagnetism Edited by L. N. Mulay \& E. A. Boudreaux An invaluable reference for solving chemical problems in magnetics, magnetochemistry, and related areas where magnetic data are important, such as solid\textendash{}state physics and optical spectroscopy. 1976},
  language = {English},
  publisher = {{Wiley-Blackwell}},
  author = {Nicolis, G. and Prigogine, I.},
  month = jun,
  year = {1977}
}

@article{ashbyPrinciplesSelfOrganizingDynamic1947,
  title = {Principles of the {{Self}}-{{Organizing Dynamic System}}},
  volume = {37},
  issn = {0022-1309},
  number = {2},
  journal = {The Journal of General Psychology},
  doi = {10.1080/00221309.1947.9918144},
  author = {Ashby, W. R.},
  month = oct,
  year = {1947},
  pages = {125-128},
  file = {/Users/lancelotdacosta/Zotero/storage/7TIHC2Y9/00221309.1947.html},
  pmid = {20270223}
}

@book{LecturesPhenomenaCommon,
  title = {Lectures on the Phenomena Common to Animals and Plants}
}

@book{bernardLecturesPhenomenaLife1974,
  title = {Lectures on the Phenomena of Life Common to Animals and Plants},
  isbn = {978-0-398-02857-2},
  language = {en},
  publisher = {{Thomas}},
  author = {Bernard, Claude},
  year = {1974},
  keywords = {Life (Biology),Physiology; Comparative},
  googlebooks = {zMhqAAAAMAAJ}
}

@inproceedings{mackayFreeEnergyMinimization1995,
  title = {A {{Free Energy Minimization Algorithm}} for {{Decoding}} and {{Cryptanalysis}}},
  booktitle = {Electronic Letters},
  author = {MacKay, David},
  year = {1995},
  keywords = {free energy,Inference,Algorithm,Bit array,Const (computer programming),Cryptanalysis,disease transmission,Energy minimization,Linear code,Linear-feedback shift register,Log probability,Polynomial,Shift Register Device Component},
  file = {/Users/lancelotdacosta/Zotero/storage/2CAVTZBQ/CryptanalysisDavid et al. - 1994 - A Free Energy Minimization Algorithmfor Decoding.pdf}
}

@book{mackayInformationTheoryInference2003,
  address = {{Cambridge, UK ; New York}},
  edition = {Sixth Printing 2007 edition},
  title = {Information {{Theory}}, {{Inference}} and {{Learning Algorithms}}},
  isbn = {978-0-521-64298-9},
  abstract = {Information theory and inference, taught together in this exciting textbook, lie at the heart of many important areas of modern technology - communication, signal processing, data mining, machine learning, pattern recognition, computational neuroscience, bioinformatics and cryptography. The book introduces theory in tandem with applications. Information theory is taught alongside practical communication systems such as arithmetic coding for data compression and sparse-graph codes for error-correction. Inference techniques, including message-passing algorithms, Monte Carlo methods and variational approximations, are developed alongside applications to clustering, convolutional codes, independent component analysis, and neural networks. Uniquely, the book covers state-of-the-art error-correcting codes, including low-density-parity-check codes, turbo codes, and digital fountain codes - the twenty-first-century standards for satellite communications, disk drives, and data broadcast. Richly illustrated, filled with worked examples and over 400 exercises, some with detailed solutions, the book is ideal for self-learning, and for undergraduate or graduate courses. It also provides an unparalleled entry point for professionals in areas as diverse as computational biology, financial engineering and machine learning.},
  language = {English},
  publisher = {{Cambridge University Press}},
  author = {MacKay, David J. C.},
  month = sep,
  year = {2003}
}

@book{helmholtzHelmholtzTreatisePhysiological1962,
  address = {{New York}},
  title = {Helmholtz's Treatise on Physiological Optics.},
  language = {English},
  publisher = {{Dover Publications}},
  author = {von Helmholtz, Hermann and Southall, James P. C},
  year = {1962},
  note = {OCLC: 523553}
}

@book{kahnemanProspectTheoryAnalysis1988,
  address = {{New York, NY, US}},
  series = {Decision, Probability, and Utility:  {{Selected}} Readings},
  title = {Prospect Theory:  {{An}} Analysis of Decision under Risk},
  isbn = {978-0-521-33391-7 978-0-521-33658-1},
  shorttitle = {Prospect Theory},
  abstract = {describes several classes of choice problems in which preferences systematically violate the axioms of expected utility theory  certainty, probability, and possibility / reflection effect / probabilistic insurance / prospect theory  show how prospect theory accounts for observed attitudes toward risk, discuss alternative representations of choice problems induced by shifts of reference point, and sketch several extensions of the present treatment (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/CBO9780511609220.014},
  author = {Kahneman, Daniel and Tversky, Amos},
  year = {1988},
  keywords = {Decision Making,Probability,Preferences},
  file = {/Users/lancelotdacosta/Zotero/storage/5GURQ2SU/Kahneman and Tversky - 1988 - Prospect theory  An analysis of decision under ri.pdf;/Users/lancelotdacosta/Zotero/storage/QEUUGV36/1988-97762-011.html}
}

@book{vonneumannTheoryGamesEconomic1944,
  address = {{Princeton, NJ, US}},
  series = {Theory of Games and Economic Behavior},
  title = {Theory of Games and Economic Behavior},
  abstract = {The authors analyze some fundamental questions of economic theory in terms of a mathematical theory of games. The common elements of economic behavior and such factors as strategy in games are presented, and the interrelated concepts are analyzed around the more or less central problem of utility. The book is divided into 12 chapters, the first being a formulation of the economic problem, presenting the objectives of the system used, the notion of utility, and a description of the structure of the authors' theory. The second chapter is a general formal description of games of strategy, and chapters 3-8 treat particular classifications of games. In chapter 9, the authors discuss the composition and decomposition of games. Chapter 10 is on simple games, and chapter 11 on general non-zero-sum games. (These are games in which the sum of all payments received by all players is not zero. Thus such games involve production or destruction of goods and are more closely analogous to general social and economic situations than zero-sum games.) The concluding chapter presents a generalization of the concept of utility and the discussion of an example. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  publisher = {{Princeton University Press}},
  author = {Von Neumann, J. and Morgenstern, O.},
  year = {1944},
  file = {/Users/lancelotdacosta/Zotero/storage/AS5ATXNX/1945-00500-000.html}
}

@book{bergerStatisticalDecisionTheory1985,
  address = {{New York}},
  edition = {2},
  series = {Springer {{Series}} in {{Statistics}}},
  title = {Statistical {{Decision Theory}} and {{Bayesian Analysis}}},
  isbn = {978-0-387-96098-2},
  abstract = {In this new edition the author has added substantial material on Bayesian analysis, including lengthy new sections on such important topics as empirical and hierarchical Bayes analysis, Bayesian calculation, Bayesian communication, and group decision making. With these changes, the book can be used as a self-contained introduction to Bayesian analysis. In addition, much of the decision-theoretic portion of the text was updated, including new sections covering such modern topics as minimax multivariate (Stein) estimation.},
  language = {en},
  publisher = {{Springer-Verlag}},
  doi = {10.1007/978-1-4757-4286-2},
  author = {Berger, James O.},
  year = {1985},
  file = {/Users/lancelotdacosta/Zotero/storage/42JHLWE7/9780387960982.html}
}

@article{jaynesInformationTheoryStatistical1957,
  title = {Information {{Theory}} and {{Statistical Mechanics}}},
  volume = {106},
  abstract = {Information theory provides a constructive criterion for setting up probability distributions on the basis of partial knowledge, and leads to a type of statistical inference which is called the maximum-entropy estimate. It is the least biased estimate possible on the given information; i.e., it is maximally noncommittal with regard to missing information. If one considers statistical mechanics as a form of statistical inference rather than as a physical theory, it is found that the usual computational rules, starting with the determination of the partition function, are an immediate consequence of the maximum-entropy principle. In the resulting "subjective statistical mechanics," the usual rules are thus justified independently of any physical argument, and in particular independently of experimental verification; whether or not the results agree with experiment, they still represent the best estimates that could have been made on the basis of the information available.},
  number = {4},
  journal = {Physical Review},
  doi = {10.1103/PhysRev.106.620},
  author = {Jaynes, E. T.},
  month = may,
  year = {1957},
  pages = {620-630},
  file = {/Users/lancelotdacosta/Zotero/storage/BXNIVUMJ/PhysRev.106.html}
}

@article{dehaeneNeuralRepresentationSequences2015,
  title = {The {{Neural Representation}} of {{Sequences}}: {{From Transition Probabilities}} to {{Algebraic Patterns}} and {{Linguistic Trees}}},
  volume = {88},
  issn = {1097-4199},
  shorttitle = {The {{Neural Representation}} of {{Sequences}}},
  abstract = {A sequence of images, sounds, or words can be stored at several levels of detail, from specific items and their timing to abstract structure. We propose a taxonomy of five distinct cerebral mechanisms for sequence coding: transitions and timing knowledge, chunking, ordinal knowledge, algebraic patterns, and nested tree structures. In each case, we review the available experimental paradigms and list the behavioral and neural signatures of the systems involved. Tree structures require a specific recursive neural code, as yet unidentified by electrophysiology, possibly unique to humans, and which may explain the singularity of human language and cognition.},
  language = {eng},
  number = {1},
  journal = {Neuron},
  doi = {10.1016/j.neuron.2015.09.019},
  author = {Dehaene, Stanislas and Meyniel, Florent and Wacongne, Catherine and Wang, Liping and Pallier, Christophe},
  month = oct,
  year = {2015},
  keywords = {Humans,Cognition,Brain,Probability,Language},
  pages = {2-19},
  file = {/Users/lancelotdacosta/Zotero/storage/3C3IH62E/Dehaene et al. - 2015 - The Neural Representation of Sequences From Trans.pdf},
  pmid = {26447569}
}

@article{fonollosaLearningChunkingSequences2015,
  title = {Learning of {{Chunking Sequences}} in {{Cognition}} and {{Behavior}}},
  volume = {11},
  issn = {1553-7358},
  abstract = {We often learn and recall long sequences in smaller segments, such as a phone number 858 534 22 30 memorized as four segments. Behavioral experiments suggest that humans and some animals employ this strategy of breaking down cognitive or behavioral sequences into chunks in a wide variety of tasks, but the dynamical principles of how this is achieved remains unknown. Here, we study the temporal dynamics of chunking for learning cognitive sequences in a chunking representation using a dynamical model of competing modes arranged to evoke hierarchical Winnerless Competition (WLC) dynamics. Sequential memory is represented as trajectories along a chain of metastable fixed points at each level of the hierarchy, and bistable Hebbian dynamics enables the learning of such trajectories in an unsupervised fashion. Using computer simulations, we demonstrate the learning of a chunking representation of sequences and their robust recall. During learning, the dynamics associates a set of modes to each information-carrying item in the sequence and encodes their relative order. During recall, hierarchical WLC guarantees the robustness of the sequence order when the sequence is not too long. The resulting patterns of activities share several features observed in behavioral experiments, such as the pauses between boundaries of chunks, their size and their duration. Failures in learning chunking sequences provide new insights into the dynamical causes of neurological disorders such as Parkinson's disease and Schizophrenia.},
  language = {en},
  number = {11},
  journal = {PLOS Computational Biology},
  doi = {10.1371/journal.pcbi.1004592},
  author = {Fonollosa, Jordi and Neftci, Emre and Rabinovich, Mikhail},
  year = {19-Nov-2015},
  keywords = {Neurons,Behavior,Learning,Memory,Dynamical systems,Human learning,Nonlinear dynamics,Synapses},
  pages = {e1004592},
  file = {/Users/lancelotdacosta/Zotero/storage/SY8SM8CT/Fonollosa et al. - 2015 - Learning of Chunking Sequences in Cognition and Be.pdf;/Users/lancelotdacosta/Zotero/storage/YN4SWHB7/article.html}
}

@book{harunoHierarchicalMOSAICMovement2003,
  title = {Hierarchical {{MOSAIC}} for Movement Generation},
  abstract = {Hierarchy plays a key role in human motor control and learning. We can generate a variety of structured motor sequences such as writing or speech and learn to combine elemental actions in novel orders. We previously proposed the Modular Selection and Identification for Control (MOSAIC) model to explain the remarkable ability animals show in motor learning, adaptation and behavioral switching. In this paper, we extend this to a hierarchical MOSAIC (HMOSAIC). Each layer of HMOSAIC consists of a MOSAIC, which is a set of paired control and predictive models. The higher-level MOSAIC receives two inputs: an abstract (symbolic) desired trajectory and posterior probabilities of its subordinate level, which represent which modules are playing a crucial role in the lower level under the current behavioral situation. The higher control model generates, as a motor command, prior probabilities for the lower-level modules, and therefore prioritizes which lower-level modules should be selected. In contrast, the higher predictive model learns to estimate the posterior probability at the next time step. The outputs from controllers as well as the learning of both predictors and controllers are weighted by the precision of the prediction. We first show that this bidirectional architecture provides a general framework capable of hierarchical motor learning that is chunking of movement patterns. Then, we discuss the similarities between the HMOSAIC},
  author = {Haruno, Masahiko and Wolpert, Daniel and Kawato, Mitsuo},
  year = {2003},
  file = {/Users/lancelotdacosta/Zotero/storage/LI2Z7LSF/A et al. - 2003 - Hierarchical MOSAIC for movement generation.pdf;/Users/lancelotdacosta/Zotero/storage/2AFF29UQ/summary.html}
}


