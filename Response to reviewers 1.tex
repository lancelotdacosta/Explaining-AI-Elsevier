

\documentclass{article}

%%%Packages
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amssymb, amsfonts, dsfont, mathrsfs, dirtytalk, tikz-cd, adjustbox, url, nicefrac, booktabs,array,longtable, tabu,bm, hyperref, graphicx, geometry,comment}
\usepackage{caption}

\usepackage[T1]{fontenc}    
%\usepackage[margin=1in]{geometry}
\usepackage[english]{babel}
%\usepackage[pagebackref=true]{hyperref}
%\usepackage{hyperref}
\usepackage[capitalise]{cleveref}
%\usepackage{arxiv}

%%%Backreferences
%\renewcommand*{\backref}[1]{}
%\renewcommand*{\backrefalt}[4]{%
%    \ifcase #1%
%          \or (p.~#2.)%
 %         \else (p.~#2.)%
  %  \fi%
   % }

%%%%Macros
\newcommand{\e}{\epsilon}
\newcommand{\f}{\forall}
\newcommand{\R}{\mathbb R}
\newcommand{\st}{\text{ s.t. }}
\newcommand{\ex}{\exists}
\newcommand{\p}{\mathbb P}
\newcommand{\D}{\mathcal D(\Omega)}
\newcommand{\N}{\mathbb N}
\newcommand{\oo}{\Omega}
\newcommand{\harpoon}{\rightharpoondown}
\newcommand{\E}{\mathbb E}


%%%%Theorem environments%%%%
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{example}[theorem]{Example}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{remark}[theorem]{Remark}
\newenvironment{claim}[1]{\par\noindent\emph{Claim:}\space#1}{}
\newenvironment{proofclaim}[1]{\par\noindent\textit{Proof of claim.}\space#1}{\hfill $\blacksquare$}
\newenvironment{sketchproof}[1]{\par\noindent\textit{Sketch proof.}\space#1}{\hfill $\square$}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times,authoryear]{elsarticle}
%% \documentclass[final,1p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,3p,times,authoryear]{elsarticle}
%% \documentclass[final,3p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,5p,times,authoryear]{elsarticle}
%% \documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
%\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.\usepackage{lineno}

%\journal{Journal of Mathematical Psychology}

\begin{document}
Dear Reviewers and Editor,

Thank you very much for taking the time to deconstruct the manuscript and your useful comments which we feel have greatly improved the presentation. Please find below our answers to your remarks.

With very best wishes,

Lancelot

--------

\textbf{Reviewer #1:}
\begin{itemize}
    \item[R1] On page 15, it talks about the likelihood matrix (A) denoting the probabilities of states given observations. I believe this should be the other way around (observations given states). The former would correspond to the posterior.
    \item[A] Well spotted, we have changed this to “the probability of outcomes given states”.
    \item[R1] The glossary of notation in pages 15-17 is very helpful. It would be very helpful to add, however, the types of variables specifically for the discrete state space formulation which is tackled in this paper. I.e. explicitly note that are represented as parameters of categorical distributions (and as such as vectors of real numbers). That the approximate posterior is a vector of real numbers, that the free energy and expected free energy are functionals that evaluate to a scalar quantity. Being explicit about this helps the reader 'type-check' the maths and can forestall much confusion.
    \item[A] Good point. We have added: “that evaluates to a scalar quantity”, for the free energy and expected free energy. For policy we added "actions sequence indexed in time". For the approximate posterior "Approximate posterior distribution over the latent variables of the generative model $s_{1:T},A, \pi$." and " Scalar valued probability distribution over $S \times \{x \in \mathbb R^{n} | x_i >0, \sum_i x_i =1\}^m \times \Pi$". Also in the main text, when we define the mean-field approximation, we thought following your comment to explicit the types of distributions there as well: "(see Table 2 for an explanation of the different distributions and variables in play)
      		\begin{align}
  		\label{eq: mean field approx}
  		  	Q(s_{1:T},A,\pi) &= Q(A)Q(\pi) \prod_{\tau =1}^T Q(s_\tau|\pi) \\
  		  	Q(s_\tau|\pi)&= Cat(\mathbf s_{\pi \tau}), \quad \mathbf s_{\pi \tau} \in  \{x \in \mathbb R^{m} \:|\: x_i >0, \sum_i x_i =1\} \nonumber \\
  		  	Q(\pi) &= Cat(\pmb \pi), \quad \{x \in \mathbb R^{|\Pi|} \:|\: x_i >0, \sum_i x_i =1\} \nonumber \\
  		  	Q(A)&= \prod_{i=1}^m Q(A_{\bullet i}) , \quad Q(A_{\bullet i})= Dir(\mathbf a_{\bullet i}), \quad \mathbf a_{\bullet i} \in (\mathbb R_{>0})^n \nonumber"
  		\end{align}
    \item[R1] p21 - is 'mathematical yoga' the expression that was intended? I like it though.
    \item[A] Yes, this was on purpose. I am glad that you like it.
    \item[R1] p22 -- it should be clarified that the VFE does not particularly measure surprise except when the approximate and true posterior are very close. Rather it is a bound on the true posterior, and since the true posterior is generally intractable, how tight this bound is is generally unknowable.
    \item[A] Yes, we added: "One should note that the free energy equals the surprise $-\log P(o_{1:t})$ only at the global free energy minimum, when the approximate posterior $Q(s_{1:T}, A, \pi)$ equals to the true posterior $P(s_{1:T}, A, \pi |o_{1:t})$. Outside of the global free energy minimum, the free energy upper bounds the surprise, in which case, since the true posterior is generally intractable, the tightness of the bound is generally unknowable."
    \item[R1] For equations 7,8,9 I think some further explanation of how to reach one from the other would be helpful here (if only in an appendix). Refreshing the reader's memory about how the probability densities (and expectations) are represented in terms of matrix operations would make this transition easier to follow.
    \item[A] We added detailed derivation of these equations in a new Appendix (Appendix B), making explicit how to form the expressions in terms of matrix multiplications.
    \item[R1] In equation 9, I don't believe $v_dot$ or v are defined anywhere (are these the prediction errors). Also using $\dot v$ is odd in a discrete time formulation. Could this just be formulated as a discrete time gradient descent - i.e. $v_{t+1} = v_t + \grad F$.
    \item[A] $v$ is an auxiliary variable, we further defined $v$ in the paragraph just before the expression for the dynamics: "with state-estimation expressed as a softmax function of accumulated (negative) free energy gradients, that we denote by $v$". The free energy is the prediction error (this wasn't explained properly, but see our next comment). Regarding the continuous dynamics with $\dot v$, we have added the following just before Section 5.1:  "Note the continuous time gradient descent on the free energy; although we focus on active inference with discrete generative models, this does not preclude the belief updating from occurring in continuous time (this is particularly important when relating these dynamics to neurobiological processes, see below). Yet, any numerical implementation of active inference would implement a discretised version of [the dynamics] until convergence, for example
  	\begin{equation*}
  	    \begin{split}
  	         v_{\pi \tau}^{(k)}&= v_{\pi \tau}^{(k-1)} -\kappa \nabla_{\bold s^{(k-1)}_{\pi\tau}} F_\pi(\bold s^{(k-1)}_{\pi 1},...,\bold s^{(k-1)}_{\pi T}) \text{ for small } \kappa >0 \\
  	        \bold s_{\pi\tau}^{(k)} &= \sigma (v_{\pi \tau}^{(k)})."
  	    \end{split}
  	\end{equation*}
    \item[R1] Also some further clarification of how exactly equation 8 which involves essentially the difference between the approximate posterior and the generative model maps onto the concept of prediction errors as in predictive coding would be appreciated. For instance, is the posterior or the generative model meant to be considered the 'prediction'?
    \item[A] We added a sentence in the section that introduces the free energy: "one can see that by varying $Q$ to minimise the variational free energy enables us to approximate the true posterior, while simultaneously ensuring that surprise remains low. The former offers the intuitive interpretation of the free energy as a generalised prediction error, as minimising free energy corresponds to suppressing the discrepancy between predictions, i.e., $Q$, and the actual state of affairs, i.e., the posterior; and indeed for a particular class of generative models, we recover the prediction error given by predictive coding schemes".
    \item[R1] In equation 12, I don't believe $s_\pi_\tau$ is defined anywhere. Indeed, what does it mean to just have a state instead of a probability density here?
    \item[A] We have modified the sentence before equation 12 to say: "In addition, we obtain a policy independent state-estimation at any time point $Q(s_\tau),\tau \in \{1,...,T\}$, as a Bayesian model average of approximate posterior beliefs about hidden states under policies, which may be expressed in terms of the distribution's parameters ($Q(s_\tau)= Cat(\bold s_\tau),Q(s_\tau |\pi)= Cat(\bold s_{\pi\tau})$)".
    \item[R1] on page 29, the pederson citation doesn't have any date associated with it in the text
    \item[A] Thanks, we fixed this.
    \item[R1] in equation 17, should the sums run up to T (not t), as in they should be up to the time horizon?
    \item[A] No. The sum should run over the time-steps where outcomes have been observed and in the paper we consider that outcomes have been observed up to a generic time $t$. But you are right that the software implementation of learning in active inference occurs once at the end of every trial, i.e., when $t=T$. We have the following paragraph that explains this: "Since synaptic plasticity dynamics occur at a much slower pace than perceptual inference, it is computationally much cheaper in numerical simulations to do a one-step belief update at the end of each trial of observation epochs. Explicitly, setting the free energy gradient to zero at the end of the trial gives the following update for Dirichlet parameters:
	\begin{equation}
	\label{eq: learning dirichlet one step}
  	    \bold a=  a +\sum_{\tau =1}^T o_\tau \otimes \bold s_\tau"
  	\end{equation}
  	\item[R1] The structure learning equations (eq 22-26) use a completely different notation than the rest of the paper. Unless there is compelling reason not to do this, I think it would be good to bring them into alignment.
  	\item[A] We replaced the variable $\nu$ (which was the odd one) by $A$, to illustrate that structure learning can be performed on the Dirichlet parameters of the likelihood matrix, linking back with the rest of the paper. Furthermore, we added: "Note that BMR is generic and could be used on any other variable that may be optimised during learning (e.g., see Appendix A.1), just by replacing $A$ in the following lines."
  	\item[R1] on page 52, the date on the mirza paper is weird (goes into the day)
    \item[A] Thanks we corrected this.
    \item[R1] on page 53 this paragraph:
Examples of
660 generative models that do not fall within the current discrete state-space, continuous
661 state-space (Buckley et al., 2017; Friston et al., 2012c; Adams et al., 2013b; Brown
662 et al., 2013; Adams et al., 2013a; Friston et al., 2012b; Brown and Friston, 2012)
663 or mixed (Friston et al., 2017c; Parr and Friston, 2018d, 2019) models - currently
664 implemented in active inference - include Markov decision trees (Jordan et al., 1998,
665 1997) and Boltzmann machines (Stone, 2019; Ackley et al., 1985; Salakhutdinov and
666 Hinton, 2012)
seems garbled to me
\item[A] We have simplified this paragraph and instead have: "Discovering new generative models corresponding to complex behavioural data will demand to extend the current process theory to these models, in order to provide testable predictions and reproduce the observed behaviour in-silico. Examples of generative models that are used in learning and decision-making, yet are not accommodated by the current process theory, include Markov decision trees and Boltzmann machines.
\item[R1] In appendix B is where novel (to me) results are presented. While the maths seems valid, I had some concerns about the presentation and explanation of the ideas. My core worry or confusion is about the justification of the Gibbs free energy. This quantity is introduced without any motivation or justification for why it should be considered. While the derivations about policies follow directly from the definition ,it is unclear that the gibbs free-energy is unique in this sense. Effectively, the core idea is to define a 'free energy' of some form G = Q(pi) - KL[Q(s,a)||p(s,a)] and thus that when the KL between the predicted and steady state is 0, then Q(pi) = G. However this specifies a very large class of possible functionals (by multiplying and dividing by different quantities) and it is unclear why the relationship between these functionals which are straightforwardly derived 'backwards' and the KL between the predicted and steady state is important at all. I think the
logic behind these manipulations should be spelled much more clearly out in the text to motivate why the gibbs free energy or some variant is useful to consider in this context.
\item[A] We rewrote a large part of this Appendix (now Appendix C) which hopefully makes the goal clearer. Let me highlight some points that we added: "The purpose of this Appendix is to motivate the definition of expected free energy from the perspective of reaching steady-state. Specifically, we will show how a family of distributions $Q(\pi)$, which comprises the (negative softmax) expected free energy, guarantee reaching steady-state.", then: "each distribution
\begin{equation}
   Q(\pi) =\sigma(-G(\pi; \beta)),\quad \beta \geq 0,
\end{equation}
describes a certain kind of system that reaches some steady-state distribution. In particular, the case $\beta =1$ corresponds to the approximate posterior over policies that is used in the main text."
\item[R1] In this sentence: "These solutions exist in virtue of conditional independencies, where the hidden states provide a Markov blanket that separates policies from outcomes". It is claimed that solutions for these policies always exist. This is not at all clear in the general case and I think needs stronger justification. It is easy to imagine cases where no possible policy allows you to reach a steady state density -- for instance if the steady state density exists on the other side of an impassable gulf. I feel like there are important smoothness (and perhaps ergodicity?) assumptions on the state and action space that are being elided here and which would be interesting to make explicit.
\item[A] Thanks a lot for pointing this out. It turns out that the missing assumption is the finiteness of $D_{KL}[Q(s_\tau, A) ||P(s_\tau, A)]$, otherwise the proof doesn't work. In addition, we unpacked this assumption to make your comment explicit: "An important consequence of Lemma 1 is that when (C.1) holds, we either have $D_{KL}[Q(s_\tau, A) ||P(s_\tau, A)]=+\infty$ or $D_{KL}[Q(s_\tau, A) ||P(s_\tau, A)]=0$ (steady-state). Intuitively, $D_{KL}[Q(s_\tau, A) ||P(s_\tau, A)]$ being infinite means that $Q(s_\tau, A)$ is singular with respect to $P(s_\tau, A)$. This is the case, for example, when the steady-state density sits on the other side of an impassable gulf, or when $Q(s_\tau, A)$ and $P(s_\tau, A)$ do not overlap. Conversely, the requirement that $D_{KL}[Q(s_\tau, A) ||P(s_\tau, A)]$ is finite implies that $Q(s_\tau, A)$ is absolutely continuous with respect to $P(s_\tau, A)$, that is $P(s_\tau, A)>0$ whenever $Q(s_\tau, A)>0$."
\item[R1] KL control is presented as being a special case of the gibbs free energy when beta = 0. However when beta = 0, that implies that the posterior density over policies is 0 (!!!) which is a very odd situation given that KL control agents can presumably be thought of as having policies. I feel like a more intuitive justification and explanation of the gibbs free energy and these sorts of relations would be much appreciated in demystifying this section.
\item[A] When $\beta =0$, the posterior over policies is not zero, rather
$$Q(\pi) = \sigma(-G(\pi; 0)) = \sigma(-D_{KL}[Q(s_\tau, A|\pi)||P(s_\tau, A)])$$
and indeed, KL control agents do have policies.
\item[R1] Further explanation of corollary 3 (that the markov blanket implies beta = 1 and thus the EFE is returned) would also be very much appreciated.
\item[A] This claim rested on a previous piece of work which derives the expected free energy for systems of coupled diffusions under a Markov blanket at non-equilibrium steady-state (see "Markov blankets, information geometry and stochastic thermodynamics" (2020)). However, in the meantime we realised that $\beta$ should arise there as well, so we removed the claim that $\beta =1$ from this Appendix and instead we added: "in active inference, we currently assume $\beta =1$ for simplicity, however, the implications of different values of $\beta$ on behaviour are interesting and will be examined in future work."
\end{itemize}

\textbf{Reviewer #2:}

\begin{itemize}
    \item[R2] As this journal is the journal of mathematical *psychology*, I suggest that the authors discuss how the theory of active inference is beneficial for psychologists, who want to understand psychological or cognitive processes, without mentioning to neurophysiology. Psychologists often use mathematical models of psychological processes, e.g., drift-diffusion models and the Rescorla-Wagner model, to fit or simulate behavioral data. These mechanistic models can be easily related to psychological processes. On the other hand, it is somehow difficult to understand what computational process works in the active inference framework, especially for those who are not familiar with the concept of statistical physics. I believe that active inference offers a principle of behavior, but the authors may want to clarify these issues, e.g., how simulating experimental behavior in active inference is useful for the understanding (psychological) processes, and, how the active inference provides a principle underlying the existing process models.
\item[A] Great point. We have added the following on line 22: "In addition to simulate behaviour, active inference allows to answer questions about an individual's psychological processes, by comparing the evidence of different mechanistic hypotheses in relation to behavioural data. Active inference is very generic and allows to view different models of behaviour in the same light. For example, a drift diffusion model can now be seen in relation to predictive coding as they can both be interpreted as minimising free energy through a process of evidence accumulation. Similarly, a dynamic programming model of choice behaviour corresponds to minimising expected free energy under the prior preference of maximising reward. In being generic active inference is not meant to replace any of the existing models, rather it should be used as a tool to uncover the commitments and assumptions of more specific models."
\item[R2] The idea that is difficult to understand for beginners is dealing with the policy $\pi$ as a random variable. Readers may want a kind notion regarding this around equation (1).
\item[A] Right after equation (1), we added "Note the policy $\pi$ is a random variable. This entails planning as inferring the best action sequence from observations".
\item[R2] Some terminology for mathematical terms is used without explaining the reason. For example, why the first term of equation (10) is termed "Risk"? -- in decision science, the risk usually refers to the variance of outcome (i.e., financial risk), but this does not seem the case here. Also, this is true for "ambiguity" and "intrinsic value."
\item[A] Risk is indeed meant as in financial risk. After the first instance of risk and ambiguity we added: "By risk we mean the difference between predicted and a priori predictions in the future (e.g., the quantification of losses as in financial risk) and ambiguity is the uncertainty associated to future observations, given states." After the first instance of intrinsic value we added: "Intrinsic value corresponds to the expected information gain about model parameters."
\item[R2] Line 155, "and the probability of states given outcomes": perhaps this should read "and the probability of outcomes given states"?
\item[A] Well spotted, we have changed this to “the probability of outcomes given states”.
\item[R2] The name of the person in the photo is missing in Fig. 4 while there is in Fig.3.
\item[A] Thank you. We have added the names on Figure 4.
\end{itemize}


\end{document}